{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e03f3c7d69264457a26f5bd8ea054b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8e85d11d5a945f4925687585a440769",
              "IPY_MODEL_7226de859e964251a091a8040dccde1d",
              "IPY_MODEL_b8a699ae55d94508b7ef4f8b8f6abdf0"
            ],
            "layout": "IPY_MODEL_1908c9a87b7f4d4f91788bb73ae41bab"
          }
        },
        "d8e85d11d5a945f4925687585a440769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaf9d30bc0164c6e8a722e301d601026",
            "placeholder": "​",
            "style": "IPY_MODEL_0d14a78af01f4c28b8b2b353c90816aa",
            "value": "Map: 100%"
          }
        },
        "7226de859e964251a091a8040dccde1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9358ffa5c0a428c9a24104c1da8251b",
            "max": 32430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d8a0da559554f419016cf635902d79d",
            "value": 32430
          }
        },
        "b8a699ae55d94508b7ef4f8b8f6abdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c836b8fde7344b6a30e6029c4da854d",
            "placeholder": "​",
            "style": "IPY_MODEL_dcc32f8b17a8479bb7646e94de155939",
            "value": " 32430/32430 [00:13&lt;00:00, 2295.46 examples/s]"
          }
        },
        "1908c9a87b7f4d4f91788bb73ae41bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf9d30bc0164c6e8a722e301d601026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d14a78af01f4c28b8b2b353c90816aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9358ffa5c0a428c9a24104c1da8251b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8a0da559554f419016cf635902d79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c836b8fde7344b6a30e6029c4da854d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc32f8b17a8479bb7646e94de155939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6e08900d984034b9d1261052560eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96224c23afa94221bed9a525f7823012",
              "IPY_MODEL_ab9991598cee47b2a4efdde5e36e6076",
              "IPY_MODEL_49955636c2354111961af5a9f1ec98f5"
            ],
            "layout": "IPY_MODEL_c94087d9f232474f953f1d4a64a841ad"
          }
        },
        "96224c23afa94221bed9a525f7823012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2e5f29a0db4ed4ab3bc3e139b88d12",
            "placeholder": "​",
            "style": "IPY_MODEL_6b3bc9fc83dc467bac523f81b98d39de",
            "value": "Map: 100%"
          }
        },
        "ab9991598cee47b2a4efdde5e36e6076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52536b7a24f54e40a4989b348442f4be",
            "max": 8015,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e92efa28d8ba41048fdfa384fc03a7e5",
            "value": 8015
          }
        },
        "49955636c2354111961af5a9f1ec98f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0c4d762aef4ef9b4d3202562fea557",
            "placeholder": "​",
            "style": "IPY_MODEL_f943550ee4834634871e140622282a70",
            "value": " 8015/8015 [00:03&lt;00:00, 2519.80 examples/s]"
          }
        },
        "c94087d9f232474f953f1d4a64a841ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2e5f29a0db4ed4ab3bc3e139b88d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b3bc9fc83dc467bac523f81b98d39de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52536b7a24f54e40a4989b348442f4be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92efa28d8ba41048fdfa384fc03a7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d0c4d762aef4ef9b4d3202562fea557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f943550ee4834634871e140622282a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3uPNqPBXelY",
        "outputId": "641924a9-d347-4bbc-8e07-cf8b981ec2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets accelerate evaluate sentencepiece\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 0) Imports\n",
        "# ---------------------------\n",
        "import os, re, math, time, random, shutil, difflib, glob, json, zipfile, tarfile\n",
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
        "    EarlyStoppingCallback, TrainerCallback, TrainingArguments,\n",
        "    TrainerControl, TrainerState\n",
        ")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "except Exception:\n",
        "    files = None"
      ],
      "metadata": {
        "id": "koqN8PI3Xrg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 1) Config (parallel to Code A)\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DATA_DIR   = \"/content/output\"\n",
        "TRAIN_FILE = f\"{DATA_DIR}/llm_train_gen.jsonl\"\n",
        "VALID_FILE = f\"{DATA_DIR}/llm_valid_gen.jsonl\"\n",
        "TEST_FILE  = f\"{DATA_DIR}/llm_test_gen.jsonl\"\n",
        "\n",
        "# >>> Use FLAN-T5 here <<<\n",
        "BASE_MODEL = \"google/flan-t5-base\"   # or \"google/flan-t5-large\" (watch VRAM)\n",
        "OUT_DIR    = \"/content/flan_t5_degree2_ckpt\"\n",
        "FINAL_DIR  = \"/content/flan_t5_degree2_final\"\n",
        "\n",
        "MAX_IN_LEN  = 512\n",
        "MAX_OUT_LEN = 256\n",
        "EVENT_TOKEN = \"<EVENTSEP>\"\n",
        "CHUNK_N_SENT = None"
      ],
      "metadata": {
        "id": "pBXD5WU2Xxo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2) Regex & utils\n",
        "# -----------------------------\n",
        "EVENT_SEP = re.compile(r\"\\s*<EVENTSEP>\\s*\", re.IGNORECASE)\n",
        "TYPE_RE   = re.compile(r'event\\s*type\\s*:\\s*(.+?)\\.', re.IGNORECASE)\n",
        "TRIG_RE   = re.compile(r'trigger\\s*:\\s*(.+?)\\.', re.IGNORECASE)\n",
        "TOK_RE    = re.compile(r\"\\w+\", re.UNICODE)\n",
        "\n",
        "def norm(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\",\" \", s.strip().lower()) if s else s\n",
        "\n",
        "def parse_pairs(text: str) -> List[Tuple[str,str]]:\n",
        "    if not text or \"no events\" in text.lower(): return []\n",
        "    out=[]\n",
        "    for ch in [c for c in EVENT_SEP.split(text) if c.strip()]:\n",
        "        t = TYPE_RE.search(ch); g = TRIG_RE.search(ch)\n",
        "        et = norm(t.group(1)) if t else None\n",
        "        tr = norm(g.group(1)) if g else None\n",
        "        if et and tr: out.append((et,tr))\n",
        "    return out\n",
        "\n",
        "def dedup_events_str(text: str) -> str:\n",
        "    if not text: return \"\"\n",
        "    seen=set(); kept=[]\n",
        "    for p in [p.strip() for p in EVENT_SEP.split(text) if p.strip()]:\n",
        "        key = norm(p)\n",
        "        if key and key not in seen:\n",
        "            seen.add(key); kept.append(p)\n",
        "    return f\" {EVENT_TOKEN} \".join(kept)\n",
        "\n",
        "def token_set(s): return set(TOK_RE.findall(s.lower())) if s else set()\n",
        "\n",
        "def trigger_overlap(p,g):\n",
        "    if not p or not g: return 0.0\n",
        "    if norm(p)==norm(g): return 1.0\n",
        "    ps,gs = token_set(p), token_set(g)\n",
        "    return 0.5 if (ps and gs and ps&gs) else 0.0"
      ],
      "metadata": {
        "id": "TotFzdEuX5iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3) Data loading + cleaning / alignment (+ optional upsampling)\n",
        "# -----------------------------\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "# ---- A. Input normalization ----\n",
        "_WS    = re.compile(r\"\\s+\")\n",
        "_CTRL  = re.compile(r\"[\\u0000-\\u001F\\u007F]\")\n",
        "HTML_TAG = re.compile(r\"</?[^>]+>\")\n",
        "\n",
        "def normalize_input_text(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = (s.replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\")\n",
        "           .replace(\"‘\",\"'\").replace(\"’\",\"'\")\n",
        "           .replace(\"–\",\"-\").replace(\"—\",\"-\"))\n",
        "    s = _CTRL.sub(\" \", s)\n",
        "    s = HTML_TAG.sub(\" \", s)                 # strip naive HTML tags\n",
        "    s = re.sub(r\"\\\\[a-zA-Z]+\", \" \", s)       # \\alpha, \\cite, ...\n",
        "    s = re.sub(r\"\\{[^{}]{0,200}\\}\", \" \", s)  # {…} light cleanup\n",
        "    s = _WS.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# ---- B. Target (gold) normalization ----\n",
        "# strict single-line validator\n",
        "EVENT_LINE_RE = re.compile(\n",
        "    r'^\\s*Event\\s*type\\s*:\\s*([^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*([^.\\n\\r<]+)\\.\\s*$',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def normalize_output_line(line: str) -> str | None:\n",
        "    if not isinstance(line, str): return None\n",
        "    line = unicodedata.normalize(\"NFKC\", line)\n",
        "    line = line.replace(\"–\",\"-\").replace(\"—\",\"-\").strip()\n",
        "    if not line.endswith(\".\"):\n",
        "        line += \".\"\n",
        "    m = EVENT_LINE_RE.match(line)\n",
        "    if not m:\n",
        "        return None\n",
        "    et = re.sub(r\"\\s+\", \" \", m.group(1).strip().lower())\n",
        "    tr = re.sub(r\"\\s+\", \" \", m.group(2).strip().lower())\n",
        "    return f\"Event type: {et}. Trigger: {tr}.\"\n",
        "\n",
        "def normalize_target_text(y: str, token: str = EVENT_TOKEN) -> str:\n",
        "    if not isinstance(y, str) or not y.strip():\n",
        "        return \"No events.\"\n",
        "    parts = [p for p in re.split(rf\"\\s*{re.escape(token)}\\s*\", y) if p.strip()]\n",
        "    clean = []\n",
        "    seen = set()\n",
        "    for p in parts:\n",
        "        nl = normalize_output_line(p)\n",
        "        if nl and nl not in seen:\n",
        "            seen.add(nl)\n",
        "            clean.append(nl)\n",
        "    return f\" {token} \".join(clean) if clean else \"No events.\"\n",
        "\n",
        "# ---- C. Anti-hallucination: ensure trigger appears (roughly) in source ----\n",
        "def _rough_contains(text: str, phrase: str, threshold=0.82) -> bool:\n",
        "    text_l   = text.lower()\n",
        "    phrase_l = phrase.lower()\n",
        "    if phrase_l in text_l:\n",
        "        return True\n",
        "    n = len(phrase_l)\n",
        "    if n < 3:\n",
        "        return False\n",
        "    # coarse sliding window; hop = n//2 (fast & robust)\n",
        "    hop = max(1, n // 2)\n",
        "    best = 0.0\n",
        "    for i in range(0, max(1, len(text_l) - n + 1), hop):\n",
        "        cand = text_l[i:i+n]\n",
        "        best = max(best, SequenceMatcher(None, cand, phrase_l).ratio())\n",
        "        if best >= threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def align_and_filter_output(src: str, y: str, token: str = EVENT_TOKEN) -> str:\n",
        "    src = normalize_input_text(src)\n",
        "    if y.strip().lower() == \"no events.\":\n",
        "        return y\n",
        "    kept = []\n",
        "    for chunk in [c for c in re.split(rf\"\\s*{re.escape(token)}\\s*\", y) if c.strip()]:\n",
        "        nl = normalize_output_line(chunk.strip())\n",
        "        if not nl:\n",
        "            continue\n",
        "        m = EVENT_LINE_RE.match(nl)\n",
        "        trig = m.group(2) if m else \"\"\n",
        "        # keep only if trigger is (roughly) anchored in source\n",
        "        if _rough_contains(src, trig, threshold=0.82):\n",
        "            kept.append(nl)\n",
        "    return f\" {token} \".join(kept) if kept else \"No events.\"\n",
        "\n",
        "# ---- D. Example-level validation ----\n",
        "def is_valid_example(inp: str, out: str, min_chars_src=20, max_out_len=800) -> bool:\n",
        "    if not inp or len(inp) < min_chars_src:\n",
        "        return False\n",
        "    if not out or len(out) > max_out_len:\n",
        "        return False\n",
        "    if out.strip().lower() == \"no events.\":\n",
        "        return True\n",
        "    # must contain at least one valid event line\n",
        "    for chunk in re.split(rf\"\\s*{re.escape(EVENT_TOKEN)}\\s*\", out):\n",
        "        if normalize_output_line(chunk.strip()):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ---- E. Load raw → clean/align → filter ----\n",
        "def load_plain(path):\n",
        "    return load_dataset(\"json\", data_files={\"data\": path})[\"data\"]\n",
        "\n",
        "raw_train = load_plain(TRAIN_FILE)\n",
        "raw_valid = load_plain(VALID_FILE)\n",
        "raw_test  = load_plain(TEST_FILE)\n",
        "\n",
        "def _clean_map(ex):\n",
        "    # normalize source\n",
        "    src = normalize_input_text(ex[\"input\"])\n",
        "    # normalize target format\n",
        "    tgt = normalize_target_text(ex[\"output\"], token=EVENT_TOKEN)\n",
        "    # align triggers to source (drop unanchored)\n",
        "    tgt = align_and_filter_output(src, tgt, token=EVENT_TOKEN)\n",
        "    return {\"input\": src, \"output\": tgt}\n",
        "\n",
        "train_clean = raw_train.map(_clean_map)\n",
        "valid_clean = raw_valid.map(_clean_map)\n",
        "test_clean  = raw_test.map(_clean_map)\n",
        "\n",
        "# filter malformed/noisy\n",
        "train_clean = train_clean.filter(lambda ex: is_valid_example(ex[\"input\"], ex[\"output\"]))\n",
        "valid_clean = valid_clean.filter(lambda ex: is_valid_example(ex[\"input\"], ex[\"output\"]))\n",
        "test_clean  = test_clean.filter(lambda ex: is_valid_example(ex[\"input\"], ex[\"output\"]))\n",
        "\n",
        "# ---- F. Prevent leakage: drop exact source duplicates across splits ----\n",
        "train_keys = set(train_clean.map(lambda ex: {\"k\": normalize_input_text(ex[\"input\"])} )[\"k\"])\n",
        "\n",
        "def _not_in_train(ex):\n",
        "    return normalize_input_text(ex[\"input\"]) not in train_keys\n",
        "\n",
        "valid_clean = valid_clean.filter(_not_in_train)\n",
        "test_clean  = test_clean.filter(_not_in_train)\n",
        "\n",
        "# ---- G. Build ontology from *clean* train ----\n",
        "TYPE_FREQ = Counter()\n",
        "for ex in train_clean:\n",
        "    for et,tr in parse_pairs(ex[\"output\"]):\n",
        "        TYPE_FREQ[et] += 1\n",
        "ONTOLOGY = sorted(TYPE_FREQ.keys())\n",
        "\n",
        "def nearest_type(t: str) -> str:\n",
        "    if not t or not ONTOLOGY: return t\n",
        "    t = norm(t)\n",
        "    cand = difflib.get_close_matches(t, ONTOLOGY, n=1, cutoff=0.8)\n",
        "    return cand[0] if cand else t\n",
        "\n",
        "# ---- H. (Optional) Upsample rare types to improve recall on tail classes ----\n",
        "UPSAMPLE_RARE_TYPES = False           # set True to enable\n",
        "TARGET_PER_TYPE     = None            # None → median; or set an integer\n",
        "\n",
        "def _extract_types_from_target(y: str) -> list[str]:\n",
        "    out=[]\n",
        "    for ch in [c for c in EVENT_SEP.split(y) if c.strip()]:\n",
        "        m = TYPE_RE.search(ch)\n",
        "        if m:\n",
        "            out.append(norm(m.group(1)))\n",
        "    return out\n",
        "\n",
        "if UPSAMPLE_RARE_TYPES:\n",
        "    # convert to a Python list to rebalance, then back to Dataset\n",
        "    rows = []\n",
        "    for ex in train_clean:\n",
        "        rows.append({\"input\": ex[\"input\"], \"output\": ex[\"output\"], \"_types\": list(set(_extract_types_from_target(ex[\"output\"])))})\n",
        "    from collections import defaultdict\n",
        "    buckets = defaultdict(list)\n",
        "    for r in rows:\n",
        "        types = r[\"_types\"] or [\"_no_event\"]\n",
        "        for t in types:\n",
        "            buckets[t].append({\"input\": r[\"input\"], \"output\": r[\"output\"]})\n",
        "\n",
        "    # choose target size\n",
        "    if TARGET_PER_TYPE is None:\n",
        "        freqs = [len(v) for k,v in buckets.items() if k != \"_no_event\"]\n",
        "        TARGET_PER_TYPE = int(sorted(freqs)[len(freqs)//2]) if freqs else 0\n",
        "\n",
        "    aug = []\n",
        "    for t, items in buckets.items():\n",
        "        if t == \"_no_event\":\n",
        "            aug.extend(items)  # keep as-is\n",
        "            continue\n",
        "        if len(items) >= TARGET_PER_TYPE:\n",
        "            aug.extend(items)\n",
        "        else:\n",
        "            need = TARGET_PER_TYPE - len(items)\n",
        "            aug.extend(items + random.choices(items, k=need))\n",
        "    train_clean = Dataset.from_list(aug)"
      ],
      "metadata": {
        "id": "Yzn3rOjzX6xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4) Prompt builder (now with retrieval-based few-shot; fallback to random)\n",
        "# -----------------------------\n",
        "# Prepare a bank of good few-shot candidates from *clean* train\n",
        "def build_fewshot_bank(ds, k_keep=400):\n",
        "    bank=[]\n",
        "    for ex in ds:\n",
        "        out = ex[\"output\"]\n",
        "        if (EVENT_TOKEN in out) and (len(out) < 600):\n",
        "            src = ex[\"input\"]\n",
        "            bank.append((src.strip(), out.strip()))\n",
        "    random.shuffle(bank)\n",
        "    return bank[:k_keep]\n",
        "\n",
        "FEWSHOT_BANK = build_fewshot_bank(train_clean, k_keep=400)\n",
        "\n",
        "# Retrieval-based few-shot (TF-IDF); falls back to random if sklearn is missing\n",
        "RETRIEVER = None\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    _FS_TEXTS = [s for s,_ in FEWSHOT_BANK]\n",
        "    _VEC = TfidfVectorizer(ngram_range=(1,2), min_df=2).fit(_FS_TEXTS)\n",
        "    _X   = _VEC.transform(_FS_TEXTS)\n",
        "\n",
        "    def _extract_types(y: str) -> set[str]:\n",
        "        return set(_extract_types_from_target(y))\n",
        "\n",
        "    def retriever_for(query: str, k=3):\n",
        "        q = normalize_input_text(query)\n",
        "        v = _VEC.transform([q])\n",
        "        sims = cosine_similarity(v, _X).ravel()\n",
        "        idx = sims.argsort()[::-1]\n",
        "        chosen, seen_types = [], set()\n",
        "        for i in idx:\n",
        "            s, o = FEWSHOT_BANK[i]\n",
        "            types_i = _extract_types(o)\n",
        "            # encourage type diversity in the shots\n",
        "            if types_i and (seen_types & types_i) == types_i:\n",
        "                continue\n",
        "            chosen.append((s, o))\n",
        "            seen_types |= types_i\n",
        "            if len(chosen) >= k:\n",
        "                break\n",
        "        if not chosen:  # fallback\n",
        "            chosen = [FEWSHOT_BANK[i] for i in idx[:k]]\n",
        "        return chosen\n",
        "\n",
        "    RETRIEVER = retriever_for\n",
        "except Exception:\n",
        "    # No sklearn → fallback to random sampler\n",
        "    def retriever_for(query: str, k=3):\n",
        "        k = min(k, len(FEWSHOT_BANK))\n",
        "        return random.sample(FEWSHOT_BANK, k) if k>0 else []\n",
        "    RETRIEVER = retriever_for\n",
        "\n",
        "# Rebuild the prompted datasets using the cleaned splits + retrieval few-shot\n",
        "def build_prompt(sentence: str, k=3) -> str:\n",
        "    head = (\n",
        "        \"Extract ALL events from the sentence below.\\n\"\n",
        "        f\"Output only lines like: {EVENT_TOKEN} Event type: <TYPE>. Trigger: <TRIGGER>.\\n\"\n",
        "        \"If no events, output exactly: No events.\\n\\n\"\n",
        "    )\n",
        "    shots = RETRIEVER(sentence, k=k) if RETRIEVER else []\n",
        "    if shots:\n",
        "        head += \"### Examples\\n\"\n",
        "        for s, o in shots:\n",
        "            head += f'Sentence: \"{s}\"\\n{o}\\n\\n'\n",
        "    head += \"### Now extract\\n\"\n",
        "    head += f'Sentence: \"{sentence}\"\\nOutput:\\n'\n",
        "    return head\n",
        "\n",
        "def chunk_text_to_sentences(text: str) -> list:\n",
        "    return re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n",
        "\n",
        "def chunk_doc(sentence_or_doc: str, n=3):\n",
        "    sents = chunk_text_to_sentences(sentence_or_doc)\n",
        "    if len(sents) <= n:\n",
        "        return [sentence_or_doc]\n",
        "    chunks=[]\n",
        "    for i in range(0, len(sents), n):\n",
        "        chunks.append(\" \".join(sents[i:i+n]))\n",
        "    return chunks\n",
        "\n",
        "def to_prompted(ds, fewshot_k=3):\n",
        "    def _map(ex):\n",
        "        txt = ex[\"input\"]\n",
        "        if CHUNK_N_SENT:\n",
        "            pieces = chunk_doc(txt, n=CHUNK_N_SENT)\n",
        "            txt = pieces[0]\n",
        "        return {\"input\": build_prompt(txt, k=fewshot_k), \"output\": ex[\"output\"]}\n",
        "    keep_cols = [c for c in ds.column_names if c not in (\"input\",\"output\")]\n",
        "    return ds.map(_map, remove_columns=keep_cols)\n",
        "\n",
        "# final, prompted datasets (cleaned → prompted)\n",
        "train_ds = to_prompted(train_clean, fewshot_k=3)\n",
        "valid_ds = to_prompted(valid_clean, fewshot_k=3)\n",
        "test_ds  = to_prompted(test_clean,  fewshot_k=3)"
      ],
      "metadata": {
        "id": "nUSBlIfMX8Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5) Robust resume: checkpoint extraction & tokenizer load\n",
        "# -----------------------------\n",
        "RESUME_DIR = f\"{OUT_DIR}/checkpoint-7200\"\n",
        "def looks_like_checkpoint_dir(p: str) -> bool:\n",
        "    if not os.path.isdir(p):\n",
        "        return False\n",
        "    files = set(os.listdir(p))\n",
        "    has_model = any(f in files for f in (\n",
        "        \"pytorch_model.bin\", \"pytorch_model.bin.index.json\",\n",
        "        \"model.safetensors\", \"model.safetensors.index.json\"\n",
        "    ))\n",
        "    has_state = \"trainer_state.json\" in files\n",
        "    return has_model and has_state\n",
        "\n",
        "def print_tree(path: str, max_depth: int = 2):\n",
        "    print(f\"\\n>>> Tree under: {path}\")\n",
        "    base_depth = path.rstrip(\"/\").count(\"/\")\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        depth = root.count(\"/\") - base_depth\n",
        "        if depth > max_depth:\n",
        "            continue\n",
        "        show = [f for f in files if f.endswith((\".json\",\".bin\",\".safetensors\",\".pt\",\".pth\"))]\n",
        "        if show:\n",
        "            print(\"  -\", root, \"->\", show)\n",
        "\n",
        "def extract_archive_gently(archive_path: str, dest_dir: str):\n",
        "    if zipfile.is_zipfile(archive_path):\n",
        "        with zipfile.ZipFile(archive_path, \"r\") as zf:\n",
        "            zf.extractall(dest_dir)\n",
        "        print(f\"[ok] Extracted zip into: {dest_dir}\")\n",
        "        return True\n",
        "    try:\n",
        "        if tarfile.is_tarfile(archive_path):\n",
        "            with tarfile.open(archive_path, \"r:*\") as tf:\n",
        "                tf.extractall(dest_dir)\n",
        "            print(f\"[ok] Extracted tar into: {dest_dir}\")\n",
        "            return True\n",
        "    except tarfile.TarError:\n",
        "        pass\n",
        "    print(f\"[warn] '{archive_path}' is not a valid zip/tar archive.\")\n",
        "    return False\n",
        "\n",
        "\n",
        "archive_candidates = (\n",
        "    glob.glob(os.path.join(OUT_DIR, \"checkpoint-7200*\")) +\n",
        "    glob.glob(\"/content/checkpoint-7200*\")\n",
        ")\n",
        "archive_candidates = [p for p in archive_candidates if os.path.isfile(p) and not p.endswith(\".ipynb\")]\n",
        "archive_candidates.sort()\n",
        "if archive_candidates and not looks_like_checkpoint_dir(RESUME_DIR):\n",
        "    candidate = archive_candidates[-1]\n",
        "    print(f\"[info] Found checkpoint archive: {candidate}\")\n",
        "    extract_archive_gently(candidate, OUT_DIR)\n",
        "\n",
        "\n",
        "if not looks_like_checkpoint_dir(RESUME_DIR):\n",
        "    alts = [d for d in glob.glob(os.path.join(OUT_DIR, \"checkpoint-7200*\")) if os.path.isdir(d)]\n",
        "    valid_alts = [d for d in alts if looks_like_checkpoint_dir(d)]\n",
        "    if valid_alts:\n",
        "        src = valid_alts[0]\n",
        "        if src != RESUME_DIR:\n",
        "            if os.path.exists(RESUME_DIR): shutil.rmtree(RESUME_DIR)\n",
        "            os.rename(src, RESUME_DIR)\n",
        "            print(f\"[fix] Renamed '{src}' -> '{RESUME_DIR}'\")\n",
        "\n",
        "if not looks_like_checkpoint_dir(RESUME_DIR):\n",
        "    needed = [\n",
        "        \"config.json\", \"trainer_state.json\",\n",
        "        \"pytorch_model.bin\", \"pytorch_model.bin.index.json\",\n",
        "        \"model.safetensors\", \"model.safetensors.index.json\",\n",
        "        \"optimizer.pt\", \"scheduler.pt\", \"rng_state.pth\", \"scaler.pt\",\n",
        "        \"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\",\n",
        "        \"added_tokens.json\", \"generation_config.json\", \"training_args.bin\"\n",
        "    ]\n",
        "    present = [f for f in needed if os.path.exists(os.path.join(OUT_DIR, f))]\n",
        "    if present:\n",
        "        print(f\"[fix] Creating '{RESUME_DIR}' and moving loose files into it…\")\n",
        "        os.makedirs(RESUME_DIR, exist_ok=True)\n",
        "        for f in present:\n",
        "            src = os.path.join(OUT_DIR, f)\n",
        "            if os.path.exists(src):\n",
        "                shutil.move(src, os.path.join(RESUME_DIR, f))\n",
        "\n",
        "print_tree(OUT_DIR, max_depth=2)\n",
        "\n",
        "# Load tokenizer (from checkpoint if present, else base)\n",
        "try:\n",
        "    if looks_like_checkpoint_dir(RESUME_DIR):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(RESUME_DIR)\n",
        "        print(\"[tok] Loaded tokenizer from checkpoint.\")\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "        print(\"[tok] Loaded tokenizer from base model.\")\n",
        "except Exception as e:\n",
        "    print(f\"[tok] Fallback to base tokenizer due to: {e}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# Ensure EVENT_TOKEN exists (keeps vocab consistent if it was in the checkpoint)\n",
        "_ = tokenizer.add_special_tokens({\"additional_special_tokens\":[EVENT_TOKEN]})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7r33SOyX_gm",
        "outputId": "e30b8a2d-1b10-4dcd-cbd8-9c797afe9571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] Found checkpoint archive: /content/checkpoint-7200_20250910-125541.zip\n",
            "[ok] Extracted zip into: /content/flan_t5_degree2_ckpt\n",
            "[fix] Creating '/content/flan_t5_degree2_ckpt/checkpoint-7200' and moving loose files into it…\n",
            "\n",
            ">>> Tree under: /content/flan_t5_degree2_ckpt\n",
            "  - /content/flan_t5_degree2_ckpt/checkpoint-7200 -> ['added_tokens.json', 'model.safetensors', 'config.json', 'optimizer.pt', 'training_args.bin', 'generation_config.json', 'special_tokens_map.json', 'scheduler.pt', 'rng_state.pth', 'tokenizer.json', 'tokenizer_config.json', 'trainer_state.json']\n",
            "[tok] Loaded tokenizer from checkpoint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6) Save helpers + StopAtStep (same)\n",
        "# -----------------------------\n",
        "def get_latest_ckpt_path(out_dir):\n",
        "    try:\n",
        "        return get_last_checkpoint(out_dir)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def save_final_snapshot(trainer, tokenizer, base_dir, tag=None):\n",
        "    step = int(trainer.state.global_step) if trainer and trainer.state else 0\n",
        "    tag  = tag or f\"final-{step}\"\n",
        "    snap_dir = os.path.join(base_dir, f\"checkpoint-{tag}\")\n",
        "    os.makedirs(snap_dir, exist_ok=True)\n",
        "    trainer.save_model(snap_dir)\n",
        "    if tokenizer is not None:\n",
        "        tokenizer.save_pretrained(snap_dir)\n",
        "    with open(os.path.join(base_dir, \"LATEST.txt\"), \"w\") as f:\n",
        "        f.write(snap_dir)\n",
        "    print(f\"[Snapshot] Saved '{snap_dir}' and updated LATEST.txt\")\n",
        "    return snap_dir\n",
        "\n",
        "class StopAtStepCallback(TrainerCallback):\n",
        "    def __init__(self, target_step=None, out_dir=None, tokenizer=None):\n",
        "        self.target_step = target_step\n",
        "        self.out_dir = out_dir\n",
        "        self.tokenizer = tokenizer\n",
        "    def _read_target(self):\n",
        "        env_val = os.getenv(\"STOP_AT_STEP\", \"\").strip()\n",
        "        if env_val.isdigit():\n",
        "            return int(env_val)\n",
        "        fpath = \"/content/stop_at_step.txt\"\n",
        "        if os.path.exists(fpath):\n",
        "            try:\n",
        "                with open(fpath) as f:\n",
        "                    val = f.read().strip()\n",
        "                if val.isdigit():\n",
        "                    return int(val)\n",
        "            except:\n",
        "                pass\n",
        "        return self.target_step\n",
        "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        tgt = self._read_target()\n",
        "        if tgt is None: return\n",
        "        if state.global_step >= tgt:\n",
        "            trainer = kwargs.get(\"trainer\", None)\n",
        "            print(f\"\\n[StopAtStep] Reached step {state.global_step} (target={tgt}). Saving and stopping...\")\n",
        "            if trainer is not None and self.out_dir is not None:\n",
        "                save_final_snapshot(trainer, self.tokenizer, self.out_dir, tag=f\"stop-{state.global_step}\")\n",
        "            control.should_training_stop = True\n",
        "            control.should_save = True\n",
        "            return control"
      ],
      "metadata": {
        "id": "6cnunLy7YGee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 7) Load model (resume if possible; else fresh)\n",
        "# -----------------------------\n",
        "if looks_like_checkpoint_dir(RESUME_DIR):\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(RESUME_DIR)\n",
        "    print(f\"[model] Loaded from checkpoint: {RESUME_DIR}\")\n",
        "else:\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
        "    print(f\"[model] Loaded fresh from base: {BASE_MODEL}\")\n",
        "\n",
        "# Expand embeddings if we added special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Gradient checkpointing parity with Code A\n",
        "try:\n",
        "    model.gradient_checkpointing_enable()\n",
        "    if getattr(model.config, \"use_cache\", None):\n",
        "        model.config.use_cache = False\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8knyuW0MYHtm",
        "outputId": "8b435822-672f-48ac-a9cc-30b974ed11fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model] Loaded from checkpoint: /content/flan_t5_degree2_ckpt/checkpoint-7200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 8) TrainingArguments (EXACT Code A hyper-params)\n",
        "# -----------------------------\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=800,\n",
        "    save_steps=800,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    num_train_epochs=10,\n",
        "    max_steps=-1,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-5,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    optim=\"adafactor\",\n",
        "\n",
        "    label_smoothing_factor=0.1,\n",
        "    max_grad_norm=1.0,\n",
        "    predict_with_generate=False,\n",
        "\n",
        "    fp16=False,\n",
        "    bf16=use_bf16,\n",
        "    report_to=\"none\",\n",
        "    logging_steps=100,\n",
        "    save_safetensors=True,\n",
        "    seed=SEED, data_seed=SEED,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "l4ADL3MrYI1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 9) Tokenize (text_target) → Collator (with model) → Trainer → Train/Resume\n",
        "# -----------------------------\n",
        "def preprocess(batch):\n",
        "    enc = tokenizer(\n",
        "        batch[\"input\"],\n",
        "        max_length=MAX_IN_LEN, truncation=True, padding=\"max_length\"\n",
        "    )\n",
        "    tgt = tokenizer(\n",
        "        text_target=batch[\"output\"],\n",
        "        max_length=MAX_OUT_LEN, truncation=True, padding=\"max_length\"\n",
        "    )\n",
        "    enc[\"labels\"] = tgt[\"input_ids\"]\n",
        "    return enc\n",
        "\n",
        "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
        "valid_tok = valid_ds.map(preprocess, batched=True, remove_columns=valid_ds.column_names)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=\"longest\",\n",
        "    label_pad_token_id=-100\n",
        ")\n",
        "\n",
        "stop_cb = StopAtStepCallback(target_step=None, out_dir=OUT_DIR, tokenizer=tokenizer)\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3), stop_cb]\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=valid_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Optional auto-stop (delete to disable)\n",
        "os.environ[\"STOP_AT_STEP\"] = \"20000\"\n",
        "\n",
        "resume_path = RESUME_DIR if looks_like_checkpoint_dir(RESUME_DIR) else None\n",
        "if resume_path:\n",
        "    print(\"Resuming EXACTLY from:\", resume_path)\n",
        "    # If you previously switched optim/scheduler types, you can delete optimizer/scheduler state here.\n",
        "    trainer.train(resume_from_checkpoint=resume_path)\n",
        "else:\n",
        "    print(\"Starting fresh training…\")\n",
        "    trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767,
          "referenced_widgets": [
            "e03f3c7d69264457a26f5bd8ea054b9f",
            "d8e85d11d5a945f4925687585a440769",
            "7226de859e964251a091a8040dccde1d",
            "b8a699ae55d94508b7ef4f8b8f6abdf0",
            "1908c9a87b7f4d4f91788bb73ae41bab",
            "aaf9d30bc0164c6e8a722e301d601026",
            "0d14a78af01f4c28b8b2b353c90816aa",
            "a9358ffa5c0a428c9a24104c1da8251b",
            "6d8a0da559554f419016cf635902d79d",
            "4c836b8fde7344b6a30e6029c4da854d",
            "dcc32f8b17a8479bb7646e94de155939",
            "2e6e08900d984034b9d1261052560eb4",
            "96224c23afa94221bed9a525f7823012",
            "ab9991598cee47b2a4efdde5e36e6076",
            "49955636c2354111961af5a9f1ec98f5",
            "c94087d9f232474f953f1d4a64a841ad",
            "2b2e5f29a0db4ed4ab3bc3e139b88d12",
            "6b3bc9fc83dc467bac523f81b98d39de",
            "52536b7a24f54e40a4989b348442f4be",
            "e92efa28d8ba41048fdfa384fc03a7e5",
            "0d0c4d762aef4ef9b4d3202562fea557",
            "f943550ee4834634871e140622282a70"
          ]
        },
        "id": "xx5HTJz8YKDe",
        "outputId": "49f90708-4376-40ed-c13f-d6438482f2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/32430 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e03f3c7d69264457a26f5bd8ea054b9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8015 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e6e08900d984034b9d1261052560eb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2282032513.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming EXACTLY from: /content/flan_t5_degree2_ckpt/checkpoint-7200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12803' max='40540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12803/40540 4:30:40 < 22:20:24, 0.34 it/s, Epoch 3.16/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.399400</td>\n",
              "      <td>1.389901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>1.398300</td>\n",
              "      <td>1.389069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>1.398600</td>\n",
              "      <td>1.389795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>1.396000</td>\n",
              "      <td>1.388636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>1.396900</td>\n",
              "      <td>1.388947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>1.394800</td>\n",
              "      <td>1.387686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>1.392000</td>\n",
              "      <td>1.387259</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2282032513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resuming EXACTLY from:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# If you previously switched optim/scheduler types, you can delete optimizer/scheduler state here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting fresh training…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     )\n\u001b[1;32m   2671\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4058\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4060\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4062\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 10) Save final + zip best (robust)\n",
        "# -----------------------------\n",
        "def _is_valid_checkpoint_dir(p: str) -> bool:\n",
        "    if not os.path.isdir(p): return False\n",
        "    files = set(os.listdir(p))\n",
        "    has_model = any(f in files for f in (\n",
        "        \"pytorch_model.bin\", \"pytorch_model.bin.index.json\",\n",
        "        \"model.safetensors\", \"model.safetensors.index.json\"\n",
        "    ))\n",
        "    has_state = \"trainer_state.json\" in files\n",
        "    return has_model and has_state\n",
        "\n",
        "def _safe_zip_dir(src_dir: str, out_prefix: str) -> str:\n",
        "    stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    base = f\"/content/{out_prefix}_{stamp}\"\n",
        "    zip_path = shutil.make_archive(base, \"zip\", src_dir)\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            bad = zf.testzip()\n",
        "        if bad is not None:\n",
        "            print(f\"[warn] Zip integrity issue with file: {bad}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"[warn] Created file is not recognized as zip (BadZipFile).\")\n",
        "    size_gb = os.path.getsize(zip_path) / (1024**3)\n",
        "    print(f\"[zip] {zip_path}  (~{size_gb:.2f} GB)\")\n",
        "    return zip_path\n",
        "\n",
        "final_snap = save_final_snapshot(trainer, tokenizer, OUT_DIR)\n",
        "os.makedirs(FINAL_DIR, exist_ok=True)\n",
        "trainer.save_model(FINAL_DIR); tokenizer.save_pretrained(FINAL_DIR)\n",
        "print(\"Saved best model to:\", FINAL_DIR)\n",
        "\n",
        "def find_best_checkpoint(out_dir: str) -> str | None:\n",
        "    state_path = os.path.join(out_dir, \"trainer_state.json\")\n",
        "    best = None\n",
        "    if os.path.exists(state_path):\n",
        "        try:\n",
        "            with open(state_path, \"r\") as f:\n",
        "                st = json.load(f)\n",
        "            cand = st.get(\"best_model_checkpoint\", None)\n",
        "            if cand and os.path.exists(cand) and _is_valid_checkpoint_dir(cand):\n",
        "                best = cand\n",
        "            if best is None:\n",
        "                for rec in reversed(st.get(\"log_history\", [])):\n",
        "                    if isinstance(rec, dict) and \"best_model_checkpoint\" in rec:\n",
        "                        cand = rec[\"best_model_checkpoint\"]\n",
        "                        if cand and os.path.exists(cand) and _is_valid_checkpoint_dir(cand):\n",
        "                            best = cand; break\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] Could not parse trainer_state.json ({e}).\")\n",
        "    if best is None:\n",
        "        try:\n",
        "            latest = get_last_checkpoint(out_dir)\n",
        "            if latest and _is_valid_checkpoint_dir(latest):\n",
        "                best = latest\n",
        "        except Exception:\n",
        "            pass\n",
        "    if best is None and final_snap and _is_valid_checkpoint_dir(final_snap):\n",
        "        best = final_snap\n",
        "    return best\n",
        "\n",
        "def pick_checkpoint_to_zip(out_dir: str) -> str:\n",
        "    ckpt = find_best_checkpoint(out_dir)\n",
        "    if ckpt is None:\n",
        "        raise FileNotFoundError(\"No valid checkpoint found to zip.\")\n",
        "    print(f\"[info] Best checkpoint selected: {ckpt}\")\n",
        "    return ckpt\n",
        "\n",
        "try:\n",
        "    ckpt_dir = pick_checkpoint_to_zip(OUT_DIR)\n",
        "    zip_path = _safe_zip_dir(ckpt_dir, out_prefix=Path(ckpt_dir).name)\n",
        "    if files is not None:\n",
        "        files.download(zip_path)\n",
        "except Exception as e:\n",
        "    print(\"[zip-best] skip:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "N0r1DqkeYLzY",
        "outputId": "865a2fef-6f66-4b29-c721-2462e868d443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Snapshot] Saved '/content/flan_t5_degree2_ckpt/checkpoint-final-12802' and updated LATEST.txt\n",
            "Saved best model to: /content/flan_t5_degree2_final\n",
            "[info] Best checkpoint selected: /content/flan_t5_degree2_ckpt/checkpoint-12800\n",
            "[zip] /content/checkpoint-12800_20250911-104520.zip  (~0.86 GB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0713bf61-b0b8-4a63-95c9-48b3112d7450\", \"checkpoint-12800_20250911-104520.zip\", 921423580)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 11) Inference & Evaluation (beam or consensus + smart postproc)\n",
        "# -----------------------------\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ---------- Smart post-processing (optional) ----------\n",
        "_STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"for\",\"from\",\"by\",\"with\",\n",
        "    \"and\",\"or\",\"but\",\"if\",\"as\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\"\n",
        "}\n",
        "\n",
        "def _basic_lemma(word: str) -> str:\n",
        "    w = word\n",
        "    for suf in (\"'s\",\"’s\"):\n",
        "        if w.endswith(suf) and len(w) > len(suf)+1:\n",
        "            w = w[:-len(suf)]\n",
        "    for suf in (\"ing\",\"ed\",\"es\",\"s\"):\n",
        "        if w.endswith(suf) and len(w) > len(suf)+1:\n",
        "            w = w[:-len(suf)]\n",
        "            break\n",
        "    return w\n",
        "\n",
        "def normalize_trigger_str(tr: str) -> str:\n",
        "    if not tr:\n",
        "        return \"\"\n",
        "    tr = tr.strip()\n",
        "    tr = tr.replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\").replace(\"‘\",\"'\").replace(\"’\",\"'\")\n",
        "    tr = tr.replace(\"–\",\"-\").replace(\"—\",\"-\")\n",
        "    tr = tr.strip(\" '\\\"`\")\n",
        "    tr = tr.lower()\n",
        "    tr = re.sub(r\"[^\\w\\s\\-/]\", \" \", tr)   # keep letters/digits/space/- and /\n",
        "    tr = re.sub(r\"\\s+\", \" \", tr).strip()\n",
        "    if len(tr) <= 1:\n",
        "        return \"\"\n",
        "    toks = [t for t in tr.split() if t not in _STOPWORDS]\n",
        "    toks = [_basic_lemma(t) for t in toks]\n",
        "    tr2 = \" \".join(toks).strip() or tr\n",
        "    return tr2\n",
        "\n",
        "def _token_jaccard(a: str, b: str) -> float:\n",
        "    A, B = set(a.split()), set(b.split())\n",
        "    if not A or not B:\n",
        "        return 0.0\n",
        "    inter = len(A & B); union = len(A | B)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "def _char_sim(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def _near_duplicate(tr1: str, tr2: str, j_th=0.60, c_th=0.80) -> bool:\n",
        "    if not tr1 or not tr2:\n",
        "        return False\n",
        "    return (_token_jaccard(tr1, tr2) >= j_th) or (_char_sim(tr1, tr2) >= c_th)\n",
        "\n",
        "def merge_near_duplicates(pairs):\n",
        "    \"\"\"pairs: list[(etype, trigger)] → merge near-duplicate triggers per type (keep shorter).\"\"\"\n",
        "    kept = []\n",
        "    for et, tr in pairs:\n",
        "        merged = False\n",
        "        for i, (et2, tr2) in enumerate(kept):\n",
        "            if et == et2 and _near_duplicate(tr, tr2):\n",
        "                rep = tr if (0 < len(tr) <= len(tr2)) else tr2\n",
        "                kept[i] = (et2, rep)\n",
        "                merged = True\n",
        "                break\n",
        "        if not merged:\n",
        "            kept.append((et, tr))\n",
        "    return kept\n",
        "\n",
        "# ---------- Canonicalizers ----------\n",
        "def clean_and_canon_basic(text: str) -> str:\n",
        "    text = dedup_events_str(text)\n",
        "    parts=[]\n",
        "    for ch in [c for c in EVENT_SEP.split(text) if c.strip()]:\n",
        "        t = TYPE_RE.search(ch); g = TRIG_RE.search(ch)\n",
        "        et = norm(t.group(1)) if t else None\n",
        "        tr = norm(g.group(1)) if g else None\n",
        "        if et:\n",
        "            et = nearest_type(et)\n",
        "        if et and tr:\n",
        "            parts.append(f\"Event type: {et}. Trigger: {tr}.\")\n",
        "    return f\" {EVENT_TOKEN} \".join(parts) if parts else \"No events.\"\n",
        "\n",
        "def clean_and_canon_smart(text: str) -> str:\n",
        "    \"\"\"Parse -> normalize triggers -> nearest_type -> filter -> dedup-near -> rebuild.\"\"\"\n",
        "    raw = parse_pairs(text)\n",
        "    norm_pairs = []\n",
        "    for et, tr in raw:\n",
        "        et_norm = nearest_type(et) if et else et\n",
        "        tr_norm = normalize_trigger_str(tr)\n",
        "        if not tr_norm:\n",
        "            continue\n",
        "        if tr_norm.isdigit():\n",
        "            continue\n",
        "        # At least 2 alnum chars\n",
        "        if len(re.sub(r\"[^a-z0-9]+\", \"\", tr_norm)) < 2:\n",
        "            continue\n",
        "        norm_pairs.append((et_norm, tr_norm))\n",
        "    if not norm_pairs:\n",
        "        return \"No events.\"\n",
        "    merged = merge_near_duplicates(norm_pairs)\n",
        "    seen = set(); final=[]\n",
        "    for et,tr in merged:\n",
        "        key=(et,tr)\n",
        "        if key not in seen:\n",
        "            seen.add(key); final.append((et,tr))\n",
        "    if not final:\n",
        "        return \"No events.\"\n",
        "    return f\" {EVENT_TOKEN} \".join([f\"Event type: {et}. Trigger: {tr}.\" for et,tr in final])\n",
        "\n",
        "# ---------- Generator (beam or consensus sampling) ----------\n",
        "def generate_batch(\n",
        "    prompts: List[str],\n",
        "    mdl,\n",
        "    tok,\n",
        "    bs: int = 8,\n",
        "    device: str | None = None,\n",
        "    strategy: str = \"beam\",          # \"beam\" | \"consensus\"\n",
        "    samples: int = 3,                 # used when strategy=\"consensus\"\n",
        "    top_p: float = 0.9,\n",
        "    temperature: float = 0.7,\n",
        "    max_new_tokens: int = 160,\n",
        "    min_new_tokens: int = 12,\n",
        "    no_repeat_ngram_size: int = 3,\n",
        "    repetition_penalty: float = 1.05,\n",
        "    postproc: str = \"basic\"           # \"basic\" | \"smart\"\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mdl.eval().to(device)\n",
        "\n",
        "    canon = clean_and_canon_smart if postproc == \"smart\" else clean_and_canon_basic\n",
        "    outs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), bs):\n",
        "            batch_prompts = prompts[i:i+bs]\n",
        "            enc = tok(\n",
        "                batch_prompts, return_tensors=\"pt\",\n",
        "                padding=True, truncation=True, max_length=MAX_IN_LEN\n",
        "            ).to(device)\n",
        "\n",
        "            if strategy == \"beam\":\n",
        "                gen_kwargs = dict(\n",
        "                    max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens,\n",
        "                    num_beams=6, num_beam_groups=3, diversity_penalty=0.2,\n",
        "                    no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "                    length_penalty=0.9, repetition_penalty=repetition_penalty,\n",
        "                    early_stopping=False, trust_remote_code=True\n",
        "                )\n",
        "                gen = mdl.generate(**enc, **gen_kwargs)\n",
        "                dec = tok.batch_decode(gen, skip_special_tokens=True)\n",
        "                outs += [canon(t) for t in dec]\n",
        "\n",
        "            else:  # consensus sampling\n",
        "                all_decoded = []\n",
        "                for _ in range(samples):\n",
        "                    gen_kwargs = dict(\n",
        "                        do_sample=True, top_p=top_p, temperature=temperature,\n",
        "                        max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens,\n",
        "                        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "                        repetition_penalty=repetition_penalty,\n",
        "                        early_stopping=False, trust_remote_code=True\n",
        "                    )\n",
        "                    gen = mdl.generate(**enc, **gen_kwargs)\n",
        "                    dec = tok.batch_decode(gen, skip_special_tokens=True)\n",
        "                    all_decoded.append(dec)\n",
        "\n",
        "                # merge K samples per input, then canonicalize\n",
        "                for k in range(len(batch_prompts)):\n",
        "                    variants = [all_decoded[s][k] for s in range(samples)]\n",
        "                    merged_text = f\" {EVENT_TOKEN} \".join(variants)\n",
        "                    outs.append(canon(merged_text))\n",
        "\n",
        "    return outs\n",
        "\n",
        "# ---------- Metrics (unchanged) ----------\n",
        "def match_partial(preds, golds):\n",
        "    used=set(); exact=partial=0\n",
        "    for pt,ptr in preds:\n",
        "        best=(0.0,-1)\n",
        "        for j,(gt,gtr) in enumerate(golds):\n",
        "            if j in used: continue\n",
        "            if pt!=gt:   continue\n",
        "            ov = trigger_overlap(ptr,gtr)\n",
        "            if ov>best[0]: best=(ov,j)\n",
        "        if best[1]!=-1:\n",
        "            used.add(best[1])\n",
        "            if math.isclose(best[0],1.0): exact+=1\n",
        "            elif best[0]>=0.5: partial+=1\n",
        "    return exact, partial, len(preds), len(golds)\n",
        "\n",
        "def prf(e,p,pt,gt, w=0.5):\n",
        "    wtp = e + w*p\n",
        "    P = wtp/pt if pt else 0.0\n",
        "    R = wtp/gt if gt else 0.0\n",
        "    F = (2*P*R)/(P+R) if (P+R) else 0.0\n",
        "    return P,R,F\n",
        "\n",
        "def relaxed_recall_by_chunks(pairs_pred, pairs_gold, chunk_size=1):\n",
        "    correct=partial=extra=possible=impossible=0\n",
        "    for pp,gg in zip(pairs_pred, pairs_gold):\n",
        "        ce,cp,pt,gt = match_partial(pp,gg)\n",
        "        matched = ce + 0.5*cp\n",
        "        possible += gt\n",
        "        impossible += max(0, gt-1)\n",
        "        extra += max(0.0, matched-1.0)\n",
        "        correct += ce; partial += cp\n",
        "    denom = max(1, possible - impossible)\n",
        "    num   = max(0.0, (correct + 0.5*partial) - extra)\n",
        "    return num/denom\n",
        "\n",
        "def evaluate(\n",
        "    ds, mdl, tok,\n",
        "    strategy: str = \"beam\",      # \"beam\" | \"consensus\"\n",
        "    postproc: str = \"basic\",     # \"basic\" | \"smart\"\n",
        "    samples: int = 3,\n",
        "    bs: int = 8,\n",
        "    top_p: float = 0.9,\n",
        "    temperature: float = 0.7\n",
        "):\n",
        "    prompts = [ex[\"input\"] for ex in ds]\n",
        "    gtexts  = [ex[\"output\"] for ex in ds]\n",
        "    preds   = generate_batch(\n",
        "        prompts, mdl, tok, bs=bs,\n",
        "        strategy=strategy, postproc=postproc,\n",
        "        samples=samples, top_p=top_p, temperature=temperature\n",
        "    )\n",
        "\n",
        "    strict_tp=strict_pred=strict_gold=0\n",
        "    part_e=part_p=part_pt=part_gt=0\n",
        "    chunks_pred=[]; chunks_gold=[]\n",
        "\n",
        "    for ptxt,gtxt in zip(preds, gtexts):\n",
        "        pp = parse_pairs(ptxt)\n",
        "        gg = parse_pairs(gtxt)\n",
        "        sp,sg = set(pp), set(gg)\n",
        "        tp = len(sp & sg)\n",
        "        strict_tp += tp; strict_pred += len(sp); strict_gold += len(sg)\n",
        "        ce,cp,pt,gt = match_partial(pp,gg)\n",
        "        part_e += ce; part_p += cp; part_pt += pt; part_gt += gt\n",
        "        chunks_pred.append(pp); chunks_gold.append(gg)\n",
        "\n",
        "    sP = strict_tp/strict_pred if strict_pred else 0.0\n",
        "    sR = strict_tp/strict_gold if strict_gold else 0.0\n",
        "    sF = (2*sP*sR)/(sP+sR) if (sP+sR) else 0.0\n",
        "\n",
        "    pP,pR,pF = prf(part_e,part_p,part_pt,part_gt, w=0.5)\n",
        "    r_rel = relaxed_recall_by_chunks(chunks_pred, chunks_gold)\n",
        "\n",
        "    print(\"\\n===== STRICT =====\")\n",
        "    print(f\"P={sP:.4f} R={sR:.4f} F1={sF:.4f}\")\n",
        "    print(\"===== PARTIAL (MUC 0.5) =====\")\n",
        "    print(f\"P={pP:.4f} R={pR:.4f} F1={pF:.4f}\")\n",
        "    print(\"===== RELAXED (DEGREE2) =====\")\n",
        "    rF = (2*pP*r_rel)/(pP+r_rel) if (pP+r_rel)>0 else 0.0\n",
        "    print(f\"Relaxed-Recall={r_rel:.4f} | Relaxed-F1≈{rF:.4f}\")\n",
        "    return dict(strict_f1=sF, partial_f1=pF, relaxed_recall=r_rel, relaxed_f1=rF)\n",
        "\n",
        "# ---------- Quick sanity + Eval with FINAL_DIR ----------\n",
        "best_tok   = AutoTokenizer.from_pretrained(FINAL_DIR)\n",
        "best_model = AutoModelForSeq2SeqLM.from_pretrained(FINAL_DIR)\n",
        "\n",
        "print(\"\\n--- VALID (beam + basic) ---\")\n",
        "evaluate(valid_ds, best_model, best_tok, strategy=\"beam\", postproc=\"basic\", bs=8)\n",
        "\n",
        "print(\"\\n--- TEST  (beam + basic) ---\")\n",
        "evaluate(test_ds,  best_model, best_tok, strategy=\"beam\", postproc=\"basic\", bs=8)\n",
        "\n",
        "# To try recall-oriented mode:\n",
        "# print(\"\\n--- VALID (consensus + smart) ---\")\n",
        "# evaluate(valid_ds, best_model, best_tok, strategy=\"consensus\", postproc=\"smart\",samples=3, bs=16, top_p=0.9, temperature=0.7)\n",
        "\n",
        "demo_sent = (\n",
        "    \"The hijacking of Lufthansa Flight 615 was an act of terrorism committed by a Palestinian group \"\n",
        "    \"that occurred on 29 October 1972 and aimed at the liberation of the three surviving perpetrators \"\n",
        "    \"of the Munich massacre from a West German prison.\"\n",
        ")\n",
        "demo_prompt = build_prompt(demo_sent, k=3)\n",
        "print(\"\\nDEMO (beam+basic):\\n\", generate_batch([demo_prompt], best_model, best_tok, strategy=\"beam\", postproc=\"basic\", bs=1)[0])\n",
        "# print(\"\\nDEMO (consensus+smart):\\n\", generate_batch([demo_prompt], best_model, best_tok, strategy=\"consensus\", postproc=\"smart\", samples=3, bs=1)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ngY0zK4YNB_",
        "outputId": "496d053a-281b-426e-ed7a-a40acc8ef98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- VALID (beam + basic) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== STRICT =====\n",
            "P=0.5966 R=0.2746 F1=0.3761\n",
            "===== PARTIAL (MUC 0.5) =====\n",
            "P=0.5986 R=0.2756 F1=0.3774\n",
            "===== RELAXED (DEGREE2) =====\n",
            "Relaxed-Recall=0.6490 | Relaxed-F1≈0.6228\n",
            "\n",
            "--- TEST  (beam + basic) ---\n",
            "\n",
            "===== STRICT =====\n",
            "P=0.6019 R=0.1379 F1=0.2244\n",
            "===== PARTIAL (MUC 0.5) =====\n",
            "P=0.6037 R=0.1383 F1=0.2251\n",
            "===== RELAXED (DEGREE2) =====\n",
            "Relaxed-Recall=0.6127 | Relaxed-F1≈0.6082\n",
            "\n",
            "DEMO (beam+basic):\n",
            " Event type: coming_to_be. Trigger: occurred.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 11) Inference & Evaluation (hybrid beam+consensus with soft-smart postproc + reranker)\n",
        "# -----------------------------\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ======== Soft-smart post-processing ========\n",
        "_STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"for\",\"from\",\"by\",\"with\",\n",
        "    \"and\",\"or\",\"but\",\"if\",\"as\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\"\n",
        "}\n",
        "\n",
        "def _soft_stem(word: str) -> str:\n",
        "    \"\"\"Very light stemming; avoid over-truncating (keeps >=4 chars if possible).\"\"\"\n",
        "    w = word\n",
        "    for suf in (\"'s\",\"’s\"):\n",
        "        if w.endswith(suf) and len(w) > len(suf)+1:\n",
        "            w = w[:-len(suf)]\n",
        "    for suf in (\"ing\",\"ed\",\"es\",\"s\"):\n",
        "        if w.endswith(suf) and len(w) >= 5:  # <-- guard to prevent occurr/committ\n",
        "            w = w[:-len(suf)]\n",
        "            break\n",
        "    return w\n",
        "\n",
        "def normalize_trigger_str(tr: str) -> str:\n",
        "    if not tr:\n",
        "        return \"\"\n",
        "    tr = tr.strip()\n",
        "    tr = (tr.replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\")\n",
        "            .replace(\"‘\",\"'\").replace(\"’\",\"'\")\n",
        "            .replace(\"–\",\"-\").replace(\"—\",\"-\"))\n",
        "    tr = tr.strip(\" '\\\"`\").lower()\n",
        "    tr = re.sub(r\"[^\\w\\s\\-/]\", \" \", tr)\n",
        "    tr = re.sub(r\"\\s+\", \" \", tr).strip()\n",
        "    if len(tr) <= 1:\n",
        "        return \"\"\n",
        "    toks = [t for t in tr.split() if t not in _STOPWORDS]\n",
        "    toks = [_soft_stem(t) for t in toks]\n",
        "    tr2 = \" \".join(toks).strip() or tr\n",
        "    return tr2\n",
        "\n",
        "def _token_jaccard(a: str, b: str) -> float:\n",
        "    A, B = set(a.split()), set(b.split())\n",
        "    if not A or not B:\n",
        "        return 0.0\n",
        "    inter = len(A & B); union = len(A | B)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "def _char_sim(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def _near_duplicate(tr1: str, tr2: str, j_th=0.55, c_th=0.78) -> bool:\n",
        "    if not tr1 or not tr2:\n",
        "        return False\n",
        "    return (_token_jaccard(tr1, tr2) >= j_th) or (_char_sim(tr1, tr2) >= c_th)\n",
        "\n",
        "def merge_near_duplicates(pairs):\n",
        "    \"\"\"pairs: list[(etype, trigger)] → merge near-duplicate triggers per type (keep better-shaped/shorter).\"\"\"\n",
        "    kept = []\n",
        "    for et, tr in pairs:\n",
        "        merged = False\n",
        "        for i, (et2, tr2) in enumerate(kept):\n",
        "            if et == et2 and _near_duplicate(tr, tr2):\n",
        "                # choose representative by: longer >=4 preferred (readability) but avoid multiword bloat\n",
        "                def _score_shape(s):\n",
        "                    letters = len(re.sub(r\"[^a-z0-9]+\",\"\", s))\n",
        "                    tokens  = len(s.split())\n",
        "                    return letters - 0.5 * max(0, tokens-1)\n",
        "                rep = tr if _score_shape(tr) >= _score_shape(tr2) else tr2\n",
        "                kept[i] = (et2, rep)\n",
        "                merged = True\n",
        "                break\n",
        "        if not merged:\n",
        "            kept.append((et, tr))\n",
        "    return kept\n",
        "\n",
        "# Two canonicalizers: basic (Code A parity) and soft-smart\n",
        "def clean_and_canon_basic(text: str) -> str:\n",
        "    text = dedup_events_str(text)\n",
        "    parts=[]\n",
        "    for ch in [c for c in EVENT_SEP.split(text) if c.strip()]:\n",
        "        t = TYPE_RE.search(ch); g = TRIG_RE.search(ch)\n",
        "        et = norm(t.group(1)) if t else None\n",
        "        tr = norm(g.group(1)) if g else None\n",
        "        if et:\n",
        "            et = nearest_type(et)\n",
        "        if et and tr:\n",
        "            parts.append(f\"Event type: {et}. Trigger: {tr}.\")\n",
        "    return f\" {EVENT_TOKEN} \".join(parts) if parts else \"No events.\"\n",
        "\n",
        "def clean_and_canon_softsmart(text: str) -> str:\n",
        "    raw = parse_pairs(text)\n",
        "    norm_pairs = []\n",
        "    for et, tr in raw:\n",
        "        et_norm = nearest_type(et) if et else et\n",
        "        tr_norm = normalize_trigger_str(tr)\n",
        "        if not tr_norm:\n",
        "            continue\n",
        "        if tr_norm.isdigit():\n",
        "            continue\n",
        "        if len(re.sub(r\"[^a-z0-9]+\", \"\", tr_norm)) < 2:\n",
        "            continue\n",
        "        norm_pairs.append((et_norm, tr_norm))\n",
        "    if not norm_pairs:\n",
        "        return \"No events.\"\n",
        "    merged = merge_near_duplicates(norm_pairs)\n",
        "    seen = set(); final=[]\n",
        "    for et,tr in merged:\n",
        "        key=(et,tr)\n",
        "        if key not in seen:\n",
        "            seen.add(key); final.append((et,tr))\n",
        "    if not final:\n",
        "        return \"No events.\"\n",
        "    return f\" {EVENT_TOKEN} \".join([f\"Event type: {et}. Trigger: {tr}.\" for et,tr in final])\n",
        "\n",
        "# ======== Helpers for reranking / scoring ========\n",
        "def _anchor_score(src: str, trig: str) -> float:\n",
        "    \"\"\"How strongly 'trig' appears in 'src' (0..1). Fast Jaccard/char mixture.\"\"\"\n",
        "    s = norm(src)\n",
        "    t = norm(trig)\n",
        "    if not s or not t: return 0.0\n",
        "    if t in s: return 1.0\n",
        "    # character similarity on windows around occurrences of first token\n",
        "    toks = t.split()\n",
        "    if not toks: return 0.0\n",
        "    probe = toks[0]\n",
        "    best = 0.0\n",
        "    # quick scan\n",
        "    for m in re.finditer(re.escape(probe), s):\n",
        "        i = m.start()\n",
        "        span = s[max(0, i-20):i+len(t)+20]\n",
        "        best = max(best, _char_sim(span, t))\n",
        "        if best >= 0.92: break\n",
        "    # token Jaccard as tie-breaker\n",
        "    best = max(best, _token_jaccard(s, t))\n",
        "    return float(best)\n",
        "\n",
        "def _type_prior(et: str) -> float:\n",
        "    # TYPE_FREQ built earlier from clean train\n",
        "    return float(TYPE_FREQ.get(et, 0)) ** 0.5  # sublinear\n",
        "\n",
        "def _shape_bonus(tr: str) -> float:\n",
        "    letters = len(re.sub(r\"[^a-z0-9]+\",\"\", tr))\n",
        "    if letters <= 2: return -0.5\n",
        "    if letters <= 4: return 0.0\n",
        "    return 0.1\n",
        "\n",
        "def _score_event(src: str, et: str, tr: str) -> float:\n",
        "    \"\"\"Weighted score; tune-able.\"\"\"\n",
        "    a = _anchor_score(src, tr)             # 0..1\n",
        "    p = _type_prior(et)                    # 0..sqrt(freq)\n",
        "    sh= _shape_bonus(tr)                   # small +/-\n",
        "    return 0.70*a + 0.25*(p/(p+1.0)) + 0.05*sh\n",
        "\n",
        "def _parse_pairs_canon(text: str, canonizer) -> list[tuple[str,str]]:\n",
        "    txt = canonizer(text)\n",
        "    return parse_pairs(txt)\n",
        "\n",
        "# ======== Generator variants ========\n",
        "def _generate_once(prompts, mdl, tok, device, **gen_kwargs):\n",
        "    enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_IN_LEN).to(device)\n",
        "    gen = mdl.generate(**enc, trust_remote_code=True, **gen_kwargs)\n",
        "    return tok.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "def _beam_texts(prompts, mdl, tok, device, **kwargs):\n",
        "    gk = dict(\n",
        "        max_new_tokens=kwargs.get(\"max_new_tokens\", 160),\n",
        "        min_new_tokens=kwargs.get(\"min_new_tokens\", 8),\n",
        "        num_beams=8, num_beam_groups=4, diversity_penalty=0.2,\n",
        "        no_repeat_ngram_size=4,\n",
        "        length_penalty=0.8, repetition_penalty=1.03,\n",
        "        early_stopping=False\n",
        "    )\n",
        "    return _generate_once(prompts, mdl, tok, device, **gk)\n",
        "\n",
        "def _sample_texts(prompts, mdl, tok, device, **kwargs):\n",
        "    gk = dict(\n",
        "        do_sample=True, top_p=kwargs.get(\"top_p\", 0.92), temperature=kwargs.get(\"temperature\", 0.6),\n",
        "        max_new_tokens=kwargs.get(\"max_new_tokens\", 160), min_new_tokens=kwargs.get(\"min_new_tokens\", 8),\n",
        "        no_repeat_ngram_size=3, repetition_penalty=1.02, early_stopping=False\n",
        "    )\n",
        "    return _generate_once(prompts, mdl, tok, device, **gk)\n",
        "\n",
        "# ======== Hybrid generator with reranking ========\n",
        "def generate_batch(\n",
        "    prompts: List[str],\n",
        "    mdl, tok,\n",
        "    bs: int = 8,\n",
        "    device: str | None = None,\n",
        "    strategy: str = \"hybrid\",        # \"beam\" | \"consensus\" | \"hybrid\"\n",
        "    samples: int = 3,                # for sampling/consensus\n",
        "    postproc: str = \"softsmart\",     # \"basic\" | \"softsmart\"\n",
        "    top_p: float = 0.92,\n",
        "    temperature: float = 0.6,\n",
        "    max_new_tokens: int = 160,\n",
        "    min_new_tokens: int = 8,\n",
        "    max_events_total: int = 6,       # cap to prevent explosion\n",
        "    max_events_per_type: int = 3,    # per-type cap\n",
        "    tau_frac: float = 0.62,          # relative threshold\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mdl.eval().to(device)\n",
        "    canonizer = clean_and_canon_softsmart if postproc == \"softsmart\" else clean_and_canon_basic\n",
        "\n",
        "    outs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), bs):\n",
        "            batch = prompts[i:i+bs]\n",
        "\n",
        "            # 1) BEAM baseline\n",
        "            beam_dec = _beam_texts(batch, mdl, tok, device,\n",
        "                                   max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens)\n",
        "            beam_pairs = [_parse_pairs_canon(t, canonizer) for t in beam_dec]\n",
        "\n",
        "            if strategy == \"beam\":\n",
        "                # Beam + postproc only\n",
        "                outs.extend([canonizer(t) for t in beam_dec])\n",
        "                continue\n",
        "\n",
        "            # 2) CONSENSUS candidates (K samples)\n",
        "            all_samples = []\n",
        "            for _ in range(samples):\n",
        "                dec = _sample_texts(batch, mdl, tok, device,\n",
        "                                    top_p=top_p, temperature=temperature,\n",
        "                                    max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens)\n",
        "                all_samples.append([_parse_pairs_canon(t, canonizer) for t in dec])\n",
        "\n",
        "            # 3) Fuse per item with reranking + fallback\n",
        "            for k in range(len(batch)):\n",
        "                src_prompt = batch[k]\n",
        "                # start from beam pairs\n",
        "                pool = list(beam_pairs[k])\n",
        "\n",
        "                # add consensus variants (merged by near-duplicate)\n",
        "                # gather raw candidates\n",
        "                cands = []\n",
        "                for s_id in range(samples):\n",
        "                    cands.extend(all_samples[s_id][k])\n",
        "\n",
        "                # fuzzy merge across candidates\n",
        "                merged = []\n",
        "                for et,tr in cands:\n",
        "                    matched = False\n",
        "                    for i2,(et2,tr2) in enumerate(merged):\n",
        "                        if et==et2 and _near_duplicate(tr,tr2):\n",
        "                            # keep better-shaped\n",
        "                            def _shape(s):\n",
        "                                letters = len(re.sub(r\"[^a-z0-9]+\",\"\", s))\n",
        "                                toks = len(s.split())\n",
        "                                return letters - 0.5*max(0,toks-1)\n",
        "                            rep = tr if _shape(tr)>=_shape(tr2) else tr2\n",
        "                            merged[i2]=(et2,rep)\n",
        "                            matched=True; break\n",
        "                    if not matched:\n",
        "                        merged.append((et,tr))\n",
        "\n",
        "                # add merged consensus to pool if not exact dup of beam\n",
        "                for et,tr in merged:\n",
        "                    if (et,tr) not in pool:\n",
        "                        pool.append((et,tr))\n",
        "\n",
        "                # 4) Score & select with adaptive threshold + caps\n",
        "                # score each (et,tr)\n",
        "                scored = []\n",
        "                for et,tr in pool:\n",
        "                    s = _score_event(src_prompt, et, tr)\n",
        "                    scored.append((s, et, tr))\n",
        "                if not scored:\n",
        "                    outs.append(\"No events.\")\n",
        "                    continue\n",
        "\n",
        "                m = max(s for s,_,_ in scored)\n",
        "                tau = tau_frac * m\n",
        "                # keep only above threshold\n",
        "                keep = [(s,et,tr) for (s,et,tr) in sorted(scored, key=lambda x: x[0], reverse=True) if s >= tau]\n",
        "\n",
        "                # per-type cap\n",
        "                per_type_count = {}\n",
        "                final_pairs=[]\n",
        "                for s,et,tr in keep:\n",
        "                    if len(final_pairs) >= max_events_total:\n",
        "                        break\n",
        "                    c = per_type_count.get(et,0)\n",
        "                    if c >= max_events_per_type:\n",
        "                        continue\n",
        "                    per_type_count[et]=c+1\n",
        "                    final_pairs.append((et,tr))\n",
        "\n",
        "                # 5) If everything got pruned, fall back to beam only\n",
        "                if not final_pairs and beam_pairs[k]:\n",
        "                    final_pairs = beam_pairs[k]\n",
        "\n",
        "                if not final_pairs:\n",
        "                    outs.append(\"No events.\")\n",
        "                else:\n",
        "                    parts = [f\"Event type: {et}. Trigger: {tr}.\" for et,tr in final_pairs]\n",
        "                    outs.append(f\" {EVENT_TOKEN} \".join(parts))\n",
        "\n",
        "    return outs\n",
        "\n",
        "# ======== Metrics (unchanged) ========\n",
        "def match_partial(preds, golds):\n",
        "    used=set(); exact=partial=0\n",
        "    for pt,ptr in preds:\n",
        "        best=(0.0,-1)\n",
        "        for j,(gt,gtr) in enumerate(golds):\n",
        "            if j in used: continue\n",
        "            if pt!=gt:   continue\n",
        "            ov = trigger_overlap(ptr,gtr)\n",
        "            if ov>best[0]: best=(ov,j)\n",
        "        if best[1]!=-1:\n",
        "            used.add(best[1])\n",
        "            if math.isclose(best[0],1.0): exact+=1\n",
        "            elif best[0]>=0.5: partial+=1\n",
        "    return exact, partial, len(preds), len(golds)\n",
        "\n",
        "def prf(e,p,pt,gt, w=0.5):\n",
        "    wtp = e + w*p\n",
        "    P = wtp/pt if pt else 0.0\n",
        "    R = wtp/gt if gt else 0.0\n",
        "    F = (2*P*R)/(P+R) if (P+R) else 0.0\n",
        "    return P,R,F\n",
        "\n",
        "def relaxed_recall_by_chunks(pairs_pred, pairs_gold, chunk_size=1):\n",
        "    correct=partial=extra=possible=impossible=0\n",
        "    for pp,gg in zip(pairs_pred, pairs_gold):\n",
        "        ce,cp,pt,gt = match_partial(pp,gg)\n",
        "        matched = ce + 0.5*cp\n",
        "        possible += gt\n",
        "        impossible += max(0, gt-1)\n",
        "        extra += max(0.0, matched-1.0)\n",
        "        correct += ce; partial += cp\n",
        "    denom = max(1, possible - impossible)\n",
        "    num   = max(0.0, (correct + 0.5*partial) - extra)\n",
        "    return num/denom\n",
        "\n",
        "def evaluate(\n",
        "    ds, mdl, tok,\n",
        "    strategy: str = \"hybrid\",      # \"beam\" | \"consensus\" | \"hybrid\"\n",
        "    postproc: str = \"softsmart\",   # \"basic\" | \"softsmart\"\n",
        "    samples: int = 3,\n",
        "    bs: int = 8,\n",
        "    top_p: float = 0.92,\n",
        "    temperature: float = 0.6\n",
        "):\n",
        "    prompts = [ex[\"input\"] for ex in ds]\n",
        "    gtexts  = [ex[\"output\"] for ex in ds]\n",
        "    preds   = generate_batch(\n",
        "        prompts, mdl, tok, bs=bs,\n",
        "        strategy=strategy, postproc=postproc,\n",
        "        samples=samples, top_p=top_p, temperature=temperature\n",
        "    )\n",
        "\n",
        "    strict_tp=strict_pred=strict_gold=0\n",
        "    part_e=part_p=part_pt=part_gt=0\n",
        "    chunks_pred=[]; chunks_gold=[]\n",
        "\n",
        "    for ptxt,gtxt in zip(preds, gtexts):\n",
        "        pp = parse_pairs(ptxt)\n",
        "        gg = parse_pairs(gtxt)\n",
        "        sp,sg = set(pp), set(gg)\n",
        "        tp = len(sp & sg)\n",
        "        strict_tp += tp; strict_pred += len(sp); strict_gold += len(sg)\n",
        "        ce,cp,pt,gt = match_partial(pp,gg)\n",
        "        part_e += ce; part_p += cp; part_pt += pt; part_gt += gt\n",
        "        chunks_pred.append(pp); chunks_gold.append(gg)\n",
        "\n",
        "    sP = strict_tp/strict_pred if strict_pred else 0.0\n",
        "    sR = strict_tp/strict_gold if strict_gold else 0.0\n",
        "    sF = (2*sP*sR)/(sP+sR) if (sP+sR) else 0.0\n",
        "\n",
        "    pP,pR,pF = prf(part_e,part_p,part_pt,part_gt, w=0.5)\n",
        "    r_rel = relaxed_recall_by_chunks(chunks_pred, chunks_gold)\n",
        "\n",
        "    print(\"\\n===== STRICT =====\")\n",
        "    print(f\"P={sP:.4f} R={sR:.4f} F1={sF:.4f}\")\n",
        "    print(\"===== PARTIAL (MUC 0.5) =====\")\n",
        "    print(f\"P={pP:.4f} R={pR:.4f} F1={pF:.4f}\")\n",
        "    print(\"===== RELAXED (DEGREE2) =====\")\n",
        "    rF = (2*pP*r_rel)/(pP+r_rel) if (pP+r_rel)>0 else 0.0\n",
        "    print(f\"Relaxed-Recall={r_rel:.4f} | Relaxed-F1≈{rF:.4f}\")\n",
        "    return dict(strict_f1=sF, partial_f1=pF, relaxed_recall=r_rel, relaxed_f1=rF)\n",
        "\n",
        "# ======== Quick sanity + Eval with FINAL_DIR ========\n",
        "best_tok   = AutoTokenizer.from_pretrained(FINAL_DIR)\n",
        "best_model = AutoModelForSeq2SeqLM.from_pretrained(FINAL_DIR)\n",
        "\n",
        "#print(\"\\n--- VALID (hybrid + softsmart) ---\")\n",
        "#evaluate(valid_ds, best_model, best_tok, strategy=\"hybrid\", postproc=\"softsmart\", bs=8)\n",
        "\n",
        "#print(\"\\n--- TEST  (hybrid + softsmart) ---\")\n",
        "#evaluate(test_ds,  best_model, best_tok, strategy=\"hybrid\", postproc=\"softsmart\", bs=8)\n",
        "\n",
        "# Optional: compare with beam-only (parity with your previous baseline)\n",
        "print(\"\\n--- VALID (beam + basic) ---\")\n",
        "evaluate(valid_ds, best_model, best_tok, strategy=\"beam\", postproc=\"basic\", bs=8)\n",
        "\n",
        "print(\"\\n--- TEST (beam + basic) ---\")\n",
        "evaluate(test_ds, best_model, best_tok, strategy=\"beam\", postproc=\"basic\", bs=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "jSUndqLiFw1S",
        "outputId": "456679f5-6ee0-4c07-df3e-9d4e7652d503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- VALID (hybrid + softsmart) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-543171877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- VALID (hybrid + softsmart) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hybrid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostproc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softsmart\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- TEST  (hybrid + softsmart) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-543171877.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ds, mdl, tok, strategy, postproc, samples, bs, top_p, temperature)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mgtexts\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     preds   = generate_batch(\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostproc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-543171877.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(prompts, mdl, tok, bs, device, strategy, samples, postproc, top_p, temperature, max_new_tokens, min_new_tokens, max_events_total, max_events_per_type, tau_frac)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# 1) BEAM baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             beam_dec = _beam_texts(batch, mdl, tok, device,\n\u001b[0m\u001b[1;32m    213\u001b[0m                                    max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens)\n\u001b[1;32m    214\u001b[0m             \u001b[0mbeam_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_parse_pairs_canon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeam_dec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-543171877.py\u001b[0m in \u001b[0;36m_beam_texts\u001b[0;34m(prompts, mdl, tok, device, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sample_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-543171877.py\u001b[0m in \u001b[0;36m_generate_once\u001b[0;34m(prompts, mdl, tok, device, **gen_kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_generate_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_IN_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m                 \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m             )\n\u001b[0;32m-> 2576\u001b[0;31m             result = self._group_beam_search(\n\u001b[0m\u001b[1;32m   2577\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3703\u001b[0m                 \u001b[0;31m# stateless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3704\u001b[0m                 \u001b[0mprocess_beam_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbeam_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3705\u001b[0;31m                 beam_outputs = beam_scorer.process(\n\u001b[0m\u001b[1;32m   3706\u001b[0m                     \u001b[0mgroup_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3707\u001b[0m                     \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/beam_search.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mbeam_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             ):\n\u001b[1;32m    271\u001b[0m                 \u001b[0mbatch_beam_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             )\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 11) Inference++ (Hybrid + Vote + Anchor-aware Rerank) + Tiny Tuner\n",
        "# ==============================\n",
        "import re, math\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# --------- Utilities ----------\n",
        "_STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"of\",\"in\",\"on\",\"at\",\"to\",\"for\",\"from\",\"by\",\"with\",\n",
        "    \"and\",\"or\",\"but\",\"if\",\"as\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\"\n",
        "}\n",
        "\n",
        "def _soft_stem(w: str) -> str:\n",
        "    for suf in (\"'s\",\"’s\"):\n",
        "        if w.endswith(suf) and len(w) > len(suf)+1:\n",
        "            w = w[:-len(suf)]\n",
        "    for suf in (\"ing\",\"ed\",\"es\",\"s\"):\n",
        "        if w.endswith(suf) and len(w) >= 5:\n",
        "            w = w[:-len(suf)]\n",
        "            break\n",
        "    return w\n",
        "\n",
        "def normalize_trigger_str(tr: str) -> str:\n",
        "    if not tr: return \"\"\n",
        "    tr = tr.strip().replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\").replace(\"‘\",\"'\").replace(\"’\",\"'\")\n",
        "    tr = tr.replace(\"–\",\"-\").replace(\"—\",\"-\").strip(\" '\\\"`\").lower()\n",
        "    tr = re.sub(r\"[^\\w\\s\\-/]\", \" \", tr)\n",
        "    tr = re.sub(r\"\\s+\",\" \", tr).strip()\n",
        "    if len(tr) <= 1: return \"\"\n",
        "    toks = [t for t in tr.split() if t not in _STOPWORDS]\n",
        "    toks = [_soft_stem(t) for t in toks]\n",
        "    tr2  = \" \".join(toks).strip() or tr\n",
        "\n",
        "    if len(re.sub(r\"[^a-z0-9]+\",\"\", tr2)) < 2:\n",
        "        return \"\"\n",
        "\n",
        "    if tr2.isdigit():\n",
        "        return \"\"\n",
        "    return tr2\n",
        "\n",
        "def _token_jaccard(a: str, b: str) -> float:\n",
        "    A, B = set(a.split()), set(b.split())\n",
        "    if not A or not B: return 0.0\n",
        "    inter = len(A & B); union = len(A | B)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "def _char_sim(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def _near_dup(tr1: str, tr2: str, j=0.55, c=0.78) -> bool:\n",
        "    if not tr1 or not tr2: return False\n",
        "    return _token_jaccard(tr1, tr2) >= j or _char_sim(tr1, tr2) >= c\n",
        "\n",
        "def _merge_near_dups(pairs):\n",
        "    kept=[]\n",
        "    for et,tr in pairs:\n",
        "        placed=False\n",
        "        for i,(et2,tr2) in enumerate(kept):\n",
        "            if et==et2 and _near_dup(tr,tr2):\n",
        "                def _shape(s):\n",
        "                    letters = len(re.sub(r\"[^a-z0-9]+\",\"\", s))\n",
        "                    toks = len(s.split())\n",
        "                    return letters - 0.5*max(0,toks-1)\n",
        "                kept[i]=(et, tr if _shape(tr)>=_shape(tr2) else tr2)\n",
        "                placed=True; break\n",
        "        if not placed:\n",
        "            kept.append((et,tr))\n",
        "    return kept\n",
        "\n",
        "def _canonize(text: str):\n",
        "    \"\"\"soft-smart canonizer\"\"\"\n",
        "    raw = parse_pairs(text)\n",
        "    norm_pairs=[]\n",
        "    for et,tr in raw:\n",
        "        etn = nearest_type(et) if et else et\n",
        "        trn = normalize_trigger_str(tr)\n",
        "        if etn and trn:\n",
        "            norm_pairs.append((etn,trn))\n",
        "    if not norm_pairs: return []\n",
        "    return _merge_near_dups(norm_pairs)\n",
        "\n",
        "\n",
        "_SRC_RE = re.compile(r'Sentence:\\s*\"(.+?)\"\\s*?\\nOutput:', re.DOTALL|re.IGNORECASE)\n",
        "def recover_src_from_prompt(prompt: str) -> str:\n",
        "    m = _SRC_RE.search(prompt)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    # fallback: شاید ساختار فرق داشته باشد\n",
        "    m2 = re.search(r'Sentence:\\s*\"(.+?)\"', prompt, re.DOTALL|re.IGNORECASE)\n",
        "    return m2.group(1).strip() if m2 else prompt\n",
        "\n",
        "# --------- Anchor-aware scoring ----------\n",
        "def _anchor_score(src: str, trig: str) -> float:\n",
        "    s = norm(src); t = norm(trig)\n",
        "    if not s or not t: return 0.0\n",
        "    if t in s: return 1.0\n",
        "    toks = t.split()\n",
        "    if not toks: return 0.0\n",
        "    probe = toks[0]\n",
        "    best = 0.0\n",
        "    for m in re.finditer(re.escape(probe), s):\n",
        "        i = m.start()\n",
        "        span = s[max(0, i-24):i+len(t)+24]\n",
        "        best = max(best, _char_sim(span, t))\n",
        "        if best >= 0.93: break\n",
        "    best = max(best, _token_jaccard(s, t))\n",
        "    return float(best)\n",
        "\n",
        "def _type_prior(et: str) -> float:\n",
        "    return float(TYPE_FREQ.get(et,0))**0.5\n",
        "\n",
        "def _shape_bonus(tr: str) -> float:\n",
        "    letters = len(re.sub(r\"[^a-z0-9]+\",\"\", tr))\n",
        "    if letters <= 2: return -0.4\n",
        "    if letters <= 4: return 0.0\n",
        "    return 0.1\n",
        "\n",
        "def _score_event(src: str, et: str, tr: str, w_anchor=0.70, w_prior=0.25, w_shape=0.05) -> float:\n",
        "    a  = _anchor_score(src, tr)                     # 0..1\n",
        "    pr = _type_prior(et)                            # 0..sqrt(freq)\n",
        "    pr = pr/(pr+1.0)\n",
        "    sh = _shape_bonus(tr)\n",
        "    return w_anchor*a + w_prior*pr + w_shape*sh\n",
        "\n",
        "# --------- Text generation backends ----------\n",
        "def _generate_once(prompts, mdl, tok, device, **gen_kwargs):\n",
        "    enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_IN_LEN).to(device)\n",
        "    gen = mdl.generate(**enc, trust_remote_code=True, **gen_kwargs)\n",
        "    return tok.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "def _beam_texts(prompts, mdl, tok, device,\n",
        "                max_new_tokens=160, min_new_tokens=8,\n",
        "                num_beams=8, num_beam_groups=4, no_repeat_ngram_size=4,\n",
        "                length_penalty=0.85, repetition_penalty=1.03, diversity_penalty=0.2):\n",
        "    gk = dict(max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens,\n",
        "              num_beams=num_beams, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty,\n",
        "              no_repeat_ngram_size=no_repeat_ngram_size, length_penalty=length_penalty,\n",
        "              repetition_penalty=repetition_penalty, early_stopping=False)\n",
        "    return _generate_once(prompts, mdl, tok, device, **gk)\n",
        "\n",
        "def _sample_texts(prompts, mdl, tok, device,\n",
        "                  top_p=0.92, temperature=0.6, max_new_tokens=160, min_new_tokens=8,\n",
        "                  no_repeat_ngram_size=3, repetition_penalty=1.02):\n",
        "    gk = dict(do_sample=True, top_p=top_p, temperature=temperature,\n",
        "              max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens,\n",
        "              no_repeat_ngram_size=no_repeat_ngram_size, repetition_penalty=repetition_penalty,\n",
        "              early_stopping=False)\n",
        "    return _generate_once(prompts, mdl, tok, device, **gk)\n",
        "\n",
        "# --------- Hybrid + Vote + Rerank ----------\n",
        "def generate_batch_plus(\n",
        "    prompts: List[str],\n",
        "    mdl, tok,\n",
        "    bs: int = 8,\n",
        "    device: str | None = None,\n",
        "    samples: int = 3,\n",
        "    vote_k: int = 2,\n",
        "    tau_frac: float = 0.60,\n",
        "    max_events_total: int = 6,\n",
        "    max_events_per_type: int = 3,\n",
        "    top_p: float = 0.92,\n",
        "    temperature: float = 0.6,\n",
        "    beam_conf: dict | None = None,\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mdl.eval().to(device)\n",
        "    outs=[]\n",
        "\n",
        "    beam_conf = beam_conf or {}\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), bs):\n",
        "            batch = prompts[i:i+bs]\n",
        "            # BEAM\n",
        "            beam_dec = _beam_texts(batch, mdl, tok, device, **beam_conf)\n",
        "            beam_pairs = [_canonize(t) for t in beam_dec]\n",
        "\n",
        "            # CONSENSUS samples\n",
        "            all_samples = []\n",
        "            for _ in range(samples):\n",
        "                dec = _sample_texts(batch, mdl, tok, device, top_p=top_p, temperature=temperature)\n",
        "                all_samples.append([_canonize(t) for t in dec])\n",
        "\n",
        "            # Fuse per item\n",
        "            for k in range(len(batch)):\n",
        "                votes = {}\n",
        "                def add_vote(pairs):\n",
        "                    for (et,tr) in pairs:\n",
        "                        votes[(et,tr)] = votes.get((et,tr), 0) + 1\n",
        "\n",
        "                add_vote(beam_pairs[k])\n",
        "                for s_id in range(samples):\n",
        "                    add_vote(all_samples[s_id][k])\n",
        "\n",
        "                cand = [(et,tr, votes[(et,tr)]) for (et,tr) in votes if votes[(et,tr)] >= vote_k]\n",
        "\n",
        "                if not cand:\n",
        "                    keep = beam_pairs[k]\n",
        "                else:\n",
        "                    src_text = recover_src_from_prompt(batch[k])\n",
        "                    scored=[]\n",
        "                    for et,tr,v in cand:\n",
        "                        s = _score_event(src_text, et, tr)\n",
        "                        s += 0.02 * max(0, v-1)\n",
        "                        scored.append((s, et, tr))\n",
        "                    m = max(s for s,_,_ in scored) if scored else 0.0\n",
        "                    tau = tau_frac * m\n",
        "                    keep = [(et,tr) for (s,et,tr) in sorted(scored, key=lambda x: x[0], reverse=True) if s >= tau]\n",
        "\n",
        "\n",
        "                if keep:\n",
        "                    per_type={}\n",
        "                    final=[]\n",
        "                    for et,tr in keep:\n",
        "                        if len(final) >= max_events_total: break\n",
        "                        c = per_type.get(et,0)\n",
        "                        if c >= max_events_per_type: continue\n",
        "                        per_type[et]=c+1\n",
        "                        final.append((et,tr))\n",
        "                else:\n",
        "                    final=[]\n",
        "\n",
        "                if not final:\n",
        "                    outs.append(\"No events.\")\n",
        "                else:\n",
        "                    outs.append(\" {} \".format(EVENT_TOKEN).join([f\"Event type: {et}. Trigger: {tr}.\" for et,tr in final]))\n",
        "    return outs\n",
        "\n",
        "# --------- Metrics & Evaluate ----------\n",
        "def match_partial(preds, golds):\n",
        "    used=set(); exact=partial=0\n",
        "    for pt,ptr in preds:\n",
        "        best=(0.0,-1)\n",
        "        for j,(gt,gtr) in enumerate(golds):\n",
        "            if j in used: continue\n",
        "            if pt!=gt:   continue\n",
        "            ov = trigger_overlap(ptr,gtr)\n",
        "            if ov>best[0]: best=(ov,j)\n",
        "        if best[1]!=-1:\n",
        "            used.add(best[1])\n",
        "            if math.isclose(best[0],1.0): exact+=1\n",
        "            elif best[0]>=0.5: partial+=1\n",
        "    return exact, partial, len(preds), len(golds)\n",
        "\n",
        "def prf(e,p,pt,gt, w=0.5):\n",
        "    wtp = e + w*p\n",
        "    P = wtp/pt if pt else 0.0\n",
        "    R = wtp/gt if gt else 0.0\n",
        "    F = (2*P*R)/(P+R) if (P+R) else 0.0\n",
        "    return P,R,F\n",
        "\n",
        "def relaxed_recall_by_chunks(pairs_pred, pairs_gold, chunk_size=1):\n",
        "    correct=partial=extra=possible=impossible=0\n",
        "    for pp,gg in zip(pairs_pred, pairs_gold):\n",
        "        ce,cp,pt,gt = match_partial(pp,gg)\n",
        "        matched = ce + 0.5*cp\n",
        "        possible += gt\n",
        "        impossible += max(0, gt-1)\n",
        "        extra += max(0.0, matched-1.0)\n",
        "        correct += ce; partial += cp\n",
        "    denom = max(1, possible - impossible)\n",
        "    num   = max(0.0, (correct + 0.5*partial) - extra)\n",
        "    return num/denom\n",
        "\n",
        "def evaluate_plus(\n",
        "    ds, mdl, tok,\n",
        "    bs: int = 8,\n",
        "    samples: int = 3,\n",
        "    vote_k: int = 2,\n",
        "    tau_frac: float = 0.60,\n",
        "    top_p: float = 0.92,\n",
        "    temperature: float = 0.6,\n",
        "    beam_conf: dict | None = None\n",
        "):\n",
        "    prompts = [ex[\"input\"] for ex in ds]\n",
        "    preds   = generate_batch_plus(\n",
        "        prompts, mdl, tok, bs=bs,\n",
        "        samples=samples, vote_k=vote_k, tau_frac=tau_frac,\n",
        "        top_p=top_p, temperature=temperature,\n",
        "        beam_conf=beam_conf\n",
        "    )\n",
        "    gtexts  = [ex[\"output\"] for ex in ds]\n",
        "\n",
        "    strict_tp=strict_pred=strict_gold=0\n",
        "    part_e=part_p=part_pt=part_gt=0\n",
        "    chunks_pred=[]; chunks_gold=[]\n",
        "    for ptxt,gtxt in zip(preds, gtexts):\n",
        "        pp = parse_pairs(ptxt)\n",
        "        gg = parse_pairs(gtxt)\n",
        "        sp,sg = set(pp), set(gg)\n",
        "        tp = len(sp & sg)\n",
        "        strict_tp += tp; strict_pred += len(sp); strict_gold += len(sg)\n",
        "        ce,cp,pt,gt = match_partial(pp,gg)\n",
        "        part_e += ce; part_p += cp; part_pt += pt; part_gt += gt\n",
        "        chunks_pred.append(pp); chunks_gold.append(gg)\n",
        "    sP = strict_tp/strict_pred if strict_pred else 0.0\n",
        "    sR = strict_tp/strict_gold if strict_gold else 0.0\n",
        "    sF = (2*sP*sR)/(sP+sR) if (sP+sR) else 0.0\n",
        "    pP,pR,pF = prf(part_e,part_p,part_pt,part_gt, w=0.5)\n",
        "    r_rel = relaxed_recall_by_chunks(chunks_pred, chunks_gold)\n",
        "    rF = (2*pP*r_rel)/(pP+r_rel) if (pP+r_rel)>0 else 0.0\n",
        "\n",
        "    print(\"\\n===== STRICT =====\")\n",
        "    print(f\"P={sP:.4f} R={sR:.4f} F1={sF:.4f}\")\n",
        "    print(\"===== PARTIAL (MUC 0.5) =====\")\n",
        "    print(f\"P={pP:.4f} R={pR:.4f} F1={pF:.4f}\")\n",
        "    print(\"===== RELAXED (DEGREE2) =====\")\n",
        "    print(f\"Relaxed-Recall={r_rel:.4f} | Relaxed-F1≈{rF:.4f}\")\n",
        "    return dict(strict_f1=sF, partial_f1=pF, relaxed_recall=r_rel, relaxed_f1=rF)\n",
        "\n",
        "# --------- Tiny tuner on VALID ----------\n",
        "def tiny_tune_on_valid(mdl, tok, valid_ds):\n",
        "    grid_tau   = [0.58, 0.60, 0.62]\n",
        "    grid_vote  = [2, 3]\n",
        "    grid_tpT   = [(0.92,0.6), (0.90,0.7)]   # (top_p, temperature)\n",
        "    best = (-1, None)\n",
        "    for tau in grid_tau:\n",
        "        for vk in grid_vote:\n",
        "            for top_p, temp in grid_tpT:\n",
        "                print(f\"[tune] try tau={tau}, vote_k={vk}, top_p={top_p}, T={temp}\")\n",
        "                m = evaluate_plus(valid_ds, mdl, tok, bs=16, samples=3, vote_k=vk,\n",
        "                                  tau_frac=tau, top_p=top_p, temperature=temp)\n",
        "                key = m[\"relaxed_f1\"]\n",
        "                if key > best[0]:\n",
        "                    best = (key, dict(tau_frac=tau, vote_k=vk, top_p=top_p, temperature=temp))\n",
        "    print(f\"[tune] best on VALID: relaxed_F1={best[0]:.4f} with {best[1]}\")\n",
        "    return best[1] or dict(tau_frac=0.60, vote_k=2, top_p=0.92, temperature=0.6)\n",
        "\n",
        "# --------- Run ----------\n",
        "best_tok   = AutoTokenizer.from_pretrained(FINAL_DIR)\n",
        "best_model = AutoModelForSeq2SeqLM.from_pretrained(FINAL_DIR)\n",
        "\n",
        "print(\"\\n--- TUNE on VALID (hybrid+vote+rerank) ---\")\n",
        "best_params = tiny_tune_on_valid(best_model, best_tok, valid_ds)\n",
        "\n",
        "print(\"\\n--- VALID (best tuned) ---\")\n",
        "evaluate_plus(valid_ds, best_model, best_tok, **best_params)\n",
        "\n",
        "print(\"\\n--- TEST (best tuned) ---\")\n",
        "evaluate_plus(test_ds,  best_model, best_tok, **best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "5eprHgouw6wC",
        "outputId": "a0faf350-40fa-4235-cbe3-2587443f4fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TUNE on VALID (hybrid+vote+rerank) ---\n",
            "[tune] try tau=0.58, vote_k=2, top_p=0.92, T=0.6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1513702970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- TUNE on VALID (hybrid+vote+rerank) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiny_tune_on_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- VALID (best tuned) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1513702970.py\u001b[0m in \u001b[0;36mtiny_tune_on_valid\u001b[0;34m(mdl, tok, valid_ds)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_tpT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[tune] try tau={tau}, vote_k={vk}, top_p={top_p}, T={temp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 m = evaluate_plus(valid_ds, mdl, tok, bs=16, samples=3, vote_k=vk,\n\u001b[0m\u001b[1;32m    322\u001b[0m                                   tau_frac=tau, top_p=top_p, temperature=temp)\n\u001b[1;32m    323\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"relaxed_f1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1513702970.py\u001b[0m in \u001b[0;36mevaluate_plus\u001b[0;34m(ds, mdl, tok, bs, samples, vote_k, tau_frac, top_p, temperature, beam_conf)\u001b[0m\n\u001b[1;32m    274\u001b[0m ):\n\u001b[1;32m    275\u001b[0m     \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     preds   = generate_batch_plus(\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvote_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtau_frac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1513702970.py\u001b[0m in \u001b[0;36mgenerate_batch_plus\u001b[0;34m(prompts, mdl, tok, bs, device, samples, vote_k, tau_frac, max_events_total, max_events_per_type, top_p, temperature, beam_conf)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mall_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sample_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mall_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_canonize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1513702970.py\u001b[0m in \u001b[0;36m_sample_texts\u001b[0;34m(prompts, mdl, tok, device, top_p, temperature, max_new_tokens, min_new_tokens, no_repeat_ngram_size, repetition_penalty)\u001b[0m\n\u001b[1;32m    147\u001b[0m               \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m               early_stopping=False)\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;31m# --------- Hybrid + Vote + Rerank ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1513702970.py\u001b[0m in \u001b[0;36m_generate_once\u001b[0;34m(prompts, mdl, tok, device, **gen_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_generate_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_IN_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;31m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2868\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2870\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1764\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1100\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     ):\n\u001b[0;32m--> 686\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    687\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    600\u001b[0m     ):\n\u001b[1;32m    601\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_past_key_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_value_proj_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}