{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Build \"results\" table**"
      ],
      "metadata": {
        "id": "Uq0VaxL6luT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tE8hJ7nmNQP",
        "outputId": "4f1fbdc2-79ba-47be-ee55-115288f49bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m122.9/172.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHRc3Yyae2Sf"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# Results Table Builder (Pack A -> results.xlsx) — No output_paste column\n",
        "# - Expands over models/versions/shots\n",
        "# - User pastes directly into `output_raw`\n",
        "# - `n_predicted_events` auto-counts \"<EVENTSEP>\" via Excel formula\n",
        "# - Optional normalizer to bake static values\n",
        "# ==========================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Configuration ------------------\n",
        "PACKA_PATH = \"/content/PackA_TextChunks.csv\"   # path to Pack A (must have chunk_id, text)\n",
        "OUT_XLSX   = \"/content/results.xlsx\"                 # main file with formulas\n",
        "OUT_CSV    = \"/content/results.csv\"                  # optional CSV mirror (no formulas)\n",
        "\n",
        "MODEL_NAMES = [\"ChatGPT\", \"Gemini\", \"DeepSeek\", \"Grok\"]\n",
        "CHATGPT_VERSIONS = [\"GPT5\", \"GPT-4o\"]\n",
        "OTHER_VERSION = \"\"     # for non-ChatGPT rows\n",
        "SHOT_SET = [0, 1, 3, 5]\n",
        "\n",
        "# Column widths (Excel aesthetics)\n",
        "COL_WIDTHS = {\n",
        "    \"A\": 12,   # chunk_id\n",
        "    \"B\": 60,   # text\n",
        "    \"C\": 12,   # model_name\n",
        "    \"D\": 14,   # model_version\n",
        "    \"E\": 16,   # condition_shots\n",
        "    \"F\": 120,  # output_raw (user pastes here; newlines OK)\n",
        "    \"G\": 18,   # n_predicted_events (formula)\n",
        "}"
      ],
      "metadata": {
        "id": "7ftokNllly_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Core Builders ------------------\n",
        "def require_packa(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load Pack A and validate the required columns.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Pack A not found: {path}\")\n",
        "    df = pd.read_csv(path, dtype=str).fillna(\"\")\n",
        "    required = {\"chunk_id\", \"text\"}\n",
        "    if not required.issubset(df.columns):\n",
        "        raise ValueError(f\"Pack A must have columns: {required}\")\n",
        "    return df[[\"chunk_id\", \"text\"]].copy()\n",
        "\n",
        "def expand_results_rows(packA_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create the full cartesian expansion over models/versions/shots.\"\"\"\n",
        "    rows: List[dict] = []\n",
        "    for _, r in packA_df.iterrows():\n",
        "        cid = r[\"chunk_id\"]\n",
        "        txt = r[\"text\"]\n",
        "        for model in MODEL_NAMES:\n",
        "            versions = CHATGPT_VERSIONS if model == \"ChatGPT\" else [OTHER_VERSION]\n",
        "            for ver in versions:\n",
        "                for k in SHOT_SET:\n",
        "                    rows.append({\n",
        "                        \"chunk_id\": cid,\n",
        "                        \"text\": txt,\n",
        "                        \"model_name\": model,\n",
        "                        \"model_version\": ver,\n",
        "                        \"condition_shots\": k,\n",
        "                        \"output_raw\": \"\",          # user pastes here (multi-line OK)\n",
        "                        \"n_predicted_events\": \"\"   # Excel formula will auto-count <EVENTSEP>\n",
        "                    })\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"chunk_id\",\"text\",\"model_name\",\"model_version\",\"condition_shots\",\n",
        "        \"output_raw\",\"n_predicted_events\"\n",
        "    ])\n",
        "    return df.sort_values([\"chunk_id\",\"model_name\",\"model_version\",\"condition_shots\"]).reset_index(drop=True)\n",
        "\n",
        "def save_csv(df: pd.DataFrame, out_path: str) -> None:\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "def save_xlsx_with_formula(df: pd.DataFrame, out_path: str) -> None:\n",
        "    \"\"\"Write results.xlsx with Excel formula for n_predicted_events based on output_raw.\"\"\"\n",
        "    try:\n",
        "        import xlsxwriter  # noqa\n",
        "        engine = \"xlsxwriter\"\n",
        "    except Exception:\n",
        "        engine = \"openpyxl\"  # fallback; formula text is still stored for Excel to evaluate\n",
        "\n",
        "    with pd.ExcelWriter(out_path, engine=engine) as writer:\n",
        "        df.to_excel(writer, sheet_name=\"results\", index=False)\n",
        "        ws = writer.sheets[\"results\"]\n",
        "\n",
        "        # Set column widths and freeze header\n",
        "        for col, w in COL_WIDTHS.items():\n",
        "            ws.set_column(f\"{col}:{col}\", w)\n",
        "        ws.freeze_panes(1, 0)\n",
        "\n",
        "        # Column indexes (0-based)\n",
        "        headers = list(df.columns)\n",
        "        col_idx = {name: i for i, name in enumerate(headers)}\n",
        "\n",
        "        # Insert the counting formula for each row\n",
        "        n_rows = len(df)\n",
        "        for r in range(n_rows):\n",
        "            excel_row = r + 2  # 1-based; row 1 is header\n",
        "            raw_cell = f\"F{excel_row}\"  # output_raw\n",
        "            cnt_col = col_idx[\"n_predicted_events\"]\n",
        "\n",
        "            # Count how many \"<EVENTSEP>\" substrings appear in output_raw\n",
        "            # Works with multi-line cells; Excel treats line breaks as characters.\n",
        "            count_formula = (\n",
        "                f'=IF({raw_cell}=\"\",\"\",'\n",
        "                f'(LEN({raw_cell})-LEN(SUBSTITUTE({raw_cell},\"<EVENTSEP>\",\"\")))/LEN(\"<EVENTSEP>\"))'\n",
        "            )\n",
        "            ws.write_formula(excel_row - 1, cnt_col, count_formula)\n"
      ],
      "metadata": {
        "id": "thAAMJbml59g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Optional Normalizer ------------------\n",
        "def _flatten_whitespace(s: str) -> str:\n",
        "    \"\"\"Collapse newlines and repeated spaces to a single space; ensure spacing before <EVENTSEP>.\"\"\"\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return \"\"\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'\\s*<EVENTSEP>', ' <EVENTSEP>', s).strip()\n",
        "    s = re.sub(r' {2,}', ' ', s)\n",
        "    return s\n",
        "\n",
        "def _count_eventsep(s: str) -> int:\n",
        "    if not isinstance(s, str) or not s:\n",
        "        return 0\n",
        "    return s.count(\"<EVENTSEP>\")\n",
        "\n",
        "def normalize_results(in_path: str, out_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Bake a static file (no formulas):\n",
        "      - Keep user-entered `output_raw` as-is or flattened (choose below).\n",
        "      - Persist integer `n_predicted_events` by counting \"<EVENTSEP>\".\n",
        "    \"\"\"\n",
        "    ext = Path(in_path).suffix.lower()\n",
        "    if ext == \".xlsx\":\n",
        "        df = pd.read_excel(in_path, sheet_name=\"results\", dtype=str).fillna(\"\")\n",
        "    elif ext == \".csv\":\n",
        "        df = pd.read_csv(in_path, dtype=str).fillna(\"\")\n",
        "    else:\n",
        "        raise ValueError(\"in_path must be .xlsx or .csv\")\n",
        "\n",
        "    if \"output_raw\" not in df.columns:\n",
        "        raise ValueError(\"The input file must contain 'output_raw' column.\")\n",
        "\n",
        "    # Optional: flatten multi-line outputs for portability (uncomment if desired)\n",
        "    # df[\"output_raw\"] = df[\"output_raw\"].apply(_flatten_whitespace)\n",
        "\n",
        "    df[\"n_predicted_events\"] = df[\"output_raw\"].apply(_count_eventsep).astype(int)\n",
        "\n",
        "    # Save static copy\n",
        "    if out_path.lower().endswith(\".xlsx\"):\n",
        "        with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
        "            df.to_excel(writer, sheet_name=\"results\", index=False)\n",
        "            writer.sheets[\"results\"].freeze_panes(1, 0)\n",
        "    else:\n",
        "        df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"Normalized (static) results written to: {out_path}\")"
      ],
      "metadata": {
        "id": "aQxtITF5mFMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Main ------------------\n",
        "def main():\n",
        "    packA = require_packa(PACKA_PATH)\n",
        "    results_df = expand_results_rows(packA)\n",
        "\n",
        "    # Save CSV (no formulas) and XLSX (with formula on n_predicted_events)\n",
        "    save_csv(results_df, OUT_CSV)\n",
        "    save_xlsx_with_formula(results_df, OUT_XLSX)\n",
        "\n",
        "    print(\"Created files:\")\n",
        "    print(f\"- {OUT_XLSX}  (paste into 'output_raw'; 'n_predicted_events' auto-counts)\")\n",
        "    print(f\"- {OUT_CSV}   (no formulas; for reference)\")\n",
        "\n",
        "    # Example: when you want a static copy (no formulas), run:\n",
        "    normalize_results(OUT_XLSX, \"/content/results_normalized.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM6iuBgImW2Y",
        "outputId": "a8d814d5-068d-42d4-86e7-661e78e6b432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created files:\n",
            "- /content/results.xlsx  (paste into 'output_raw'; 'n_predicted_events' auto-counts)\n",
            "- /content/results.csv   (no formulas; for reference)\n",
            "Normalized (static) results written to: /content/results_normalized.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Post-Processing & Mapping**"
      ],
      "metadata": {
        "id": "88wRILbn-6VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Evaluation-ready version (per model/version/shots)\n",
        "# ================================\n",
        "import os, re, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -------- Paths (update to your actual files) --------\n",
        "RESULTS_PATH   = \"/content/results.xlsx\"         # <-- your multi-model Excel\n",
        "PACKB_GOLD     = \"/content/PackB_Gold.csv\"\n",
        "SCHEMA_ALIASES = \"/content/packs/Schema_Aliases.csv\"  # optional\n",
        "OUT_DIR        = \"/content/out_eval\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------- Regex for parsing model outputs --------\n",
        "EVENT_LINE_RE = re.compile(\n",
        "    r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*(?P<etype>[^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*(?P<trig>[^.\\n\\r<]+)',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def normalize_type(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r'[_\\-\\s]+',' ', str(s).strip()).lower()\n",
        "\n",
        "def normalize_trigger(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip().strip('.;,:-').lower()\n",
        "    return re.sub(r'\\s+',' ', s)\n",
        "\n",
        "def parse_event_lines(output_raw: str):\n",
        "    if not isinstance(output_raw, str) or not output_raw.strip():\n",
        "        return []\n",
        "    pairs = []\n",
        "    parts = [p for p in output_raw.split(\"<EVENTSEP>\") if p.strip()] or [output_raw]\n",
        "    for part in parts:\n",
        "        for m in EVENT_LINE_RE.finditer(part):\n",
        "            et = normalize_type(m.group(\"etype\"))\n",
        "            tr = normalize_trigger(m.group(\"trig\"))\n",
        "            if et and tr:\n",
        "                pairs.append((et, tr))\n",
        "    return pairs\n",
        "\n",
        "# -------- Load results (xlsx or csv) and filter non-empty --------\n",
        "ext = Path(RESULTS_PATH).suffix.lower()\n",
        "if ext == \".xlsx\":\n",
        "    res = pd.read_excel(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "else:\n",
        "    res = pd.read_csv(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "required_cols = {\"chunk_id\",\"output_raw\",\"model_name\",\"model_version\",\"condition_shots\"}\n",
        "if not required_cols.issubset(res.columns):\n",
        "    raise ValueError(f\"RESULTS must contain: {required_cols}\")\n",
        "res = res[res[\"output_raw\"].astype(str).str.strip() != \"\"].copy()\n",
        "\n",
        "# -------- Load gold & build allowed schema set --------\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"PackB_Gold.csv must have columns: chunk_id, events_norm\")\n",
        "\n",
        "schema_types = set()\n",
        "for row in gold[\"events_norm\"]:\n",
        "    if not isinstance(row, str): continue\n",
        "    for item in row.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" in item:\n",
        "            tp,_ = item.split(\"|\",1)\n",
        "            if tp.strip():\n",
        "                schema_types.add(tp.strip())\n",
        "allowed_schema = sorted(schema_types)\n",
        "allowed_norm   = [normalize_type(x) for x in allowed_schema]\n",
        "norm2schema    = {normalize_type(x): x for x in allowed_schema}\n",
        "\n",
        "# (optional) alias table\n",
        "alias_map = {}\n",
        "if os.path.exists(SCHEMA_ALIASES):\n",
        "    alias_df = pd.read_csv(SCHEMA_ALIASES, dtype=str).fillna(\"\")\n",
        "    if {\"alias\",\"schema_type\"}.issubset(alias_df.columns):\n",
        "        for _, r in alias_df.iterrows():\n",
        "            a = normalize_type(r[\"alias\"])\n",
        "            t = str(r[\"schema_type\"]).strip()\n",
        "            if a and t:\n",
        "                alias_map[a] = t\n",
        "\n",
        "# TF-IDF over labels\n",
        "def build_label_corpus(labels):\n",
        "    expanded=[]\n",
        "    for lab in labels:\n",
        "        toks = re.split(r'[_\\-\\s]+', lab)\n",
        "        expanded.append(\" \".join([lab] + toks))\n",
        "    return expanded\n",
        "\n",
        "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_schema = tfidf.fit_transform(build_label_corpus(allowed_norm))\n",
        "\n",
        "def tfidf_best_match(open_label_norm: str, threshold=0.35):\n",
        "    v = tfidf.transform(build_label_corpus([open_label_norm]))\n",
        "    sims = cosine_similarity(v, X_schema).flatten()\n",
        "    idx  = int(np.argmax(sims))\n",
        "    score = float(sims[idx])\n",
        "    if score >= threshold:\n",
        "        best_norm = allowed_norm[idx]\n",
        "        return norm2schema[best_norm], score\n",
        "    return None, score\n",
        "\n",
        "def map_open_to_schema(open_type: str, trigger: str):\n",
        "    o_norm = normalize_type(open_type)\n",
        "    # 1) alias\n",
        "    if o_norm in alias_map:\n",
        "        tgt = alias_map[o_norm]\n",
        "        if normalize_type(tgt) in norm2schema:\n",
        "            return norm2schema[normalize_type(tgt)], \"alias\", 1.0\n",
        "        return tgt, \"alias\", 1.0\n",
        "    # 2) exact\n",
        "    if o_norm in norm2schema:\n",
        "        return norm2schema[o_norm], \"exact\", 1.0\n",
        "    # 3) light normalization\n",
        "    candidates = {o_norm, re.sub(r'(ings|ing|ed|s)$','', o_norm)}\n",
        "    for c in list(candidates):\n",
        "        candidates.add(c+\"ing\"); candidates.add(c+\"ed\")\n",
        "    for c in candidates:\n",
        "        if c in norm2schema:\n",
        "            return norm2schema[c], \"norm\", 1.0\n",
        "    # 4) tf-idf\n",
        "    best, sc = tfidf_best_match(o_norm, threshold=0.35)\n",
        "    if best: return best, \"tfidf\", sc\n",
        "    # 5) fallback\n",
        "    return \"UNMAPPED\", \"unmapped\", 0.0\n",
        "\n",
        "def normalize_gold_pairs(s: str):\n",
        "    out=set()\n",
        "    for item in str(s).split(\";\"):\n",
        "        item=item.strip()\n",
        "        if \"|\" in item:\n",
        "            tp,tr=item.split(\"|\",1)\n",
        "            out.add((normalize_type(tp), normalize_trigger(tr)))\n",
        "    return out\n",
        "\n",
        "# -------- Evaluate per (model_name, model_version, condition_shots) --------\n",
        "group_cols = [\"model_name\",\"model_version\",\"condition_shots\"]\n",
        "metrics_rows = []\n",
        "all_mapped_frames = []   # to save per-group mapped events if desired\n",
        "\n",
        "for gkeys, gdf in res.groupby(group_cols):\n",
        "    gname = {k:v for k,v in zip(group_cols, gkeys)}\n",
        "    # Parse & map\n",
        "    parsed_rows=[]\n",
        "    for _, r in gdf.iterrows():\n",
        "        cid = r[\"chunk_id\"]\n",
        "        for (ot,tr) in parse_event_lines(r[\"output_raw\"]):\n",
        "            parsed_rows.append({\"chunk_id\":cid,\"open_type\":ot,\"trigger\":tr})\n",
        "    if not parsed_rows:\n",
        "        # nothing predicted in this group\n",
        "        metrics_rows.append({**gname, \"TP\":0,\"FP\":0,\"FN\":0,\"precision\":0.0,\"recall\":0.0,\"f1\":0.0,\"pred_events\":0})\n",
        "        continue\n",
        "\n",
        "    mapped=[]\n",
        "    for pr in parsed_rows:\n",
        "        sch, mth, sc = map_open_to_schema(pr[\"open_type\"], pr[\"trigger\"])\n",
        "        mapped.append({**pr, \"schema_type\":sch, \"method\":mth, \"score\":sc})\n",
        "    mapped_df = pd.DataFrame(mapped)\n",
        "\n",
        "    # Build gold & pred pair-sets per chunk\n",
        "    gold_by_chunk = {cid: normalize_gold_pairs(ev) for cid, ev in zip(gold[\"chunk_id\"], gold[\"events_norm\"])}\n",
        "    pred_by_chunk = {}\n",
        "    for cid, sub in mapped_df.groupby(\"chunk_id\"):\n",
        "        pairs = set((normalize_type(t), normalize_trigger(tr)) for t,tr in zip(sub[\"schema_type\"], sub[\"trigger\"]))\n",
        "        pred_by_chunk[cid] = pairs\n",
        "\n",
        "    TP=FP=FN=0\n",
        "    for cid in gold_by_chunk:\n",
        "        gset = gold_by_chunk.get(cid, set())\n",
        "        pset = pred_by_chunk.get(cid, set())\n",
        "        TP += len(gset & pset)\n",
        "        FP += len(pset - gset)\n",
        "        FN += len(gset - pset)\n",
        "\n",
        "    prec = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    rec  = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
        "\n",
        "    metrics_rows.append({**gname, \"TP\":TP,\"FP\":FP,\"FN\":FN,\n",
        "                         \"precision\":round(prec,4),\"recall\":round(rec,4),\"f1\":round(f1,4),\n",
        "                         \"pred_events\":len(mapped_df)})\n",
        "\n",
        "    # (optional) save per-group mapped events\n",
        "    mapped_df[\"model_name\"]=gname[\"model_name\"]\n",
        "    mapped_df[\"model_version\"]=gname[\"model_version\"]\n",
        "    mapped_df[\"condition_shots\"]=gname[\"condition_shots\"]\n",
        "    all_mapped_frames.append(mapped_df)\n",
        "\n",
        "# -------- Write outputs --------\n",
        "metrics = pd.DataFrame(metrics_rows).sort_values([\"model_name\",\"model_version\",\"condition_shots\"])\n",
        "metrics_path = f\"{OUT_DIR}/Metrics_by_group.csv\"\n",
        "metrics.to_csv(metrics_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "if all_mapped_frames:\n",
        "    mapped_all = pd.concat(all_mapped_frames, ignore_index=True)\n",
        "    mapped_path = f\"{OUT_DIR}/Mapped_Results_by_group.csv\"\n",
        "    mapped_all.to_csv(mapped_path, index=False, encoding=\"utf-8\")\n",
        "else:\n",
        "    mapped_path = None\n",
        "\n",
        "print(f\"[OK] Wrote metrics: {metrics_path}\")\n",
        "if mapped_path:\n",
        "    print(f\"[OK] Wrote mapped events: {mapped_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6A3MKBi-7m6",
        "outputId": "6783f672-c1b3-4e2c-f4f3-e3a59823e2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Wrote metrics: /content/out_eval/Metrics_by_group.csv\n",
            "[OK] Wrote mapped events: /content/out_eval/Mapped_Results_by_group.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Mapping without aliases: Hybrid similarity (string + TF-IDF + optional embeddings) + trigger boost\n",
        "# ==========================\n",
        "import os, re, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- install helpers (Colab-friendly) ---\n",
        "try:\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio, partial_ratio\n",
        "except Exception:\n",
        "    import sys\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio, partial_ratio\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except Exception:\n",
        "    !pip -q install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# (optional) sentence embeddings\n",
        "USE_EMBEDDINGS = False\n",
        "if USE_EMBEDDINGS:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "    except Exception:\n",
        "        !pip -q install sentence-transformers\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------------- Paths ----------------\n",
        "PACKB_GOLD = \"/content/PackB_Gold.csv\"       # has: chunk_id, events_norm\n",
        "RESULTS    = \"/content/results.xlsx\"         # has: chunk_id, model_name, model_version, condition_shots, output_raw\n",
        "\n",
        "# ------------- Normalizers -------------\n",
        "def norm_type(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[_\\-\\s]+', ' ', s)   # unify separators\n",
        "    return s\n",
        "\n",
        "def norm_trigger(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[\\.;:,\\-]+$', '', s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "# ------------- Parse EVENTSEP -----------\n",
        "EVENT_LINE_RE = re.compile(\n",
        "    r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*(?P<etype>[^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*(?P<trig>[^.\\n\\r<]+)',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_output(text: str):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    parts = [p for p in text.split(\"<EVENTSEP>\") if p.strip()] or [text]\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        for m in EVENT_LINE_RE.finditer(p):\n",
        "            et = norm_type(m.group(\"etype\"))\n",
        "            tr = norm_trigger(m.group(\"trig\"))\n",
        "            if et and tr:\n",
        "                out.append((et, tr))\n",
        "    return out\n",
        "\n",
        "# -------- Load schema from GOLD ---------\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"PackB_Gold.csv must contain: chunk_id, events_norm\")\n",
        "\n",
        "# canonical schema label list\n",
        "schema_types = set()\n",
        "triggers_by_type = {}  # for trigger-based boosting\n",
        "for s in gold[\"events_norm\"]:\n",
        "    if not isinstance(s, str): continue\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp, tr = item.split(\"|\", 1)\n",
        "        tp_canon = tp.strip()\n",
        "        tr_n = norm_trigger(tr)\n",
        "        schema_types.add(tp_canon)\n",
        "        triggers_by_type.setdefault(tp_canon, {})\n",
        "        triggers_by_type[tp_canon][tr_n] = triggers_by_type[tp_canon].get(tr_n, 0) + 1\n",
        "\n",
        "schema_types = sorted(schema_types)\n",
        "schema_norm  = [norm_type(x) for x in schema_types]\n",
        "norm2canon   = {norm_type(x): x for x in schema_types}\n",
        "\n",
        "# -------- Build similarity index --------\n",
        "def expand_label(s: str) -> str:\n",
        "    toks = re.split(r'[_\\-\\s]+', s)\n",
        "    return \" \".join([s] + toks)\n",
        "\n",
        "# TF-IDF (char 3–5 grams) over schema labels\n",
        "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_tfidf = tfidf.fit_transform([expand_label(x) for x in schema_norm])\n",
        "\n",
        "# optional embeddings\n",
        "if USE_EMBEDDINGS:\n",
        "    emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    X_emb = emb_model.encode(schema_norm, normalize_embeddings=True)\n",
        "\n",
        "# -------- Similarity scorers ----------\n",
        "def jaccard_tokens(a: str, b: str) -> float:\n",
        "    A = set(a.split())\n",
        "    B = set(b.split())\n",
        "    if not A or not B: return 0.0\n",
        "    return len(A & B) / len(A | B)\n",
        "\n",
        "def tfidf_sim(q: str) -> float:\n",
        "    v = tfidf.transform([expand_label(q)])\n",
        "    sims = cosine_similarity(v, X_tfidf).ravel()\n",
        "    return float(np.max(sims))\n",
        "\n",
        "def tfidf_best(q: str):\n",
        "    v = tfidf.transform([expand_label(q)])\n",
        "    sims = cosine_similarity(v, X_tfidf).ravel()\n",
        "    idx = int(np.argmax(sims))\n",
        "    return idx, float(sims[idx])\n",
        "\n",
        "def emb_best(q: str):\n",
        "    if not USE_EMBEDDINGS: return None, 0.0\n",
        "    v = emb_model.encode([q], normalize_embeddings=True)[0]\n",
        "    sims = (X_emb @ v).ravel()\n",
        "    idx = int(np.argmax(sims))\n",
        "    return idx, float(sims[idx])\n",
        "\n",
        "# -------- Hybrid mapper (no alias) -----\n",
        "def map_open_type(open_type: str, trigger: str, tfidf_threshold=0.33, hybrid_threshold=0.38):\n",
        "    q = norm_type(open_type)\n",
        "\n",
        "    # 1) exact normalized\n",
        "    if q in norm2canon:\n",
        "        return norm2canon[q], \"exact\", 1.0\n",
        "\n",
        "    # 2) light morphology (strip/add ing/ed/s)\n",
        "    variants = {q, re.sub(r'(ings|ing|ed|s)$', '', q)}\n",
        "    for v in list(variants):\n",
        "        variants.add(v+\"ing\"); variants.add(v+\"ed\")\n",
        "    for v in variants:\n",
        "        if v in norm2canon:\n",
        "            return norm2canon[v], \"norm\", 1.0\n",
        "\n",
        "    # 3) fuzzy + jaccard + tf-idf (combine)\n",
        "    #    scale fuzzy (0–100) -> 0–1\n",
        "    fuzzy_scores = [fuzz_ratio(q, t)/100 for t in schema_norm]\n",
        "    jacc_scores  = [jaccard_tokens(q, t) for t in schema_norm]\n",
        "    idx_tfidf, sc_tfidf = tfidf_best(q)\n",
        "\n",
        "    # trigger boost\n",
        "    trig = norm_trigger(trigger)\n",
        "    boost = np.zeros(len(schema_norm))\n",
        "    if trig:\n",
        "        for i, canon in enumerate(schema_types):\n",
        "            # normalized histogram presence\n",
        "            freq = triggers_by_type.get(canon, {}).get(trig, 0)\n",
        "            if freq > 0:\n",
        "                boost[i] = min(0.15, 0.05 + 0.02*math.log1p(freq))  # small boost up to 0.15\n",
        "\n",
        "    # weighted fusion\n",
        "    # give more weight to tf-idf & fuzzy; jaccard and boost are helpers\n",
        "    fused = 0.45*np.array(fuzzy_scores) + 0.40*np.array([tfidf_sim(q)])*0 + 0.0  # placeholder\n",
        "    # we’ll compute tf-idf per index directly to avoid second pass:\n",
        "    # Build array of tf-idf scores\n",
        "    v = tfidf.transform([expand_label(q)])\n",
        "    sims_all = cosine_similarity(v, X_tfidf).ravel()\n",
        "    fused = 0.45*np.array(fuzzy_scores) + 0.40*sims_all + 0.10*np.array(jacc_scores) + boost\n",
        "\n",
        "    best_idx = int(np.argmax(fused))\n",
        "    best_sc  = float(fused[best_idx])\n",
        "\n",
        "    # optional embeddings: take max with embedding score\n",
        "    if USE_EMBEDDINGS:\n",
        "        e_idx, e_sc = emb_best(q)\n",
        "        if e_sc > best_sc:\n",
        "            best_idx, best_sc = e_idx, e_sc\n",
        "            method = \"embed\"\n",
        "        else:\n",
        "            method = \"hybrid\"\n",
        "    else:\n",
        "        method = \"hybrid\"\n",
        "\n",
        "    if best_sc >= hybrid_threshold or sc_tfidf >= tfidf_threshold:\n",
        "        return schema_types[best_idx], method, best_sc\n",
        "\n",
        "    return \"UNMAPPED\", \"unmapped\", best_sc\n",
        "\n",
        "# ---------- Example: run on a few open types ----------\n",
        "examples = [\n",
        "    (\"flood\", \"flooding\"),\n",
        "    (\"death\", \"death\"),\n",
        "    (\"dam break\", \"broke\"),\n",
        "    (\"venture\", \"venture\"),\n",
        "    (\"trapping\", \"trapping\"),\n",
        "    (\"plebiscite\", \"plebiscite\"),\n",
        "]\n",
        "\n",
        "for ot, tr in examples:\n",
        "    mapped, how, score = map_open_type(ot, tr)\n",
        "    print(f\"{ot:12s} -> {mapped:24s}  [{how}  {score:.3f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R3ABEoHlgyP",
        "outputId": "8405ceb1-bc77-47ae-b851-fba60460503d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flood        -> UNMAPPED                  [unmapped  0.247]\n",
            "death        -> Death                     [exact  1.000]\n",
            "dam break    -> UNMAPPED                  [unmapped  0.286]\n",
            "venture      -> UNMAPPED                  [unmapped  0.301]\n",
            "trapping     -> Traveling                 [hybrid  0.438]\n",
            "plebiscite   -> Rite                      [hybrid  0.367]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Post-Processing & Evaluation (per model/version/shots)\n",
        "# - Parses EVENTSEP outputs -> (open_type, trigger)\n",
        "# - Builds allowed schema set from PackB_Gold\n",
        "# - Maps open types -> canonical schema (alias/exact/morph/TF-IDF/[optional: embeddings])\n",
        "# - Evaluates micro Precision/Recall/F1 per (model_name, model_version, condition_shots)\n",
        "# - Writes: Metrics_by_group.csv, Mapped_Results_by_group.csv\n",
        "# ==========================================================\n",
        "\n",
        "import os, re, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --------------------- CONFIG ---------------------\n",
        "# Adjust these paths for your environment\n",
        "RESULTS_PATH   = \"/content/results.xlsx\"          # Your multi-model result table\n",
        "PACKB_GOLD     = \"/content/PackB_Gold.csv\"        # Gold with events_norm\n",
        "SCHEMA_ALIASES = \"/content/packs/Schema_Aliases.csv\"  # Optional (alias,schema_type)\n",
        "OUT_DIR        = \"/content/out_eval\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Optional: semantic embeddings (off by default)\n",
        "USE_EMBEDDINGS = False\n",
        "\n",
        "# TF-IDF & hybrid thresholds; tune for your data\n",
        "TFIDF_THRESHOLD   = 0.35\n",
        "HYBRID_THRESHOLD  = 0.38       # acceptance threshold for hybrid similarity\n",
        "\n",
        "# ------------------ DEPENDENCIES ------------------\n",
        "# Install missing packages only if needed (Colab-safe)\n",
        "try:\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "except Exception:\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except Exception:\n",
        "    !pip -q install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "if USE_EMBEDDINGS:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "    except Exception:\n",
        "        !pip -q install sentence-transformers\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ------------------- NORMALIZERS -------------------\n",
        "def norm_type(s: str) -> str:\n",
        "    \"\"\"Normalize type labels for matching: lowercase, unify separators.\"\"\"\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[_\\-\\s]+', ' ', s)\n",
        "    return s\n",
        "\n",
        "def canon_space_to_underscore(s: str) -> str:\n",
        "    \"\"\"Turn 'process start' -> 'Process_start' for nicer reporting (optional).\"\"\"\n",
        "    if not isinstance(s, str): return s\n",
        "    parts = re.split(r'\\s+', s.strip())\n",
        "    if not parts: return s\n",
        "    return parts[0].capitalize() + ''.join('_'+p for p in parts[1:])\n",
        "\n",
        "def norm_trigger(s: str) -> str:\n",
        "    \"\"\"Normalize triggers for pair-matching with gold.\"\"\"\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[\\.;:,\\-]+$', '', s)  # strip trailing punctuation\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "# ------------------- PARSING -------------------\n",
        "# Match: Event type: <etype>. Trigger: <trig>\n",
        "EVENT_LINE_RE = re.compile(\n",
        "    r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*(?P<etype>[^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*(?P<trig>[^.\\n\\r<]+)',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_eventsep_output(text: str):\n",
        "    \"\"\"Extract (open_type, trigger) pairs from a model output string.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    parts = [p for p in text.split(\"<EVENTSEP>\") if p.strip()] or [text]\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        for m in EVENT_LINE_RE.finditer(p):\n",
        "            et = norm_type(m.group(\"etype\"))\n",
        "            tr = norm_trigger(m.group(\"trig\"))\n",
        "            if et and tr:\n",
        "                out.append((et, tr))\n",
        "    return out\n",
        "\n",
        "# --------------- LOAD INPUT TABLES ---------------\n",
        "# Results: Excel or CSV; must contain chunk_id, model_name, model_version, condition_shots, output_raw\n",
        "ext = Path(RESULTS_PATH).suffix.lower()\n",
        "if ext == \".xlsx\":\n",
        "    results = pd.read_excel(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "else:\n",
        "    results = pd.read_csv(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "required_cols = {\"chunk_id\",\"output_raw\",\"model_name\",\"model_version\",\"condition_shots\"}\n",
        "if not required_cols.issubset(results.columns):\n",
        "    raise ValueError(f\"[Results] required columns missing: {required_cols}\")\n",
        "\n",
        "# Keep only rows where output_raw has content\n",
        "results = results[results[\"output_raw\"].astype(str).str.strip() != \"\"].copy()\n",
        "\n",
        "# Gold: must contain chunk_id, events_norm with format \"Type|trigger ; Type|trigger ; ...\"\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"[Gold] must contain columns: chunk_id, events_norm\")\n",
        "\n",
        "# --------------- BUILD SCHEMA FROM GOLD ---------------\n",
        "# Collect canonical schema labels from gold\n",
        "schema_types = set()\n",
        "triggers_by_type = {}  # trigger frequency per canonical type (for boosting)\n",
        "for s in gold[\"events_norm\"]:\n",
        "    if not isinstance(s, str): continue\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp, tr = item.split(\"|\", 1)\n",
        "        tp = tp.strip()                # canonical (as in gold)\n",
        "        tr_n = norm_trigger(tr)\n",
        "        schema_types.add(tp)\n",
        "        triggers_by_type.setdefault(tp, {})\n",
        "        triggers_by_type[tp][tr_n] = triggers_by_type[tp].get(tr_n, 0) + 1\n",
        "\n",
        "schema_types = sorted(schema_types)         # canonical labels\n",
        "schema_norm  = [norm_type(x) for x in schema_types]\n",
        "norm2canon   = {norm_type(x): x for x in schema_types}\n",
        "\n",
        "# --------------- OPTIONAL: ALIAS TABLE ---------------\n",
        "alias_map = {}\n",
        "if os.path.exists(SCHEMA_ALIASES):\n",
        "    alias_df = pd.read_csv(SCHEMA_ALIASES, dtype=str).fillna(\"\")\n",
        "    if {\"alias\",\"schema_type\"}.issubset(alias_df.columns):\n",
        "        for _, r in alias_df.iterrows():\n",
        "            a = norm_type(r[\"alias\"])\n",
        "            t = str(r[\"schema_type\"]).strip()\n",
        "            if a and t:\n",
        "                alias_map[a] = t  # t should be canonical (as in gold)\n",
        "\n",
        "# --------------- INDEXES FOR SIMILARITY ---------------\n",
        "def expand_label(s: str) -> str:\n",
        "    \"\"\"Help TF-IDF by adding tokenized variants: 'process start' -> 'process start process start'.\"\"\"\n",
        "    toks = re.split(r'[_\\-\\s]+', s)\n",
        "    return \" \".join([s] + toks)\n",
        "\n",
        "# TF-IDF over normalized schema labels (char-ngrams)\n",
        "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_tfidf = tfidf.fit_transform([expand_label(x) for x in schema_norm])\n",
        "\n",
        "# Optional semantic embeddings\n",
        "if USE_EMBEDDINGS:\n",
        "    emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    X_emb = emb_model.encode(schema_norm, normalize_embeddings=True)\n",
        "\n",
        "# --------------- SIMILARITY FUNCTIONS ---------------\n",
        "def jaccard_tokens(a: str, b: str) -> float:\n",
        "    \"\"\"Jaccard similarity over whitespace tokens.\"\"\"\n",
        "    A = set(a.split()); B = set(b.split())\n",
        "    if not A or not B: return 0.0\n",
        "    return len(A & B) / len(A | B)\n",
        "\n",
        "def tfidf_all_sims(q: str) -> np.ndarray:\n",
        "    \"\"\"Vector of TF-IDF cosine similarities to all schema labels.\"\"\"\n",
        "    v = tfidf.transform([expand_label(q)])\n",
        "    sims = cosine_similarity(v, X_tfidf).ravel()\n",
        "    return sims\n",
        "\n",
        "def embeddings_best(q: str):\n",
        "    \"\"\"Best match via sentence embeddings (optional).\"\"\"\n",
        "    if not USE_EMBEDDINGS: return None, 0.0\n",
        "    v = emb_model.encode([q], normalize_embeddings=True)[0]\n",
        "    sims = (X_emb @ v).ravel()\n",
        "    idx = int(np.argmax(sims))\n",
        "    return idx, float(sims[idx])\n",
        "\n",
        "# --------------- MAPPING (no LLM, pure post-process) ---------------\n",
        "def map_open_type(open_type: str, trigger: str):\n",
        "    \"\"\"\n",
        "    Map a free-form open_type to a canonical schema type using:\n",
        "    1) manual alias (if provided)\n",
        "    2) exact normalized label\n",
        "    3) light morphology (strip/add ing/ed/s)\n",
        "    4) hybrid similarity: RapidFuzz (char), TF-IDF(char ngram), Jaccard(tokens), + trigger-frequency boost\n",
        "    5) (optional) embeddings\n",
        "    Returns: (canonical_type or 'UNMAPPED', method, score)\n",
        "    \"\"\"\n",
        "    q = norm_type(open_type)\n",
        "\n",
        "    # 1) alias\n",
        "    if q in alias_map:\n",
        "        return alias_map[q], \"alias\", 1.0\n",
        "\n",
        "    # 2) exact normalized label\n",
        "    if q in norm2canon:\n",
        "        return norm2canon[q], \"exact\", 1.0\n",
        "\n",
        "    # 3) light morphology\n",
        "    variants = {q, re.sub(r'(ings|ing|ed|s)$', '', q)}\n",
        "    for v in list(variants):\n",
        "        variants.add(v + \"ing\"); variants.add(v + \"ed\")\n",
        "    for v in variants:\n",
        "        if v in norm2canon:\n",
        "            return norm2canon[v], \"norm\", 1.0\n",
        "\n",
        "    # 4) hybrid similarity (no alias)\n",
        "    #    Components: fuzzy (RapidFuzz), TF-IDF, Jaccard, trigger-boost\n",
        "    fuzzy_scores = [fuzz_ratio(q, t)/100 for t in schema_norm]\n",
        "    jacc_scores  = [jaccard_tokens(q, t) for t in schema_norm]\n",
        "    tfidf_scores = tfidf_all_sims(q)  # vector\n",
        "\n",
        "    # trigger-based boosting: if this trigger frequently co-occurs with a type in gold\n",
        "    trig = norm_trigger(trigger)\n",
        "    boost = np.zeros(len(schema_norm))\n",
        "    if trig:\n",
        "        for i, canon in enumerate(schema_types):\n",
        "            freq = triggers_by_type.get(canon, {}).get(trig, 0)\n",
        "            if freq > 0:\n",
        "                boost[i] = min(0.15, 0.05 + 0.02*math.log1p(freq))  # cap at 0.15\n",
        "\n",
        "    # weighted fusion (tune weights if needed)\n",
        "    fused = 0.45*np.array(fuzzy_scores) + 0.40*np.array(tfidf_scores) + 0.10*np.array(jacc_scores) + boost\n",
        "    best_idx = int(np.argmax(fused))\n",
        "    best_sc  = float(fused[best_idx])\n",
        "\n",
        "    # Optional embeddings: override if stronger\n",
        "    method = \"hybrid\"\n",
        "    if USE_EMBEDDINGS:\n",
        "        e_idx, e_sc = embeddings_best(q)\n",
        "        if e_sc > best_sc:\n",
        "            best_idx, best_sc = e_idx, e_sc\n",
        "            method = \"embed\"\n",
        "\n",
        "    # Accept if score is strong enough\n",
        "    if best_sc >= HYBRID_THRESHOLD or np.max(tfidf_scores) >= TFIDF_THRESHOLD:\n",
        "        return schema_types[best_idx], method, best_sc\n",
        "\n",
        "    # 5) fallback\n",
        "    return \"UNMAPPED\", \"unmapped\", best_sc\n",
        "\n",
        "# --------------- GOLD / PRED PAIR SETS ---------------\n",
        "def gold_pairs_for_chunk(events_norm_str: str):\n",
        "    \"\"\"Convert 'Type|trigger ; Type|trigger' -> set of (norm_type, norm_trigger).\"\"\"\n",
        "    pairs = set()\n",
        "    s = str(events_norm_str) if isinstance(events_norm_str, str) else \"\"\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp, tr = item.split(\"|\", 1)\n",
        "        pairs.add((norm_type(tp), norm_trigger(tr)))\n",
        "    return pairs\n",
        "\n",
        "# Prebuild gold per chunk\n",
        "gold_by_chunk = {cid: gold_pairs_for_chunk(ev) for cid, ev in zip(gold[\"chunk_id\"], gold[\"events_norm\"])}\n",
        "\n",
        "# --------------- EVALUATION (per model/version/shots) ---------------\n",
        "group_cols = [\"model_name\",\"model_version\",\"condition_shots\"]\n",
        "metrics_rows = []\n",
        "mapped_frames = []\n",
        "\n",
        "for gkeys, gdf in results.groupby(group_cols):\n",
        "    group_info = {k: v for k, v in zip(group_cols, gkeys)}\n",
        "\n",
        "    # Parse & map all events in this group\n",
        "    parsed = []\n",
        "    for _, r in gdf.iterrows():\n",
        "        cid = r[\"chunk_id\"]\n",
        "        events = parse_eventsep_output(r[\"output_raw\"])\n",
        "        for (ot, tr) in events:\n",
        "            parsed.append({\"chunk_id\": cid, \"open_type\": ot, \"trigger\": tr})\n",
        "\n",
        "    if not parsed:\n",
        "        metrics_rows.append({**group_info, \"TP\":0,\"FP\":0,\"FN\":0,\"precision\":0.0,\"recall\":0.0,\"f1\":0.0,\"pred_events\":0})\n",
        "        continue\n",
        "\n",
        "    mapped = []\n",
        "    for pr in parsed:\n",
        "        sch, method, score = map_open_type(pr[\"open_type\"], pr[\"trigger\"])\n",
        "        mapped.append({**pr, \"schema_type\": sch, \"method\": method, \"score\": score})\n",
        "\n",
        "    mapped_df = pd.DataFrame(mapped)\n",
        "\n",
        "    # Build predicted pair-sets per chunk\n",
        "    pred_by_chunk = {}\n",
        "    for cid, sub in mapped_df.groupby(\"chunk_id\"):\n",
        "        pairs = set((norm_type(t), norm_trigger(tr)) for t, tr in zip(sub[\"schema_type\"], sub[\"trigger\"]))\n",
        "        pred_by_chunk[cid] = pairs\n",
        "\n",
        "    # Compute micro TP/FP/FN\n",
        "    TP = FP = FN = 0\n",
        "    for cid in gold_by_chunk:\n",
        "        gset = gold_by_chunk.get(cid, set())\n",
        "        pset = pred_by_chunk.get(cid, set())\n",
        "        TP += len(gset & pset)\n",
        "        FP += len(pset - gset)\n",
        "        FN += len(gset - pset)\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "    recall    = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "    f1        = 2*precision*recall / (precision+recall) if (precision+recall) else 0.0\n",
        "\n",
        "    metrics_rows.append({\n",
        "        **group_info,\n",
        "        \"TP\": TP, \"FP\": FP, \"FN\": FN,\n",
        "        \"precision\": round(precision, 4),\n",
        "        \"recall\":    round(recall, 4),\n",
        "        \"f1\":        round(f1, 4),\n",
        "        \"pred_events\": len(mapped_df)\n",
        "    })\n",
        "\n",
        "    # Keep mapped rows for auditing\n",
        "    mapped_df[\"model_name\"]       = group_info[\"model_name\"]\n",
        "    mapped_df[\"model_version\"]    = group_info[\"model_version\"]\n",
        "    mapped_df[\"condition_shots\"]  = group_info[\"condition_shots\"]\n",
        "    # Optional: beautify schema_type for readability\n",
        "    mapped_df[\"schema_type\"] = mapped_df[\"schema_type\"].apply(lambda x: canon_space_to_underscore(x) if x != \"UNMAPPED\" else x)\n",
        "    mapped_frames.append(mapped_df)\n",
        "\n",
        "# --------------- WRITE OUTPUTS ---------------\n",
        "metrics = pd.DataFrame(metrics_rows).sort_values([\"model_name\",\"model_version\",\"condition_shots\"])\n",
        "metrics_path = f\"{OUT_DIR}/Metrics_by_group.csv\"\n",
        "metrics.to_csv(metrics_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "if mapped_frames:\n",
        "    mapped_all = pd.concat(mapped_frames, ignore_index=True)\n",
        "    mapped_path = f\"{OUT_DIR}/Mapped_Results_by_group.csv\"\n",
        "    mapped_all.to_csv(mapped_path, index=False, encoding=\"utf-8\")\n",
        "else:\n",
        "    mapped_path = None\n",
        "\n",
        "print(f\"[OK] Wrote metrics -> {metrics_path}\")\n",
        "if mapped_path:\n",
        "    print(f\"[OK] Wrote mapped events -> {mapped_path}\")\n",
        "\n",
        "# --------------- QUICK PREVIEW ---------------\n",
        "print(\"\\nPreview: metrics (top 10):\")\n",
        "display(metrics.head(10))\n",
        "\n",
        "if mapped_path:\n",
        "    print(\"\\nPreview: mapped events (top 10):\")\n",
        "    display(mapped_all.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "LjBUnV94rX7V",
        "outputId": "d14fca6e-40b7-4a41-b84a-527d269f1d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Wrote metrics -> /content/out_eval/Metrics_by_group.csv\n",
            "[OK] Wrote mapped events -> /content/out_eval/Mapped_Results_by_group.csv\n",
            "\n",
            "Preview: metrics (top 10):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  model_name model_version condition_shots  TP   FP   FN  precision  recall  \\\n",
              "0    ChatGPT        GPT-4o               0  33  167  252     0.1650  0.1158   \n",
              "1    ChatGPT        GPT-4o               1  43  200  242     0.1770  0.1509   \n",
              "2    ChatGPT        GPT-4o               3  47  193  238     0.1958  0.1649   \n",
              "3    ChatGPT        GPT-4o               5  44  205  241     0.1767  0.1544   \n",
              "4    ChatGPT          GPT5               0  29  179  256     0.1394  0.1018   \n",
              "5    ChatGPT          GPT5               1  41  202  244     0.1687  0.1439   \n",
              "6    ChatGPT          GPT5               3  44  206  241     0.1760  0.1544   \n",
              "7    ChatGPT          GPT5               5  46  213  239     0.1776  0.1614   \n",
              "8   DeepSeek                             0  39  145  246     0.2120  0.1368   \n",
              "9   DeepSeek                             1  50  197  235     0.2024  0.1754   \n",
              "\n",
              "       f1  pred_events  \n",
              "0  0.1361          206  \n",
              "1  0.1629          246  \n",
              "2  0.1790          242  \n",
              "3  0.1648          252  \n",
              "4  0.1176          213  \n",
              "5  0.1553          250  \n",
              "6  0.1645          256  \n",
              "7  0.1691          262  \n",
              "8  0.1663          192  \n",
              "9  0.1880          254  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b41abf2c-8dd4-4924-b047-d1a4da563403\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_version</th>\n",
              "      <th>condition_shots</th>\n",
              "      <th>TP</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>pred_events</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>167</td>\n",
              "      <td>252</td>\n",
              "      <td>0.1650</td>\n",
              "      <td>0.1158</td>\n",
              "      <td>0.1361</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>200</td>\n",
              "      <td>242</td>\n",
              "      <td>0.1770</td>\n",
              "      <td>0.1509</td>\n",
              "      <td>0.1629</td>\n",
              "      <td>246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>3</td>\n",
              "      <td>47</td>\n",
              "      <td>193</td>\n",
              "      <td>238</td>\n",
              "      <td>0.1958</td>\n",
              "      <td>0.1649</td>\n",
              "      <td>0.1790</td>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>205</td>\n",
              "      <td>241</td>\n",
              "      <td>0.1767</td>\n",
              "      <td>0.1544</td>\n",
              "      <td>0.1648</td>\n",
              "      <td>252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT5</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>179</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1394</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1176</td>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT5</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>202</td>\n",
              "      <td>244</td>\n",
              "      <td>0.1687</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.1553</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT5</td>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>206</td>\n",
              "      <td>241</td>\n",
              "      <td>0.1760</td>\n",
              "      <td>0.1544</td>\n",
              "      <td>0.1645</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT5</td>\n",
              "      <td>5</td>\n",
              "      <td>46</td>\n",
              "      <td>213</td>\n",
              "      <td>239</td>\n",
              "      <td>0.1776</td>\n",
              "      <td>0.1614</td>\n",
              "      <td>0.1691</td>\n",
              "      <td>262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DeepSeek</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>145</td>\n",
              "      <td>246</td>\n",
              "      <td>0.2120</td>\n",
              "      <td>0.1368</td>\n",
              "      <td>0.1663</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DeepSeek</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>197</td>\n",
              "      <td>235</td>\n",
              "      <td>0.2024</td>\n",
              "      <td>0.1754</td>\n",
              "      <td>0.1880</td>\n",
              "      <td>254</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b41abf2c-8dd4-4924-b047-d1a4da563403')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b41abf2c-8dd4-4924-b047-d1a4da563403 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b41abf2c-8dd4-4924-b047-d1a4da563403');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8d925d54-8ed6-472a-83a1-f10d98d84215\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d925d54-8ed6-472a-83a1-f10d98d84215')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8d925d54-8ed6-472a-83a1-f10d98d84215 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    display(mapped_all\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"DeepSeek\",\n          \"ChatGPT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_version\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"GPT-4o\",\n          \"GPT5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"condition_shots\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"1\",\n          \"5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 29,\n        \"max\": 50,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          39,\n          43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21,\n        \"min\": 145,\n        \"max\": 213,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          145,\n          200\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 235,\n        \"max\": 256,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          246,\n          242\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020573078417085652,\n        \"min\": 0.1394,\n        \"max\": 0.212,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.212,\n          0.177\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022555070876018594,\n        \"min\": 0.1018,\n        \"max\": 0.1754,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.1368,\n          0.1509\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020300421451563785,\n        \"min\": 0.1176,\n        \"max\": 0.188,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.1663,\n          0.1629\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_events\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 192,\n        \"max\": 262,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          192,\n          246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preview: mapped events (top 10):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   chunk_id        open_type    trigger         schema_type    method  \\\n",
              "0  TST_0001        dam break      broke            UNMAPPED  unmapped   \n",
              "1  TST_0001            flood   flooding            UNMAPPED  unmapped   \n",
              "2  TST_0001         trapping   trapping           Traveling    hybrid   \n",
              "3  TST_0001            death      death               Death     exact   \n",
              "4  TST_0002       toleration  tolerated  Military_operation    hybrid   \n",
              "5  TST_0002              aid    provide            UNMAPPED  unmapped   \n",
              "6  TST_0002            alarm    alarmed            UNMAPPED  unmapped   \n",
              "7  TST_0002  movement/travel         go       Body_movement    hybrid   \n",
              "8  TST_0002          request    request             Request     exact   \n",
              "9  TST_0003        detection   detected              Action    hybrid   \n",
              "\n",
              "      score model_name model_version condition_shots  \n",
              "0  0.286334    ChatGPT        GPT-4o               0  \n",
              "1  0.247092    ChatGPT        GPT-4o               0  \n",
              "2  0.438268    ChatGPT        GPT-4o               0  \n",
              "3  1.000000    ChatGPT        GPT-4o               0  \n",
              "4  0.447596    ChatGPT        GPT-4o               0  \n",
              "5  0.272547    ChatGPT        GPT-4o               0  \n",
              "6  0.373215    ChatGPT        GPT-4o               0  \n",
              "7  0.470750    ChatGPT        GPT-4o               0  \n",
              "8  1.000000    ChatGPT        GPT-4o               0  \n",
              "9  0.487260    ChatGPT        GPT-4o               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33fe19ff-45a8-4422-9144-e15de1388e6b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>open_type</th>\n",
              "      <th>trigger</th>\n",
              "      <th>schema_type</th>\n",
              "      <th>method</th>\n",
              "      <th>score</th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_version</th>\n",
              "      <th>condition_shots</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TST_0001</td>\n",
              "      <td>dam break</td>\n",
              "      <td>broke</td>\n",
              "      <td>UNMAPPED</td>\n",
              "      <td>unmapped</td>\n",
              "      <td>0.286334</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TST_0001</td>\n",
              "      <td>flood</td>\n",
              "      <td>flooding</td>\n",
              "      <td>UNMAPPED</td>\n",
              "      <td>unmapped</td>\n",
              "      <td>0.247092</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TST_0001</td>\n",
              "      <td>trapping</td>\n",
              "      <td>trapping</td>\n",
              "      <td>Traveling</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>0.438268</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TST_0001</td>\n",
              "      <td>death</td>\n",
              "      <td>death</td>\n",
              "      <td>Death</td>\n",
              "      <td>exact</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TST_0002</td>\n",
              "      <td>toleration</td>\n",
              "      <td>tolerated</td>\n",
              "      <td>Military_operation</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>0.447596</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TST_0002</td>\n",
              "      <td>aid</td>\n",
              "      <td>provide</td>\n",
              "      <td>UNMAPPED</td>\n",
              "      <td>unmapped</td>\n",
              "      <td>0.272547</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>TST_0002</td>\n",
              "      <td>alarm</td>\n",
              "      <td>alarmed</td>\n",
              "      <td>UNMAPPED</td>\n",
              "      <td>unmapped</td>\n",
              "      <td>0.373215</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>TST_0002</td>\n",
              "      <td>movement/travel</td>\n",
              "      <td>go</td>\n",
              "      <td>Body_movement</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>0.470750</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TST_0002</td>\n",
              "      <td>request</td>\n",
              "      <td>request</td>\n",
              "      <td>Request</td>\n",
              "      <td>exact</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TST_0003</td>\n",
              "      <td>detection</td>\n",
              "      <td>detected</td>\n",
              "      <td>Action</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>0.487260</td>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>GPT-4o</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33fe19ff-45a8-4422-9144-e15de1388e6b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33fe19ff-45a8-4422-9144-e15de1388e6b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33fe19ff-45a8-4422-9144-e15de1388e6b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e673a9ea-7f47-4a96-9e32-c08c7c177467\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e673a9ea-7f47-4a96-9e32-c08c7c177467')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e673a9ea-7f47-4a96-9e32-c08c7c177467 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    display(mapped_all\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"TST_0001\",\n          \"TST_0002\",\n          \"TST_0003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"open_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"request\",\n          \"flood\",\n          \"aid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trigger\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"request\",\n          \"flooding\",\n          \"provide\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"schema_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"UNMAPPED\",\n          \"Traveling\",\n          \"Request\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"unmapped\",\n          \"hybrid\",\n          \"exact\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2758716576052061,\n        \"min\": 0.24709199532306092,\n        \"max\": 1.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.4707498196292238,\n          0.24709199532306092,\n          0.2725472219604453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ChatGPT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_version\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"GPT-4o\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"condition_shots\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Smart Post-Processing & Evaluation (v2)\n",
        "# ----------------------------------------------------------\n",
        "# Key upgrades vs v1:\n",
        "# 1) Strong trigger-driven mapping:\n",
        "#    - Build a trigger->type prior from GOLD (lemma-level)\n",
        "#    - High-confidence prior (prop >= 0.55 & count >= 3) wins immediately\n",
        "# 2) Hybrid scoring:\n",
        "#    final_score = 0.35*TypeLabelSimilarity + 0.55*TriggerEvidence + 0.10*TokenJaccard\n",
        "#    where TriggerEvidence = normalized trigger prior + TF-IDF(sim(trigger, triggers_of_type))\n",
        "# 3) Robust normalization (light lemmatizer, punctuation cleanup)\n",
        "# 4) Still supports manual aliases (optional), exact, and light-morph matches\n",
        "# 5) Per-(model,version,shots) micro P/R/F1 + full mapped audit\n",
        "# ==========================================================\n",
        "\n",
        "import os, re, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --------------------- CONFIG ---------------------\n",
        "RESULTS_PATH   = \"/content/results.xlsx\"              # your results table\n",
        "PACKB_GOLD     = \"/content/PackB_Gold.csv\"            # gold with 'chunk_id','events_norm'\n",
        "SCHEMA_ALIASES = \"/content/packs/Schema_Aliases.csv\"  # optional alias,schema_type\n",
        "OUT_DIR        = \"/content/out_eval_v2\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Thresholds (tune if needed)\n",
        "PRIOR_PROP_MIN   = 0.55   # min share for trigger's top type to accept prior\n",
        "PRIOR_COUNT_MIN  = 3      # min count for that trigger in gold\n",
        "TFIDF_THRESHOLD  = 0.33   # accept if tf-idf on type-label is >= this\n",
        "FUSED_THRESHOLD  = 0.40   # accept if final fused score is >= this\n",
        "\n",
        "# Optional semantic embeddings (off by default)\n",
        "USE_EMBEDDINGS = False\n",
        "\n",
        "# ------------------ DEPENDENCIES ------------------\n",
        "try:\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "except Exception:\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except Exception:\n",
        "    !pip -q install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "if USE_EMBEDDINGS:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "    except Exception:\n",
        "        !pip -q install sentence-transformers\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------------- NORMALIZATION -------------------\n",
        "def norm_type(s: str) -> str:\n",
        "    \"\"\"Normalize type labels for matching.\"\"\"\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[_\\-\\s]+', ' ', s)\n",
        "    return s\n",
        "\n",
        "def norm_trigger(s: str) -> str:\n",
        "    \"\"\"Normalize triggers for pair matching.\"\"\"\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[.;,:\\-]+$', '', s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "# very light lemmatizer (no external corpora)\n",
        "IRREGULAR = {\n",
        "    \"died\":\"die\", \"was\":\"be\", \"were\":\"be\", \"began\":\"begin\",\n",
        "    \"took\":\"take\", \"torn\":\"tear\", \"led\":\"lead\", \"left\":\"leave\",\n",
        "    \"broke\":\"break\", \"ruled\":\"rule\", \"saw\":\"see\", \"made\":\"make\",\n",
        "}\n",
        "def simple_lemma(token: str) -> str:\n",
        "    \"\"\"Heuristic lemma for triggers.\"\"\"\n",
        "    t = norm_trigger(token)\n",
        "    if t in IRREGULAR: return IRREGULAR[t]\n",
        "    # strip common suffixes\n",
        "    for suf in [\"ing\",\"ed\",\"es\",\"s\"]:\n",
        "        if t.endswith(suf) and len(t) > len(suf)+2:\n",
        "            return t[:-len(suf)]\n",
        "    return t\n",
        "\n",
        "def canon_spaces_to_underscore(s: str) -> str:\n",
        "    \"\"\"'process start' -> 'Process_start' (for nicer reporting).\"\"\"\n",
        "    if not isinstance(s, str): return s\n",
        "    parts = re.split(r'\\s+', s.strip())\n",
        "    if not parts: return s\n",
        "    return parts[0].capitalize() + ''.join('_'+p for p in parts[1:])\n",
        "\n",
        "# ---------------- PARSING -------------------------\n",
        "EVENT_LINE_RE = re.compile(\n",
        "    r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*(?P<etype>[^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*(?P<trig>[^.\\n\\r<]+)',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_eventsep_output(text: str):\n",
        "    \"\"\"Extract (open_type_norm, trigger_norm, trigger_lemma) from one output string.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    parts = [p for p in text.split(\"<EVENTSEP>\") if p.strip()] or [text]\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        for m in EVENT_LINE_RE.finditer(p):\n",
        "            et = norm_type(m.group(\"etype\"))\n",
        "            tr = norm_trigger(m.group(\"trig\"))\n",
        "            if et and tr:\n",
        "                out.append((et, tr, simple_lemma(tr)))\n",
        "    return out\n",
        "\n",
        "# ---------------- LOAD TABLES ---------------------\n",
        "ext = Path(RESULTS_PATH).suffix.lower()\n",
        "if ext == \".xlsx\":\n",
        "    results = pd.read_excel(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "else:\n",
        "    results = pd.read_csv(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "required_cols = {\"chunk_id\",\"output_raw\",\"model_name\",\"model_version\",\"condition_shots\"}\n",
        "if not required_cols.issubset(results.columns):\n",
        "    raise ValueError(f\"[Results] required columns: {required_cols}\")\n",
        "results = results[results[\"output_raw\"].astype(str).str.strip() != \"\"].copy()\n",
        "\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"[Gold] must contain: chunk_id, events_norm\")\n",
        "\n",
        "# --------- SCHEMA + TRIGGER PRIORS FROM GOLD ----------\n",
        "schema_types = set()\n",
        "trig_counts_by_type = {}     # {type_canon: {trig_lemma: count}}\n",
        "trig_global_counts  = {}     # {trig_lemma: total_count}\n",
        "for s in gold[\"events_norm\"]:\n",
        "    if not isinstance(s, str): continue\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp, tr = item.split(\"|\", 1)\n",
        "        tp_canon = tp.strip()\n",
        "        tri = simple_lemma(tr)\n",
        "        schema_types.add(tp_canon)\n",
        "        d = trig_counts_by_type.setdefault(tp_canon, {})\n",
        "        d[tri] = d.get(tri, 0) + 1\n",
        "        trig_global_counts[tri] = trig_global_counts.get(tri, 0) + 1\n",
        "\n",
        "schema_types = sorted(schema_types)              # canonical labels (as in gold)\n",
        "schema_norm  = [norm_type(x) for x in schema_types]\n",
        "norm2canon   = {norm_type(x): x for x in schema_types}\n",
        "\n",
        "# Precompute trigger prior ratios per type\n",
        "# prior[type][trig] = count(type,trig) / sum_over_types count(.,trig)\n",
        "trig_prior = {}\n",
        "for tri, total in trig_global_counts.items():\n",
        "    for tp in schema_types:\n",
        "        c = trig_counts_by_type.get(tp, {}).get(tri, 0)\n",
        "        if c > 0:\n",
        "            trig_prior.setdefault(tp, {})[tri] = c / total\n",
        "\n",
        "# --------------- OPTIONAL: ALIASES ---------------------\n",
        "alias_map = {}\n",
        "if os.path.exists(SCHEMA_ALIASES):\n",
        "    alias_df = pd.read_csv(SCHEMA_ALIASES, dtype=str).fillna(\"\")\n",
        "    if {\"alias\",\"schema_type\"}.issubset(alias_df.columns):\n",
        "        for _, r in alias_df.iterrows():\n",
        "            a = norm_type(r[\"alias\"])\n",
        "            t = str(r[\"schema_type\"]).strip()\n",
        "            if a and t:\n",
        "                alias_map[a] = t  # canonical expected\n",
        "\n",
        "# --------------- SIMILARITY INDEXES --------------------\n",
        "def expand_label(s: str) -> str:\n",
        "    \"\"\"Help TF-IDF by duplicating tokenized variants.\"\"\"\n",
        "    toks = re.split(r'[_\\-\\s]+', s)\n",
        "    return \" \".join([s] + toks)\n",
        "\n",
        "# TF-IDF over normalized schema labels (type names)\n",
        "tfidf_type = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_types = tfidf_type.fit_transform([expand_label(x) for x in schema_norm])\n",
        "\n",
        "# TF-IDF over triggers-of-each-type (join all triggers for that type)\n",
        "type_trigger_corpus = []\n",
        "for tp in schema_types:\n",
        "    bag = []\n",
        "    for tri, cnt in trig_counts_by_type.get(tp, {}).items():\n",
        "        bag += [tri] * cnt  # repeat by frequency\n",
        "    type_trigger_corpus.append(\" \".join(bag) if bag else \"\")\n",
        "tfidf_trig = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_trigs = tfidf_trig.fit_transform(type_trigger_corpus)\n",
        "\n",
        "if USE_EMBEDDINGS:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    X_types_emb = emb_model.encode(schema_norm, normalize_embeddings=True)\n",
        "\n",
        "def tfidf_sim_type_label(q_norm: str) -> np.ndarray:\n",
        "    v = tfidf_type.transform([expand_label(q_norm)])\n",
        "    return cosine_similarity(v, X_types).ravel()\n",
        "\n",
        "def tfidf_sim_trigger_to_type(trig_lemma: str) -> np.ndarray:\n",
        "    \"\"\"Similarity of a trigger lemma to each type's trigger-bag.\"\"\"\n",
        "    v = tfidf_trig.transform([trig_lemma])\n",
        "    return cosine_similarity(v, X_trigs).ravel()\n",
        "\n",
        "# ----------------- MAPPING FUNCTION --------------------\n",
        "def map_open_to_schema(open_type: str, trig: str, trig_lemma: str):\n",
        "    \"\"\"\n",
        "    Map open_type to canonical schema using:\n",
        "    1) Alias (if provided)\n",
        "    2) Exact normalized label\n",
        "    3) Light morphology (strip/add ing/ed/s)\n",
        "    4) High-confidence trigger prior (prop>=PRIOR_PROP_MIN & count>=PRIOR_COUNT_MIN)\n",
        "    5) Hybrid score (type-label similarity + trigger evidence)\n",
        "    \"\"\"\n",
        "    q = norm_type(open_type)\n",
        "\n",
        "    # 1) alias\n",
        "    if q in alias_map:\n",
        "        return alias_map[q], \"alias\", 1.0\n",
        "\n",
        "    # 2) exact match\n",
        "    if q in norm2canon:\n",
        "        return norm2canon[q], \"exact\", 1.0\n",
        "\n",
        "    # 3) morphology\n",
        "    base = re.sub(r'(ings|ing|ed|s)$', '', q)\n",
        "    for cand in {q, base, base+\"ing\", base+\"ed\"}:\n",
        "        if cand in norm2canon:\n",
        "            return norm2canon[cand], \"norm\", 1.0\n",
        "\n",
        "    # 4) Trigger PRIOR (high-confidence)\n",
        "    #    pick top type for this trigger lemma; accept if dominance & support are high\n",
        "    top_tp, top_prop, top_cnt = None, 0.0, 0\n",
        "    total = trig_global_counts.get(trig_lemma, 0)\n",
        "    if total > 0:\n",
        "        for tp in schema_types:\n",
        "            prop = trig_prior.get(tp, {}).get(trig_lemma, 0.0)\n",
        "            if prop > top_prop:\n",
        "                top_tp, top_prop = tp, prop\n",
        "        top_cnt = trig_counts_by_type.get(top_tp, {}).get(trig_lemma, 0) if top_tp else 0\n",
        "        if top_tp and top_prop >= PRIOR_PROP_MIN and top_cnt >= PRIOR_COUNT_MIN:\n",
        "            return top_tp, \"trigger_prior\", float(top_prop)\n",
        "\n",
        "    # 5) Hybrid scoring\n",
        "    # 5.1 Type label similarity (open_type -> schema label)\n",
        "    sim_type_label = tfidf_sim_type_label(q)                         # vector over types\n",
        "    # add a small fuzzy component (scaled to 0..1)\n",
        "    fuzzy = np.array([fuzz_ratio(q, t)/100 for t in schema_norm])\n",
        "    type_label_score = 0.65*sim_type_label + 0.35*fuzzy\n",
        "\n",
        "    # 5.2 Trigger evidence (trigger lemma vs. each type's trigger-bag + prior weight)\n",
        "    sim_trigger_vec = tfidf_sim_trigger_to_type(trig_lemma)\n",
        "    prior_vec = np.array([trig_prior.get(tp, {}).get(trig_lemma, 0.0) for tp in schema_types])\n",
        "    trigger_evidence = 0.6*sim_trigger_vec + 0.4*prior_vec\n",
        "\n",
        "    # 5.3 Token Jaccard (minor)\n",
        "    def jaccard(a, b):\n",
        "        A, B = set(a.split()), set(b.split())\n",
        "        return len(A & B)/len(A | B) if A and B else 0.0\n",
        "    jacc = np.array([jaccard(q, t) for t in schema_norm])\n",
        "\n",
        "    fused = 0.35*type_label_score + 0.55*trigger_evidence + 0.10*jacc\n",
        "\n",
        "    # Optional embeddings: override if stronger\n",
        "    method = \"hybrid\"\n",
        "    if USE_EMBEDDINGS:\n",
        "        v = emb_model.encode([q], normalize_embeddings=True)[0]\n",
        "        sims = (X_types_emb @ v).ravel()\n",
        "        # soft fuse\n",
        "        fused = 0.7*fused + 0.3*sims\n",
        "        method = \"hybrid+embed\"\n",
        "\n",
        "    best_idx = int(np.argmax(fused))\n",
        "    best_score = float(fused[best_idx])\n",
        "\n",
        "    # Accept if fused score or type-label tf-idf is strong enough\n",
        "    if best_score >= FUSED_THRESHOLD or float(np.max(sim_type_label)) >= TFIDF_THRESHOLD:\n",
        "        return schema_types[best_idx], method, best_score\n",
        "\n",
        "    return \"UNMAPPED\", \"unmapped\", best_score\n",
        "\n",
        "# --------------- GOLD PAIRSETS --------------------\n",
        "def gold_pairs_for_chunk(events_norm_str: str):\n",
        "    \"\"\"'Type|trigger ; Type|trigger' -> set of (norm_type, norm_trigger).\"\"\"\n",
        "    pairs = set()\n",
        "    s = str(events_norm_str) if isinstance(events_norm_str, str) else \"\"\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp, tr = item.split(\"|\", 1)\n",
        "        pairs.add((norm_type(tp), norm_trigger(tr)))\n",
        "    return pairs\n",
        "\n",
        "gold_by_chunk = {cid: gold_pairs_for_chunk(ev) for cid, ev in zip(gold[\"chunk_id\"], gold[\"events_norm\"])}\n",
        "\n",
        "# --------------- EVALUATION (per group) -------------\n",
        "group_cols = [\"model_name\",\"model_version\",\"condition_shots\"]\n",
        "metrics_rows, mapped_frames = [], []\n",
        "\n",
        "for gkeys, gdf in results.groupby(group_cols):\n",
        "    group = dict(zip(group_cols, gkeys))\n",
        "\n",
        "    parsed = []\n",
        "    for _, r in gdf.iterrows():\n",
        "        cid = r[\"chunk_id\"]\n",
        "        for (ot, tr, tri_lemma) in parse_eventsep_output(r[\"output_raw\"]):\n",
        "            parsed.append({\"chunk_id\": cid, \"open_type\": ot, \"trigger\": tr, \"trig_lemma\": tri_lemma})\n",
        "\n",
        "    if not parsed:\n",
        "        metrics_rows.append({**group, \"TP\":0,\"FP\":0,\"FN\":0,\"precision\":0.0,\"recall\":0.0,\"f1\":0.0,\"pred_events\":0})\n",
        "        continue\n",
        "\n",
        "    mapped = []\n",
        "    for pr in parsed:\n",
        "        sch, how, sc = map_open_to_schema(pr[\"open_type\"], pr[\"trigger\"], pr[\"trig_lemma\"])\n",
        "        mapped.append({**pr, \"schema_type\": sch, \"method\": how, \"score\": sc})\n",
        "    mapped_df = pd.DataFrame(mapped)\n",
        "\n",
        "    # build predicted pairs per chunk\n",
        "    pred_by_chunk = {}\n",
        "    for cid, sub in mapped_df.groupby(\"chunk_id\"):\n",
        "        pairs = set((norm_type(t), norm_trigger(tr)) for t, tr in zip(sub[\"schema_type\"], sub[\"trigger\"]))\n",
        "        pred_by_chunk[cid] = pairs\n",
        "\n",
        "    # micro TP/FP/FN\n",
        "    TP = FP = FN = 0\n",
        "    for cid in gold_by_chunk:\n",
        "        gset = gold_by_chunk.get(cid, set())\n",
        "        pset = pred_by_chunk.get(cid, set())\n",
        "        TP += len(gset & pset)\n",
        "        FP += len(pset - gset)\n",
        "        FN += len(gset - pset)\n",
        "\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F1 = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "\n",
        "    metrics_rows.append({**group, \"TP\":TP,\"FP\":FP,\"FN\":FN,\n",
        "                         \"precision\":round(P,4),\"recall\":round(R,4),\"f1\":round(F1,4),\n",
        "                         \"pred_events\":len(mapped_df)})\n",
        "\n",
        "    mapped_df[\"model_name\"]      = group[\"model_name\"]\n",
        "    mapped_df[\"model_version\"]   = group[\"model_version\"]\n",
        "    mapped_df[\"condition_shots\"] = group[\"condition_shots\"]\n",
        "    mapped_df[\"schema_type\"]     = mapped_df[\"schema_type\"].apply(lambda x: canon_spaces_to_underscore(x) if x!=\"UNMAPPED\" else x)\n",
        "    mapped_frames.append(mapped_df)\n",
        "\n",
        "# --------------- WRITE OUTPUTS -------------------\n",
        "metrics = pd.DataFrame(metrics_rows).sort_values(group_cols)\n",
        "metrics_path = f\"{OUT_DIR}/Metrics_by_group.csv\"\n",
        "metrics.to_csv(metrics_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "if mapped_frames:\n",
        "    mapped_all = pd.concat(mapped_frames, ignore_index=True)\n",
        "    mapped_path = f\"{OUT_DIR}/Mapped_Results_by_group.csv\"\n",
        "    mapped_all.to_csv(mapped_path, index=False, encoding=\"utf-8\")\n",
        "else:\n",
        "    mapped_path = None\n",
        "\n",
        "print(f\"[OK] Wrote metrics -> {metrics_path}\")\n",
        "if mapped_path:\n",
        "    print(f\"[OK] Wrote mapped events -> {mapped_path}\")\n",
        "\n",
        "# Optional: overall micro\n",
        "TP, FP, FN = metrics[\"TP\"].sum(), metrics[\"FP\"].sum(), metrics[\"FN\"].sum()\n",
        "P  = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "R  = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "F1 = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "with open(f\"{OUT_DIR}/Metrics_overall_micro.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Micro Precision: {P:.4f}\\nMicro Recall: {R:.4f}\\nMicro F1: {F1:.4f}\\nTP={TP}, FP={FP}, FN={FN}\\n\")\n",
        "print(f\"[OK] Overall micro -> P={P:.4f} R={R:.4f} F1={F1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjR-T_oVs-GR",
        "outputId": "463b2c1f-e573-4177-f93b-ceba4b267068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Wrote metrics -> /content/out_eval_v2/Metrics_by_group.csv\n",
            "[OK] Wrote mapped events -> /content/out_eval_v2/Mapped_Results_by_group.csv\n",
            "[OK] Overall micro -> P=0.4150 R=0.3333 F1=0.3697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Ultra-Smart Post-Processing & Evaluation (Ensembled + Tuner)\n",
        "# ----------------------------------------------------------\n",
        "# Signals used for mapping open_type -> canonical schema:\n",
        "#  1) Manual aliases (optional)\n",
        "#  2) Exact / light-morph match (death ~ dying ~ died)\n",
        "#  3) Type-Label similarity (TF-IDF char n-grams + fuzzy + token Jaccard)\n",
        "#  4) Trigger priors learned from GOLD (lemma-level dominance + frequency)\n",
        "#  5) Trigger → Type similarity (TF-IDF: trigger vs bag-of-triggers per type)\n",
        "#  6) Context similarity (TF-IDF: CHUNK TEXT vs bag-of-texts per type)  [requires PackA]\n",
        "#  7) Optional embeddings for type labels (SentenceTransformers)\n",
        "#\n",
        "# Fusion:\n",
        "#  final_score = w1*TypeLabel + w2*TriggerEvidence + w3*Context + w4*Jaccard\n",
        "#  where TriggerEvidence mixes (trigger prior + trigger TF-IDF).\n",
        "#\n",
        "# A lightweight tuner searches weights to maximize micro F1 on a dev split.\n",
        "# Then evaluates per (model_name, model_version, condition_shots).\n",
        "# ==========================================================\n",
        "\n",
        "import os, re, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "RESULTS_PATH   = \"/content/results.xlsx\"\n",
        "PACKB_GOLD     = \"/content/PackB_Gold.csv\"\n",
        "PACKA_PATH     = \"/content/packs/PackA_TextChunks.csv\"      # optional\n",
        "SCHEMA_ALIASES = \"/content/packs/Schema_Aliases.csv\"        # optional\n",
        "OUT_DIR        = \"/content/out_eval_ultra\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Tuning and thresholds\n",
        "USE_TUNER        = True     # set False to skip weight search\n",
        "DEV_FRACTION     = 0.25     # fraction of chunks for tuning (stratified by density if PackA available)\n",
        "N_TRIALS         = 60       # random search trials\n",
        "FUSED_MIN_ACCEPT = 0.38     # minimum fused score to accept a mapping\n",
        "TFIDF_MIN_ACCEPT = 0.33     # minimum type-label tfidf to accept (guard)\n",
        "\n",
        "# Optional semantic embeddings for type label similarity (off by default)\n",
        "USE_EMBEDDINGS   = False\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# -------------- Dependencies (install-once) --------------\n",
        "try:\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "except Exception:\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except Exception:\n",
        "    !pip -q install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "if USE_EMBEDDINGS:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "    except Exception:\n",
        "        !pip -q install sentence-transformers\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ----------------- Normalizers -----------------\n",
        "def norm_type(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[_\\-\\s]+', ' ', s)\n",
        "    return s\n",
        "\n",
        "def norm_trigger(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[.;,:\\-]+$', '', s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "IRREGULAR = {\n",
        "    \"died\":\"die\", \"was\":\"be\", \"were\":\"be\", \"began\":\"begin\",\n",
        "    \"took\":\"take\", \"torn\":\"tear\", \"led\":\"lead\", \"left\":\"leave\",\n",
        "    \"broke\":\"break\", \"ruled\":\"rule\", \"saw\":\"see\", \"made\":\"make\",\n",
        "}\n",
        "def simple_lemma(token: str) -> str:\n",
        "    t = norm_trigger(token)\n",
        "    if t in IRREGULAR: return IRREGULAR[t]\n",
        "    for suf in [\"ing\",\"ed\",\"es\",\"s\"]:\n",
        "        if t.endswith(suf) and len(t) > len(suf)+2:\n",
        "            return t[:-len(suf)]\n",
        "    return t\n",
        "\n",
        "def canon_spaces_to_underscore(s: str) -> str:\n",
        "    if not isinstance(s, str): return s\n",
        "    parts = re.split(r'\\s+', s.strip())\n",
        "    if not parts: return s\n",
        "    return parts[0].capitalize() + ''.join('_'+p for p in parts[1:])\n",
        "\n",
        "# ----------------- Parsing ---------------------\n",
        "EVENT_LINE_RE = re.compile(\n",
        "    r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*(?P<etype>[^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*(?P<trig>[^.\\n\\r<]+)',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_eventsep_output(text: str):\n",
        "    if not isinstance(text, str) or not text.strip(): return []\n",
        "    parts = [p for p in text.split(\"<EVENTSEP>\") if p.strip()] or [text]\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        for m in EVENT_LINE_RE.finditer(p):\n",
        "            et = norm_type(m.group(\"etype\"))\n",
        "            tr = norm_trigger(m.group(\"trig\"))\n",
        "            if et and tr:\n",
        "                out.append((et, tr, simple_lemma(tr)))\n",
        "    return out\n",
        "\n",
        "# --------------- Load tables -------------------\n",
        "ext = Path(RESULTS_PATH).suffix.lower()\n",
        "if ext == \".xlsx\":\n",
        "    results = pd.read_excel(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "else:\n",
        "    results = pd.read_csv(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "req = {\"chunk_id\",\"output_raw\",\"model_name\",\"model_version\",\"condition_shots\"}\n",
        "if not req.issubset(results.columns):\n",
        "    raise ValueError(f\"[Results] must contain: {req}\")\n",
        "results = results[results[\"output_raw\"].astype(str).str.strip() != \"\"].copy()\n",
        "\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"[Gold] must contain: chunk_id, events_norm\")\n",
        "\n",
        "# Optional PackA for context text\n",
        "packA = None\n",
        "if os.path.exists(PACKA_PATH):\n",
        "    packA = pd.read_csv(PACKA_PATH, dtype=str).fillna(\"\")\n",
        "    if not {\"chunk_id\",\"text\"}.issubset(packA.columns):\n",
        "        packA = None\n",
        "\n",
        "# ------------- Schema + priors from GOLD -------------\n",
        "schema_types = set()\n",
        "trig_counts_by_type = {}\n",
        "trig_global_counts  = {}\n",
        "chunks_by_type      = {}   # for context (list of chunk_ids)\n",
        "\n",
        "for cid, s in zip(gold[\"chunk_id\"], gold[\"events_norm\"]):\n",
        "    if not isinstance(s, str): continue\n",
        "    mentioned_types = set()\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp, tr = item.split(\"|\", 1)\n",
        "        tp_canon = tp.strip()\n",
        "        tri = simple_lemma(tr)\n",
        "\n",
        "        schema_types.add(tp_canon)\n",
        "        d = trig_counts_by_type.setdefault(tp_canon, {})\n",
        "        d[tri] = d.get(tri, 0) + 1\n",
        "        trig_global_counts[tri] = trig_global_counts.get(tri, 0) + 1\n",
        "\n",
        "        mentioned_types.add(tp_canon)\n",
        "    for tp in mentioned_types:\n",
        "        chunks_by_type.setdefault(tp, []).append(cid)\n",
        "\n",
        "schema_types = sorted(schema_types)\n",
        "schema_norm  = [norm_type(x) for x in schema_types]\n",
        "norm2canon   = {norm_type(x): x for x in schema_types}\n",
        "\n",
        "# Trigger prior ratios\n",
        "trig_prior = {}\n",
        "for tri, total in trig_global_counts.items():\n",
        "    for tp in schema_types:\n",
        "        c = trig_counts_by_type.get(tp, {}).get(tri, 0)\n",
        "        if c > 0:\n",
        "            trig_prior.setdefault(tp, {})[tri] = c / total\n",
        "\n",
        "# ------------- Alias table (optional) -------------\n",
        "alias_map = {}\n",
        "if os.path.exists(SCHEMA_ALIASES):\n",
        "    alias_df = pd.read_csv(SCHEMA_ALIASES, dtype=str).fillna(\"\")\n",
        "    if {\"alias\",\"schema_type\"}.issubset(alias_df.columns):\n",
        "        for _, r in alias_df.iterrows():\n",
        "            a = norm_type(r[\"alias\"])\n",
        "            t = str(r[\"schema_type\"]).strip()\n",
        "            if a and t:\n",
        "                alias_map[a] = t\n",
        "\n",
        "# ------------- Vector spaces (TF-IDF) -------------\n",
        "def expand_label(s: str) -> str:\n",
        "    toks = re.split(r'[_\\-\\s]+', s)\n",
        "    return \" \".join([s] + toks)\n",
        "\n",
        "# Type label TF-IDF\n",
        "tfidf_type = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_types = tfidf_type.fit_transform([expand_label(x) for x in schema_norm])\n",
        "\n",
        "# Trigger bag TF-IDF (per type)\n",
        "type_trigger_docs = []\n",
        "for tp in schema_types:\n",
        "    bag = []\n",
        "    for tri, cnt in trig_counts_by_type.get(tp, {}).items():\n",
        "        bag += [tri] * cnt\n",
        "    type_trigger_docs.append(\" \".join(bag))\n",
        "tfidf_trig = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_trigs = tfidf_trig.fit_transform(type_trigger_docs)\n",
        "\n",
        "# Context TF-IDF (per type) using chunk texts (if PackA available)\n",
        "X_ctx = None\n",
        "if packA is not None:\n",
        "    chunk_text = dict(zip(packA[\"chunk_id\"], packA[\"text\"]))\n",
        "    type_ctx_docs = []\n",
        "    for tp in schema_types:\n",
        "        texts = [chunk_text[cid] for cid in chunks_by_type.get(tp, []) if cid in chunk_text]\n",
        "        type_ctx_docs.append(\" \".join(texts) if texts else \"\")\n",
        "    tfidf_ctx = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "    X_ctx = tfidf_ctx.fit_transform(type_ctx_docs)\n",
        "\n",
        "# Optional embeddings for type label\n",
        "if USE_EMBEDDINGS:\n",
        "    emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    X_types_emb = emb_model.encode(schema_norm, normalize_embeddings=True)\n",
        "\n",
        "# ------------- Similarity helpers -------------\n",
        "def tfidf_type_sim(q_norm: str) -> np.ndarray:\n",
        "    v = tfidf_type.transform([expand_label(q_norm)])\n",
        "    return cosine_similarity(v, X_types).ravel()\n",
        "\n",
        "def tfidf_trigger_sim(tri: str) -> np.ndarray:\n",
        "    v = tfidf_trig.transform([tri])\n",
        "    return cosine_similarity(v, X_trigs).ravel()\n",
        "\n",
        "def tfidf_context_sim(text: str) -> np.ndarray:\n",
        "    if X_ctx is None: return np.zeros(len(schema_types))\n",
        "    v = tfidf_ctx.transform([text])\n",
        "    return cosine_similarity(v, X_ctx).ravel()\n",
        "\n",
        "def token_jaccard(a: str, b: str) -> float:\n",
        "    A, B = set(a.split()), set(b.split())\n",
        "    return len(A & B)/len(A | B) if A and B else 0.0\n",
        "\n",
        "# ------------- Mapping core (parametric) -------------\n",
        "def map_one(open_type, trig, tri_lemma, chunk_text, weights):\n",
        "    \"\"\"\n",
        "    weights: dict with keys {'w_type','w_trigger','w_ctx','w_jacc'} summing ~1\n",
        "    \"\"\"\n",
        "    q = norm_type(open_type)\n",
        "\n",
        "    # 0) alias\n",
        "    if q in alias_map:\n",
        "        return alias_map[q], \"alias\", 1.0\n",
        "\n",
        "    # 1) exact / morph\n",
        "    if q in norm2canon:\n",
        "        return norm2canon[q], \"exact\", 1.0\n",
        "    base = re.sub(r'(ings|ing|ed|s)$', '', q)\n",
        "    for cand in {q, base, base+\"ing\", base+\"ed\"}:\n",
        "        if cand in norm2canon:\n",
        "            return norm2canon[cand], \"norm\", 1.0\n",
        "\n",
        "    # 2) type-label similarity\n",
        "    sims_type = tfidf_type_sim(q)\n",
        "    fuzzy = np.array([fuzz_ratio(q, t)/100 for t in schema_norm])\n",
        "    jacc  = np.array([token_jaccard(q, t) for t in schema_norm])\n",
        "    type_signal = 0.70*sims_type + 0.30*fuzzy\n",
        "\n",
        "    # 3) trigger evidence\n",
        "    sims_trig = tfidf_trigger_sim(tri_lemma)\n",
        "    prior_vec = np.array([trig_prior.get(tp, {}).get(tri_lemma, 0.0) for tp in schema_types])\n",
        "    trigger_signal = 0.60*sims_trig + 0.40*prior_vec\n",
        "\n",
        "    # 4) context (if available)\n",
        "    ctx_signal = tfidf_context_sim(chunk_text) if isinstance(chunk_text, str) else np.zeros(len(schema_types))\n",
        "\n",
        "    # 5) optional embeddings (small blend into type signal)\n",
        "    if USE_EMBEDDINGS:\n",
        "        v = emb_model.encode([q], normalize_embeddings=True)[0]\n",
        "        sims_emb = (X_types_emb @ v).ravel()\n",
        "        type_signal = 0.85*type_signal + 0.15*sims_emb\n",
        "\n",
        "    # 6) fuse\n",
        "    fused = (weights[\"w_type\"]*type_signal +\n",
        "             weights[\"w_trigger\"]*trigger_signal +\n",
        "             weights[\"w_ctx\"]*ctx_signal +\n",
        "             weights[\"w_jacc\"]*jacc)\n",
        "\n",
        "    best_idx = int(np.argmax(fused))\n",
        "    best_sc  = float(fused[best_idx])\n",
        "\n",
        "    # guard: accept if fused strong or type tfidf strong\n",
        "    if best_sc >= FUSED_MIN_ACCEPT or float(np.max(sims_type)) >= TFIDF_MIN_ACCEPT:\n",
        "        return schema_types[best_idx], \"fused\", best_sc\n",
        "    return \"UNMAPPED\", \"unmapped\", best_sc\n",
        "\n",
        "# ------------- Utility: score a set (micro) -------------\n",
        "def micro_scores(gold_pairs_by_chunk, mapped_rows):\n",
        "    # build pred pairs\n",
        "    pred_by_chunk = {}\n",
        "    for cid, sub in mapped_rows.groupby(\"chunk_id\"):\n",
        "        pairs = set((norm_type(t), norm_trigger(tr)) for t,tr in zip(sub[\"schema_type\"], sub[\"trigger\"]))\n",
        "        pred_by_chunk[cid] = pairs\n",
        "    TP=FP= FN=0\n",
        "    for cid in gold_pairs_by_chunk:\n",
        "        gset = gold_pairs_by_chunk.get(cid, set())\n",
        "        pset = pred_by_chunk.get(cid, set())\n",
        "        TP += len(gset & pset)\n",
        "        FP += len(pset - gset)\n",
        "        FN += len(gset - pset)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F1= 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return P,R,F1,TP,FP,FN\n",
        "\n",
        "def gold_pairs_for_chunk(s: str):\n",
        "    out=set()\n",
        "    for item in str(s).split(\";\"):\n",
        "        item=item.strip()\n",
        "        if \"|\" in item:\n",
        "            tp,tr=item.split(\"|\",1)\n",
        "            out.add((norm_type(tp), norm_trigger(tr)))\n",
        "    return out\n",
        "\n",
        "gold_by_chunk = {cid: gold_pairs_for_chunk(ev) for cid,ev in zip(gold[\"chunk_id\"], gold[\"events_norm\"])}\n",
        "\n",
        "# ------------- Build a unified parsed frame -------------\n",
        "# Merge chunk text (if available)\n",
        "text_map = {}\n",
        "if packA is not None:\n",
        "    text_map = dict(zip(packA[\"chunk_id\"], packA[\"text\"]))\n",
        "elif \"text\" in results.columns:\n",
        "    text_map = dict(zip(results[\"chunk_id\"], results[\"text\"]))\n",
        "\n",
        "parsed_all = []\n",
        "for _, r in results.iterrows():\n",
        "    cid = r[\"chunk_id\"]\n",
        "    txt = text_map.get(cid, \"\")\n",
        "    for (ot, tr, tri_lemma) in parse_eventsep_output(r[\"output_raw\"]):\n",
        "        parsed_all.append({\n",
        "            \"chunk_id\": cid,\n",
        "            \"model_name\": r[\"model_name\"],\n",
        "            \"model_version\": r[\"model_version\"],\n",
        "            \"condition_shots\": r[\"condition_shots\"],\n",
        "            \"open_type\": ot,\n",
        "            \"trigger\": tr,\n",
        "            \"tri_lemma\": tri_lemma,\n",
        "            \"chunk_text\": txt\n",
        "        })\n",
        "parsed_df = pd.DataFrame(parsed_all)\n",
        "if parsed_df.empty:\n",
        "    raise RuntimeError(\"No events parsed from outputs. Check 'output_raw' formatting.\")\n",
        "\n",
        "# ------------- Split dev/test for tuning -------------\n",
        "all_cids = parsed_df[\"chunk_id\"].unique().tolist()\n",
        "random.shuffle(all_cids)\n",
        "n_dev = int(len(all_cids)*DEV_FRACTION) if USE_TUNER else 0\n",
        "dev_ids = set(all_cids[:n_dev])\n",
        "test_ids= set(all_cids[n_dev:])\n",
        "\n",
        "dev_df  = parsed_df[parsed_df[\"chunk_id\"].isin(dev_ids)].copy() if USE_TUNER else None\n",
        "test_df = parsed_df[parsed_df[\"chunk_id\"].isin(test_ids)].copy() if USE_TUNER else parsed_df.copy()\n",
        "\n",
        "# ------------- Random weight tuner -------------\n",
        "def random_weights():\n",
        "    # draw Dirichlet-like weights over 4 components\n",
        "    xs = np.random.rand(4)\n",
        "    xs = xs/np.sum(xs)\n",
        "    return {\"w_type\":xs[0], \"w_trigger\":xs[1], \"w_ctx\":xs[2], \"w_jacc\":xs[3]}\n",
        "\n",
        "best_w = {\"w_type\":0.40, \"w_trigger\":0.45, \"w_ctx\":0.10, \"w_jacc\":0.05}  # sensible default\n",
        "\n",
        "if USE_TUNER and not dev_df.empty:\n",
        "    best_f1 = -1.0\n",
        "    # assemble gold subset for fast scoring\n",
        "    dev_gold = {cid: gold_by_chunk.get(cid,set()) for cid in dev_ids}\n",
        "    for _ in range(N_TRIALS):\n",
        "        w = random_weights()\n",
        "        mapped_rows = []\n",
        "        for _, row in dev_df.iterrows():\n",
        "            sch, how, sc = map_one(row[\"open_type\"], row[\"trigger\"], row[\"tri_lemma\"], row[\"chunk_text\"], w)\n",
        "            mapped_rows.append({\"chunk_id\":row[\"chunk_id\"],\"schema_type\":sch,\"trigger\":row[\"trigger\"]})\n",
        "        mapped_rows = pd.DataFrame(mapped_rows)\n",
        "        P,R,F1,_,_,_ = micro_scores(dev_gold, mapped_rows)\n",
        "        if F1 > best_f1:\n",
        "            best_f1 = F1\n",
        "            best_w  = w\n",
        "    print(f\"[TUNER] Best weights on dev: {best_w}  (F1={best_f1:.4f})\")\n",
        "else:\n",
        "    print(f\"[TUNER] Skipped. Using default weights: {best_w}\")\n",
        "\n",
        "# ------------- Evaluate per (model/version/shots) -------------\n",
        "group_cols = [\"model_name\",\"model_version\",\"condition_shots\"]\n",
        "metrics_rows = []\n",
        "mapped_frames = []\n",
        "\n",
        "for gkeys, gsub in parsed_df.groupby(group_cols):\n",
        "    ginfo = dict(zip(group_cols, gkeys))\n",
        "    mapped = []\n",
        "    for _, row in gsub.iterrows():\n",
        "        sch, how, sc = map_one(row[\"open_type\"], row[\"trigger\"], row[\"tri_lemma\"], row[\"chunk_text\"], best_w)\n",
        "        mapped.append({\n",
        "            \"chunk_id\": row[\"chunk_id\"],\n",
        "            \"schema_type\": sch,\n",
        "            \"trigger\": row[\"trigger\"],\n",
        "            \"open_type\": row[\"open_type\"],\n",
        "            \"method\": how,\n",
        "            \"score\": sc\n",
        "        })\n",
        "    mapped = pd.DataFrame(mapped)\n",
        "\n",
        "    P,R,F1,TP,FP,FN = micro_scores(gold_by_chunk, mapped)\n",
        "    metrics_rows.append({**ginfo,\n",
        "                         \"TP\":TP,\"FP\":FP,\"FN\":FN,\n",
        "                         \"precision\":round(P,4),\"recall\":round(R,4),\"f1\":round(F1,4),\n",
        "                         \"pred_events\":len(mapped)})\n",
        "\n",
        "    mapped[\"model_name\"]      = ginfo[\"model_name\"]\n",
        "    mapped[\"model_version\"]   = ginfo[\"model_version\"]\n",
        "    mapped[\"condition_shots\"] = ginfo[\"condition_shots\"]\n",
        "    mapped[\"schema_type\"]     = mapped[\"schema_type\"].apply(lambda x: canon_spaces_to_underscore(x) if x!=\"UNMAPPED\" else x)\n",
        "    mapped_frames.append(mapped)\n",
        "\n",
        "# ------------- Write outputs --------------------\n",
        "metrics = pd.DataFrame(metrics_rows).sort_values(group_cols)\n",
        "metrics_path = f\"{OUT_DIR}/Metrics_by_group.csv\"\n",
        "metrics.to_csv(metrics_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "mapped_all = pd.concat(mapped_frames, ignore_index=True) if mapped_frames else pd.DataFrame()\n",
        "if not mapped_all.empty:\n",
        "    mapped_path = f\"{OUT_DIR}/Mapped_Results_by_group.csv\"\n",
        "    mapped_all.to_csv(mapped_path, index=False, encoding=\"utf-8\")\n",
        "else:\n",
        "    mapped_path = None\n",
        "\n",
        "print(f\"[OK] Wrote metrics -> {metrics_path}\")\n",
        "if mapped_path:\n",
        "    print(f\"[OK] Wrote mapped events -> {mapped_path}\")\n",
        "\n",
        "# Overall micro\n",
        "TP,FP,FN = metrics[\"TP\"].sum(), metrics[\"FP\"].sum(), metrics[\"FN\"].sum()\n",
        "P  = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "R  = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "F1 = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "with open(f\"{OUT_DIR}/Metrics_overall_micro.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Micro Precision: {P:.4f}\\nMicro Recall: {R:.4f}\\nMicro F1: {F1:.4f}\\nTP={TP}, FP={FP}, FN={FN}\\n\")\n",
        "print(f\"[OK] Overall micro -> P={P:.4f} R={R:.4f} F1={F1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-VG4xwu2gBc",
        "outputId": "35f940f8-14cd-426c-fd52-9b521733cd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m3.2/3.3 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[TUNER] Best weights on dev: {'w_type': np.float64(0.06301231571166711), 'w_trigger': np.float64(0.5829491484483261), 'w_ctx': np.float64(0.22386915722376707), 'w_jacc': np.float64(0.13016937861623953)}  (F1=0.2428)\n",
            "[OK] Wrote metrics -> /content/out_eval_ultra/Metrics_by_group.csv\n",
            "[OK] Wrote mapped events -> /content/out_eval_ultra/Mapped_Results_by_group.csv\n",
            "[OK] Overall micro -> P=0.4060 R=0.3261 F1=0.3617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Cross-Model Self-Consistency Ensemble (CMSE)\n",
        "#   - Maps each raw output to schema types (smart mapper)\n",
        "#   - Learns reliability weights per (model,version,shots)\n",
        "#   - Aggregates predictions across all systems per chunk\n",
        "#   - Selects a consensus set (duplicate-aware, trigger-checked)\n",
        "#   - Evaluates micro P/R/F1\n",
        "# ==========================================================\n",
        "\n",
        "import os, re, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "RESULTS_PATH   = \"/content/results.xlsx\"              # must have chunk_id, model_name, model_version, condition_shots, output_raw\n",
        "PACKB_GOLD     = (\"/content/PackB_Gold.csv\")          # must have chunk_id, events_norm\n",
        "PACKA_PATH     = \"/content/packs/PackA_TextChunks.csv\" # optional (for trigger validation)\n",
        "SCHEMA_ALIASES = \"/content/packs/Schema_Aliases.csv\"   # optional (alias,schema_type)\n",
        "\n",
        "OUT_DIR        = \"/content/out_cmse\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dev split for tuning reliabilities\n",
        "DEV_FRAC        = 0.25\n",
        "N_TRIALS_WEIGHTS= 60\n",
        "\n",
        "# Thresholds\n",
        "FUSED_MIN_ACCEPT = 0.38   # mapping accept guard (same as ultra-smart)\n",
        "TYPE_TFIDF_GUARD = 0.33\n",
        "CONS_TAU_BASE    = 0.75   # base consensus threshold (scaled)\n",
        "NEAR_DUP_FUZZ    = 90     # triggers >= this fuzzy similarity are treated as duplicates\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------------- Dependencies ----------------\n",
        "try:\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "except Exception:\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except Exception:\n",
        "    !pip -q install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ---------------- Normalizers -----------------\n",
        "def ntype(s):\n",
        "    if not isinstance(s,str): return \"\"\n",
        "    return re.sub(r'[_\\-\\s]+',' ', s.strip().lower())\n",
        "\n",
        "def ntrig(s):\n",
        "    if not isinstance(s,str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[.;,:\\-]+$','', s)\n",
        "    return re.sub(r'\\s+',' ', s)\n",
        "\n",
        "IRREG = {\n",
        "  \"died\":\"die\",\"was\":\"be\",\"were\":\"be\",\"began\":\"begin\",\"took\":\"take\",\n",
        "  \"torn\":\"tear\",\"led\":\"lead\",\"left\":\"leave\",\"broke\":\"break\",\"ruled\":\"rule\",\n",
        "  \"saw\":\"see\",\"made\":\"make\",\n",
        "}\n",
        "def lemma(tok):\n",
        "    t = ntrig(tok)\n",
        "    if t in IRREG: return IRREG[t]\n",
        "    for suf in (\"ing\",\"ed\",\"es\",\"s\"):\n",
        "        if t.endswith(suf) and len(t)>len(suf)+2:\n",
        "            return t[:-len(suf)]\n",
        "    return t\n",
        "\n",
        "def canon_unders(s):\n",
        "    parts = re.split(r'\\s+', s.strip())\n",
        "    return parts[0].capitalize() + ''.join('_'+p for p in parts[1:]) if parts else s\n",
        "\n",
        "# --------------- Load data --------------------\n",
        "# results\n",
        "ext = Path(RESULTS_PATH).suffix.lower()\n",
        "res = pd.read_excel(RESULTS_PATH, dtype=str).fillna(\"\") if ext==\".xlsx\" else pd.read_csv(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "need = {\"chunk_id\",\"model_name\",\"model_version\",\"condition_shots\",\"output_raw\"}\n",
        "if not need.issubset(res.columns):\n",
        "    raise ValueError(f\"results must have {need}\")\n",
        "res = res[res[\"output_raw\"].astype(str).str.strip()!=\"\"].copy()\n",
        "\n",
        "# gold\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"PackB_Gold.csv must have chunk_id, events_norm\")\n",
        "\n",
        "# optional PackA text (for trigger validation)\n",
        "text_map = {}\n",
        "if os.path.exists(PACKA_PATH):\n",
        "    packA = pd.read_csv(PACKA_PATH, dtype=str).fillna(\"\")\n",
        "    if {\"chunk_id\",\"text\"}.issubset(packA.columns):\n",
        "        text_map = dict(zip(packA[\"chunk_id\"], packA[\"text\"]))\n",
        "\n",
        "# ---------------- Schema & priors from gold ----------------\n",
        "schema_types = set()\n",
        "trig_counts_by_type = {}\n",
        "trig_global_counts  = {}\n",
        "\n",
        "for s in gold[\"events_norm\"]:\n",
        "    if not isinstance(s,str): continue\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp,tr = item.split(\"|\",1)\n",
        "        tp_c = tp.strip()\n",
        "        tri  = lemma(tr)\n",
        "        schema_types.add(tp_c)\n",
        "        trig_counts_by_type.setdefault(tp_c,{}).update({tri: trig_counts_by_type.get(tp_c,{}).get(tri,0)+1})\n",
        "        trig_global_counts[tri] = trig_global_counts.get(tri,0)+1\n",
        "\n",
        "schema_types = sorted(schema_types)\n",
        "schema_norm  = [ntype(x) for x in schema_types]\n",
        "norm2canon   = {ntype(x):x for x in schema_types}\n",
        "\n",
        "# priors\n",
        "trig_prior = {}\n",
        "for tri,total in trig_global_counts.items():\n",
        "    for tp in schema_types:\n",
        "        c = trig_counts_by_type.get(tp,{}).get(tri,0)\n",
        "        if c>0:\n",
        "            trig_prior.setdefault(tp,{})[tri] = c/total\n",
        "\n",
        "# optional alias\n",
        "alias = {}\n",
        "if os.path.exists(SCHEMA_ALIASES):\n",
        "    aldf = pd.read_csv(SCHEMA_ALIASES, dtype=str).fillna(\"\")\n",
        "    if {\"alias\",\"schema_type\"}.issubset(aldf.columns):\n",
        "        for _,r in aldf.iterrows():\n",
        "            alias[ntype(r[\"alias\"])] = str(r[\"schema_type\"]).strip()\n",
        "\n",
        "# ----------------- TF-IDF index -----------------\n",
        "def expand_label(s): return \" \".join([s]+re.split(r'[_\\-\\s]+', s))\n",
        "\n",
        "tfidf_type = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_types = tfidf_type.fit_transform([expand_label(x) for x in schema_norm])\n",
        "\n",
        "type_trigger_docs = []\n",
        "for tp in schema_types:\n",
        "    bag=[]\n",
        "    for tri,cnt in trig_counts_by_type.get(tp,{}).items():\n",
        "        bag += [tri]*cnt\n",
        "    type_trigger_docs.append(\" \".join(bag))\n",
        "tfidf_trig = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_trigs = tfidf_trig.fit_transform(type_trigger_docs)\n",
        "\n",
        "def sim_type_label(qn):\n",
        "    v = tfidf_type.transform([expand_label(qn)])\n",
        "    return cosine_similarity(v, X_types).ravel()\n",
        "\n",
        "def sim_trig_to_type(tri):\n",
        "    v = tfidf_trig.transform([tri])\n",
        "    return cosine_similarity(v, X_trigs).ravel()\n",
        "\n",
        "# ----------------- Parse outputs ----------------\n",
        "EVENT_RE = re.compile(r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*([^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*([^.\\n\\r<]+)', re.I)\n",
        "\n",
        "def parse_lines(s):\n",
        "    if not isinstance(s,str) or not s.strip(): return []\n",
        "    parts = [p for p in s.split(\"<EVENTSEP>\") if p.strip()] or [s]\n",
        "    out=[]\n",
        "    for p in parts:\n",
        "        for m in EVENT_RE.finditer(p):\n",
        "            ot = ntype(m.group(1))\n",
        "            tr = ntrig(m.group(2))\n",
        "            if ot and tr:\n",
        "                out.append((ot,tr,lemma(tr)))\n",
        "    return out\n",
        "\n",
        "# ----------------- Mapper (same spirit as ultra-smart) ----------------\n",
        "def map_one(open_type_norm, trig_str, trig_lemma):\n",
        "    # alias\n",
        "    if open_type_norm in alias:\n",
        "        return alias[open_type_norm], \"alias\", 1.0\n",
        "\n",
        "    # exact/morph\n",
        "    if open_type_norm in norm2canon:\n",
        "        return norm2canon[open_type_norm], \"exact\", 1.0\n",
        "    base = re.sub(r'(ings|ing|ed|s)$','', open_type_norm)\n",
        "    for cand in {open_type_norm, base, base+\"ing\", base+\"ed\"}:\n",
        "        if cand in norm2canon:\n",
        "            return norm2canon[cand], \"norm\", 1.0\n",
        "\n",
        "    # trigger prior (soft)\n",
        "    sims_type = sim_type_label(open_type_norm)\n",
        "    fuzzy = np.array([fuzz_ratio(open_type_norm, t)/100 for t in schema_norm])\n",
        "    type_signal = 0.65*sims_type + 0.35*fuzzy\n",
        "\n",
        "    sims_trig = sim_trig_to_type(trig_lemma)\n",
        "    prior_vec = np.array([trig_prior.get(tp,{}).get(trig_lemma,0.0) for tp in schema_types])\n",
        "    trigger_signal = 0.6*sims_trig + 0.4*prior_vec\n",
        "\n",
        "    fused = 0.45*type_signal + 0.55*trigger_signal\n",
        "    idx = int(np.argmax(fused))\n",
        "    best = float(fused[idx])\n",
        "    if best>=FUSED_MIN_ACCEPT or float(np.max(sims_type))>=TYPE_TFIDF_GUARD:\n",
        "        return schema_types[idx], \"fused\", best\n",
        "    return \"UNMAPPED\",\"unmapped\",best\n",
        "\n",
        "# -------------- Build parsed+mapped frame --------------\n",
        "parsed_rows=[]\n",
        "for _,r in res.iterrows():\n",
        "    cid = r[\"chunk_id\"]\n",
        "    for (ot,tr,trlem) in parse_lines(r[\"output_raw\"]):\n",
        "        sch,how,sc = map_one(ot,tr,trlem)\n",
        "        parsed_rows.append({\n",
        "            \"chunk_id\": cid,\n",
        "            \"model_name\": r[\"model_name\"],\n",
        "            \"model_version\": r[\"model_version\"],\n",
        "            \"condition_shots\": r[\"condition_shots\"],\n",
        "            \"open_type\": ot, \"trigger\": tr, \"tri_lemma\": trlem,\n",
        "            \"schema_type\": sch, \"method\": how, \"map_score\": sc\n",
        "        })\n",
        "\n",
        "mapped_df = pd.DataFrame(parsed_rows)\n",
        "if mapped_df.empty:\n",
        "    raise RuntimeError(\"No events parsed/mapped. Check outputs formatting.\")\n",
        "\n",
        "# -------------- Gold pair sets -------------------\n",
        "def gold_pairs(s):\n",
        "    pairs=set()\n",
        "    if not isinstance(s,str): return pairs\n",
        "    for it in s.split(\";\"):\n",
        "        it=it.strip()\n",
        "        if \"|\" in it:\n",
        "            tp,tr=it.split(\"|\",1)\n",
        "            pairs.add((ntype(tp), ntrig(tr)))\n",
        "    return pairs\n",
        "\n",
        "gold_by_chunk = {cid: gold_pairs(ev) for cid,ev in zip(gold[\"chunk_id\"], gold[\"events_norm\"])}\n",
        "\n",
        "# -------------- Reliability tuning --------------\n",
        "# we learn alpha weights for each (model,version,shots) to maximize dev F1\n",
        "groups = mapped_df.groupby([\"model_name\",\"model_version\",\"condition_shots\"])\n",
        "keys   = list(groups.groups.keys())\n",
        "\n",
        "# dev/test split by chunk_id\n",
        "all_cids = mapped_df[\"chunk_id\"].unique().tolist()\n",
        "random.shuffle(all_cids)\n",
        "dev_ids = set(all_cids[:int(len(all_cids)*DEV_FRAC)])\n",
        "test_ids= set(all_cids[int(len(all_cids)*DEV_FRAC):])\n",
        "\n",
        "def score_micro(pred_pairs_by_chunk):\n",
        "    TP=FP=FN=0\n",
        "    for cid in gold_by_chunk:\n",
        "        g = gold_by_chunk.get(cid,set())\n",
        "        p = pred_pairs_by_chunk.get(cid,set())\n",
        "        TP += len(g & p); FP += len(p - g); FN += len(g - p)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return P,R,F\n",
        "\n",
        "def build_pred_pairs(weights, take_ids):\n",
        "    # aggregate per chunk consensus\n",
        "    pred_by_chunk={}\n",
        "    for cid, sub in mapped_df[mapped_df[\"chunk_id\"].isin(take_ids)].groupby(\"chunk_id\"):\n",
        "        cand = {}  # key=(type_norm, trig_lemma) -> score\n",
        "        freq = {}  # frequency for bonus\n",
        "        for _,row in sub.iterrows():\n",
        "            key = (ntype(row[\"schema_type\"]), row[\"tri_lemma\"])\n",
        "            # reliability of its source group\n",
        "            gk = (row[\"model_name\"], row[\"model_version\"], row[\"condition_shots\"])\n",
        "            alpha = weights.get(gk, 1.0)\n",
        "            # base score = alpha * map_score\n",
        "            sc = alpha * float(row[\"map_score\"])\n",
        "            cand[key] = cand.get(key,0.0) + sc\n",
        "            freq[key] = freq.get(key,0)+1\n",
        "\n",
        "        # add bonuses: consensus + prior\n",
        "        for k in cand:\n",
        "            tp_norm, tri = k\n",
        "            tp_canon = norm2canon.get(tp_norm, tp_norm)\n",
        "            prior = trig_prior.get(tp_canon, {}).get(tri, 0.0)\n",
        "            cand[k] += 0.10*freq[k] + 0.15*prior\n",
        "\n",
        "        # select with threshold relative to max\n",
        "        if not cand:\n",
        "            pred_by_chunk[cid]=set()\n",
        "            continue\n",
        "        m = max(cand.values())\n",
        "        tau = CONS_TAU_BASE * m\n",
        "        chosen = [k for k,v in cand.items() if v>=tau]\n",
        "\n",
        "        # de-duplicate near triggers; if multiple types share same trigger, keep best\n",
        "        final=[]\n",
        "        used=[]\n",
        "        for k in sorted(chosen, key=lambda x: cand[x], reverse=True):\n",
        "            _,tri = k\n",
        "            ok=True\n",
        "            for _,tri2 in used:\n",
        "                if fuzz_ratio(tri, tri2) >= NEAR_DUP_FUZZ:\n",
        "                    ok=False; break\n",
        "            if ok:\n",
        "                final.append(k)\n",
        "                used.append(k)\n",
        "\n",
        "        # Convert to pair set (type_norm, trigger_norm)\n",
        "        pred_by_chunk[cid] = set((k[0], k[1]) for k in final)\n",
        "    return pred_by_chunk\n",
        "\n",
        "# random search for reliability weights\n",
        "bestW={k:1.0 for k in keys}; bestF=-1.0\n",
        "for _ in range(N_TRIALS_WEIGHTS):\n",
        "    trial={k: float(np.random.uniform(0.6,1.6)) for k in keys}  # 0.6..1.6\n",
        "    pred_pairs = build_pred_pairs(trial, dev_ids)\n",
        "    _,_,F = score_micro(pred_pairs)\n",
        "    if F>bestF: bestF=F; bestW=trial\n",
        "print(f\"[tuner] best dev F1={bestF:.4f} with {len(bestW)} weights\")\n",
        "\n",
        "# -------------- Evaluate on all chunks --------------\n",
        "pred_all = build_pred_pairs(bestW, mapped_df[\"chunk_id\"].unique())\n",
        "P,R,F = score_micro(pred_all)\n",
        "with open(f\"{OUT_DIR}/Metrics_overall_micro.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Micro Precision: {P:.4f}\\nMicro Recall: {R:.4f}\\nMicro F1: {F:.4f}\\n\")\n",
        "print(f\"[overall] P={P:.4f} R={R:.4f} F1={F:.4f}\")\n",
        "\n",
        "# -------------- Save ensembled predictions ----------\n",
        "rows=[]\n",
        "for cid, pairs in pred_all.items():\n",
        "    for (tp_norm, tri) in pairs:\n",
        "        rows.append({\"chunk_id\": cid,\n",
        "                     \"schema_type\": canon_unders(norm2canon.get(tp_norm, tp_norm)),\n",
        "                     \"trigger\": tri})\n",
        "ens_df = pd.DataFrame(rows).sort_values([\"chunk_id\",\"schema_type\",\"trigger\"])\n",
        "ens_df.to_csv(f\"{OUT_DIR}/Ensembled_Predictions.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"[ok] wrote {OUT_DIR}/Ensembled_Predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l7QozXHBH1M",
        "outputId": "4020c5dd-075f-4ef0-8143-ebdd29452920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2.8/3.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[tuner] best dev F1=0.0261 with 16 weights\n",
            "[overall] P=0.1964 R=0.0772 F1=0.1108\n",
            "[ok] wrote /content/out_cmse/Ensembled_Predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CMSE v2 — Tuned Consensus + Correct Trigger Keying\n",
        "#   - Robust mapping (type-TFIDF + trigger priors + fuzzy)\n",
        "#   - Tunes BOTH reliability weights and consensus params on dev\n",
        "#   - Aggregates on (type_norm, trigger_norm)  [not lemma!]\n",
        "#   - Optional top-K per density_bin from PackA\n",
        "#   - Evaluates micro P/R/F1\n",
        "# ==========================================================\n",
        "\n",
        "import os, re, random, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "RESULTS_PATH   = \"/content/results.xlsx\"\n",
        "PACKB_GOLD     = \"/content/PackB_Gold.csv\"\n",
        "PACKA_PATH     = \"/content/packs/PackA_TextChunks.csv\"   # optional (needs: chunk_id,text,density_bin)\n",
        "SCHEMA_ALIASES = \"/content/packs/Schema_Aliases.csv\"     # optional\n",
        "\n",
        "OUT_DIR        = \"/content/out_cmse_v2\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dev split + search budgets\n",
        "DEV_FRAC            = 0.25\n",
        "N_TRIALS_WEIGHTS    = 60     # reliability weights tuning\n",
        "# small grid for consensus params\n",
        "TAU_BASE_SET        = [0.45, 0.55, 0.65]\n",
        "FREQ_BONUS_SET      = [0.05, 0.10, 0.15]\n",
        "PRIOR_BONUS_SET     = [0.05, 0.10, 0.15]\n",
        "DEDUP_FUZZ_SET      = [85, 90, 95]\n",
        "KEY_BY_LEMMA_FLAGS  = [False, True]  # expect False to win\n",
        "\n",
        "# Optional top-K by density; requires PackA density_bin\n",
        "USE_TOPK_BY_DENSITY = True\n",
        "TOPK_CFG = {\"low\": 3, \"med\": 6, \"high\": 10}  # adjust if needed\n",
        "\n",
        "# Mapping thresholds (kept reasonable)\n",
        "FUSED_MIN_ACCEPT = 0.38\n",
        "TYPE_TFIDF_GUARD = 0.33\n",
        "\n",
        "random.seed(42); np.random.seed(42)\n",
        "\n",
        "# ---------------- Dependencies ----------------\n",
        "try:\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "except Exception:\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "except Exception:\n",
        "    !pip -q install scikit-learn\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ---------------- Normalizers -----------------\n",
        "def ntype(s):\n",
        "    if not isinstance(s,str): return \"\"\n",
        "    return re.sub(r'[_\\-\\s]+',' ', s.strip().lower())\n",
        "\n",
        "def ntrig(s):\n",
        "    if not isinstance(s,str): return \"\"\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r'[.;,:\\-]+$','', s)\n",
        "    return re.sub(r'\\s+',' ', s)\n",
        "\n",
        "IRREG = {\n",
        "  \"died\":\"die\",\"was\":\"be\",\"were\":\"be\",\"began\":\"begin\",\"took\":\"take\",\n",
        "  \"torn\":\"tear\",\"led\":\"lead\",\"left\":\"leave\",\"broke\":\"break\",\"ruled\":\"rule\",\n",
        "  \"saw\":\"see\",\"made\":\"make\",\n",
        "}\n",
        "def lemma(tok):\n",
        "    t = ntrig(tok)\n",
        "    if t in IRREG: return IRREG[t]\n",
        "    for suf in (\"ing\",\"ed\",\"es\",\"s\"):\n",
        "        if t.endswith(suf) and len(t)>len(suf)+2:\n",
        "            return t[:-len(suf)]\n",
        "    return t\n",
        "\n",
        "def canon_unders(s):\n",
        "    parts = re.split(r'\\s+', s.strip())\n",
        "    return parts[0].capitalize() + ''.join('_'+p for p in parts[1:]) if parts else s\n",
        "\n",
        "# ---------------- Load data -------------------\n",
        "# results\n",
        "ext = Path(RESULTS_PATH).suffix.lower()\n",
        "res = pd.read_excel(RESULTS_PATH, dtype=str).fillna(\"\") if ext==\".xlsx\" else pd.read_csv(RESULTS_PATH, dtype=str).fillna(\"\")\n",
        "need = {\"chunk_id\",\"model_name\",\"model_version\",\"condition_shots\",\"output_raw\"}\n",
        "if not need.issubset(res.columns):\n",
        "    raise ValueError(f\"results must have {need}\")\n",
        "res = res[res[\"output_raw\"].astype(str).str.strip()!=\"\"].copy()\n",
        "\n",
        "# gold\n",
        "gold = pd.read_csv(PACKB_GOLD, dtype=str).fillna(\"\")\n",
        "if not {\"chunk_id\",\"events_norm\"}.issubset(gold.columns):\n",
        "    raise ValueError(\"PackB_Gold.csv must have chunk_id, events_norm\")\n",
        "\n",
        "# optional PackA: text + density_bin\n",
        "packA = None; density_map = {}\n",
        "if os.path.exists(PACKA_PATH):\n",
        "    tmp = pd.read_csv(PACKA_PATH, dtype=str).fillna(\"\")\n",
        "    if {\"chunk_id\",\"text\"}.issubset(tmp.columns):\n",
        "        packA = tmp\n",
        "    if \"density_bin\" in tmp.columns:\n",
        "        density_map = dict(zip(tmp[\"chunk_id\"], tmp[\"density_bin\"]))\n",
        "\n",
        "# ---------------- Schema & priors from gold ----------------\n",
        "schema_types = set()\n",
        "trig_counts_by_type = {}\n",
        "trig_global_counts  = {}\n",
        "\n",
        "for s in gold[\"events_norm\"]:\n",
        "    if not isinstance(s,str): continue\n",
        "    for item in s.split(\";\"):\n",
        "        item = item.strip()\n",
        "        if \"|\" not in item: continue\n",
        "        tp,tr = item.split(\"|\",1)\n",
        "        tp_c = tp.strip()\n",
        "        tri  = lemma(tr)\n",
        "        schema_types.add(tp_c)\n",
        "        trig_counts_by_type.setdefault(tp_c,{}).update({tri: trig_counts_by_type.get(tp_c,{}).get(tri,0)+1})\n",
        "        trig_global_counts[tri] = trig_global_counts.get(tri,0)+1\n",
        "\n",
        "schema_types = sorted(schema_types)\n",
        "schema_norm  = [ntype(x) for x in schema_types]\n",
        "norm2canon   = {ntype(x):x for x in schema_types}\n",
        "\n",
        "# priors\n",
        "trig_prior = {}\n",
        "for tri,total in trig_global_counts.items():\n",
        "    for tp in schema_types:\n",
        "        c = trig_counts_by_type.get(tp,{}).get(tri,0)\n",
        "        if c>0:\n",
        "            trig_prior.setdefault(tp,{})[tri] = c/total\n",
        "\n",
        "# optional alias\n",
        "alias = {}\n",
        "if os.path.exists(SCHEMA_ALIASES):\n",
        "    aldf = pd.read_csv(SCHEMA_ALIASES, dtype=str).fillna(\"\")\n",
        "    if {\"alias\",\"schema_type\"}.issubset(aldf.columns):\n",
        "        for _,r in aldf.iterrows():\n",
        "            alias[ntype(r[\"alias\"])] = str(r[\"schema_type\"]).strip()\n",
        "\n",
        "# ----------------- TF-IDF index -----------------\n",
        "def expand_label(s): return \" \".join([s]+re.split(r'[_\\-\\s]+', s))\n",
        "\n",
        "tfidf_type = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_types = tfidf_type.fit_transform([expand_label(x) for x in schema_norm])\n",
        "\n",
        "type_trigger_docs = []\n",
        "for tp in schema_types:\n",
        "    bag=[]\n",
        "    for tri,cnt in trig_counts_by_type.get(tp,{}).items():\n",
        "        bag += [tri]*cnt\n",
        "    type_trigger_docs.append(\" \".join(bag))\n",
        "tfidf_trig = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5))\n",
        "X_trigs = tfidf_trig.fit_transform(type_trigger_docs)\n",
        "\n",
        "def sim_type_label(qn):\n",
        "    v = tfidf_type.transform([expand_label(qn)])\n",
        "    return cosine_similarity(v, X_types).ravel()\n",
        "\n",
        "def sim_trig_to_type(tri):\n",
        "    v = tfidf_trig.transform([tri])\n",
        "    return cosine_similarity(v, X_trigs).ravel()\n",
        "\n",
        "# ----------------- Parse outputs ----------------\n",
        "EVENT_RE = re.compile(r'(?:<EVENTSEP>\\s*)?Event\\s*type\\s*:\\s*([^.\\n\\r:]+)\\.\\s*Trigger\\s*:\\s*([^.\\n\\r<]+)', re.I)\n",
        "\n",
        "def parse_lines(s):\n",
        "    if not isinstance(s,str) or not s.strip(): return []\n",
        "    parts = [p for p in s.split(\"<EVENTSEP>\") if p.strip()] or [s]\n",
        "    out=[]\n",
        "    for p in parts:\n",
        "        for m in EVENT_RE.finditer(p):\n",
        "            ot = ntype(m.group(1))\n",
        "            tr = ntrig(m.group(2))\n",
        "            if ot and tr:\n",
        "                out.append((ot,tr,lemma(tr)))\n",
        "    return out\n",
        "\n",
        "# ----------------- Mapper -----------------------\n",
        "def map_one(open_type_norm, trig_str, trig_lemma):\n",
        "    # alias\n",
        "    if open_type_norm in alias:\n",
        "        return alias[open_type_norm], \"alias\", 1.0\n",
        "\n",
        "    # exact/morph\n",
        "    if open_type_norm in norm2canon:\n",
        "        return norm2canon[open_type_norm], \"exact\", 1.0\n",
        "    base = re.sub(r'(ings|ing|ed|s)$','', open_type_norm)\n",
        "    for cand in {open_type_norm, base, base+\"ing\", base+\"ed\"}:\n",
        "        if cand in norm2canon:\n",
        "            return norm2canon[cand], \"norm\", 1.0\n",
        "\n",
        "    # fused (type + trigger evidence)\n",
        "    sims_type = sim_type_label(open_type_norm)\n",
        "    fuzzy = np.array([fuzz_ratio(open_type_norm, t)/100 for t in schema_norm])\n",
        "    type_signal = 0.65*sims_type + 0.35*fuzzy\n",
        "\n",
        "    sims_trig = sim_trig_to_type(trig_lemma)\n",
        "    prior_vec = np.array([trig_prior.get(tp,{}).get(trig_lemma,0.0) for tp in schema_types])\n",
        "    trigger_signal = 0.6*sims_trig + 0.4*prior_vec\n",
        "\n",
        "    fused = 0.45*type_signal + 0.55*trigger_signal\n",
        "    idx = int(np.argmax(fused))\n",
        "    best = float(fused[idx])\n",
        "    if best>=FUSED_MIN_ACCEPT or float(np.max(sims_type))>=TYPE_TFIDF_GUARD:\n",
        "        return schema_types[idx], \"fused\", best\n",
        "    return \"UNMAPPED\",\"unmapped\",best\n",
        "\n",
        "# -------------- Build parsed+mapped frame --------------\n",
        "parsed_rows=[]\n",
        "for _,r in res.iterrows():\n",
        "    cid = r[\"chunk_id\"]\n",
        "    for (ot,tr,trlem) in parse_lines(r[\"output_raw\"]):\n",
        "        sch,how,sc = map_one(ot,tr,trlem)\n",
        "        parsed_rows.append({\n",
        "            \"chunk_id\": cid,\n",
        "            \"model_name\": r[\"model_name\"],\n",
        "            \"model_version\": r[\"model_version\"],\n",
        "            \"condition_shots\": r[\"condition_shots\"],\n",
        "            \"open_type\": ot, \"trigger\": tr, \"tri_lemma\": trlem,\n",
        "            \"schema_type\": sch, \"method\": how, \"map_score\": sc\n",
        "        })\n",
        "\n",
        "mapped_df = pd.DataFrame(parsed_rows)\n",
        "if mapped_df.empty:\n",
        "    raise RuntimeError(\"No events parsed/mapped. Check outputs formatting.\")\n",
        "\n",
        "# -------------- Gold pairs ----------------------\n",
        "def gold_pairs(s):\n",
        "    pairs=set()\n",
        "    if not isinstance(s,str): return pairs\n",
        "    for it in s.split(\";\"):\n",
        "        it=it.strip()\n",
        "        if \"|\" in it:\n",
        "            tp,tr=it.split(\"|\",1)\n",
        "            pairs.add((ntype(tp), ntrig(tr)))\n",
        "    return pairs\n",
        "\n",
        "gold_by_chunk = {cid: gold_pairs(ev) for cid,ev in zip(gold[\"chunk_id\"], gold[\"events_norm\"])}\n",
        "\n",
        "# -------------- Dev/Test split by chunk ----------\n",
        "all_cids = mapped_df[\"chunk_id\"].unique().tolist()\n",
        "random.shuffle(all_cids)\n",
        "n_dev = max(1, int(len(all_cids)*DEV_FRAC))\n",
        "dev_ids = set(all_cids[:n_dev])\n",
        "test_ids= set(all_cids[n_dev:])\n",
        "\n",
        "# -------------- Reliability weights tuning -------\n",
        "group_keys = list(mapped_df.groupby([\"model_name\",\"model_version\",\"condition_shots\"]).groups.keys())\n",
        "\n",
        "def score_micro(pred_pairs_by_chunk):\n",
        "    TP=FP=FN=0\n",
        "    for cid in gold_by_chunk:\n",
        "        g = gold_by_chunk.get(cid,set())\n",
        "        p = pred_pairs_by_chunk.get(cid,set())\n",
        "        TP += len(g & p); FP += len(p - g); FN += len(g - p)\n",
        "    P = TP/(TP+FP) if (TP+FP) else 0.0\n",
        "    R = TP/(TP+FN) if (TP+FN) else 0.0\n",
        "    F = 2*P*R/(P+R) if (P+R) else 0.0\n",
        "    return P,R,F\n",
        "\n",
        "def build_pred_pairs(weights, params, take_ids):\n",
        "    tau_base, freq_bonus, prior_bonus, dedup_fuzz, key_by_lemma = params\n",
        "    pred_by_chunk = {}\n",
        "\n",
        "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
        "\n",
        "    for cid, sub in mapped_df[mapped_df[\"chunk_id\"].isin(take_ids)].groupby(\"chunk_id\"):\n",
        "        # choose keying (namedtuple row)\n",
        "        def keyrow(row):\n",
        "            tpn = ntype(row.schema_type)\n",
        "            tri = row.tri_lemma if key_by_lemma else ntrig(row.trigger)\n",
        "            return (tpn, tri)\n",
        "\n",
        "        cand = {}   # key -> aggregated score\n",
        "        freq = {}   # key -> vote count\n",
        "\n",
        "        # Use itertuples -> attribute access, not dict indexing\n",
        "        for row in sub.itertuples(index=False):\n",
        "            k = keyrow(row)\n",
        "            gk = (row.model_name, row.model_version, row.condition_shots)\n",
        "            alpha = weights.get(gk, 1.0)\n",
        "            sc = alpha * float(row.map_score)\n",
        "            cand[k] = cand.get(k, 0.0) + sc\n",
        "            freq[k] = freq.get(k, 0) + 1\n",
        "\n",
        "        # bonuses (consensus + prior)\n",
        "        for k in cand:\n",
        "            tp_norm, tri = k\n",
        "            tp_canon = norm2canon.get(tp_norm, tp_norm)\n",
        "            prior = trig_prior.get(tp_canon, {}).get(tri, 0.0)\n",
        "            cand[k] += freq_bonus * freq[k] + prior_bonus * prior\n",
        "\n",
        "        if not cand:\n",
        "            pred_by_chunk[cid] = set()\n",
        "            continue\n",
        "\n",
        "        # dynamic threshold\n",
        "        m = max(cand.values())\n",
        "        tau = tau_base * m\n",
        "        chosen = [k for k, v in cand.items() if v >= tau]\n",
        "\n",
        "        # optional top-K by density\n",
        "        if USE_TOPK_BY_DENSITY and density_map:\n",
        "            bin_ = density_map.get(cid, \"\")\n",
        "            K = TOPK_CFG.get(bin_, None)\n",
        "            if K is not None and len(chosen) > K:\n",
        "                chosen = sorted(chosen, key=lambda x: cand[x], reverse=True)[:K]\n",
        "\n",
        "        # de-duplicate by fuzzy trigger\n",
        "        final, used = [], []\n",
        "        for k in sorted(chosen, key=lambda x: cand[x], reverse=True):\n",
        "            _, tri = k\n",
        "            if all(fuzz_ratio(tri, tri2) < dedup_fuzz for _, tri2 in used):\n",
        "                final.append(k)\n",
        "                used.append(k)\n",
        "\n",
        "        pred_by_chunk[cid] = set(final)\n",
        "\n",
        "    return pred_by_chunk\n",
        "\n",
        "\n",
        "# tune reliability weights on dev with a fixed reasonable consensus (we'll tune consensus next)\n",
        "base_params = (0.55, 0.10, 0.10, 90, False)  # tau_base, freq_bonus, prior_bonus, fuzz, key_by_lemma\n",
        "best_w={k:1.0 for k in group_keys}; bestF=-1.0\n",
        "for _ in range(N_TRIALS_WEIGHTS):\n",
        "    trial={k: float(np.random.uniform(0.6,1.6)) for k in group_keys}\n",
        "    pred_pairs = build_pred_pairs(trial, base_params, dev_ids)\n",
        "    _,_,F = score_micro(pred_pairs)\n",
        "    if F>bestF: bestF=F; best_w=trial\n",
        "print(f\"[tuner] dev F1 (weights only)={bestF:.4f}\")\n",
        "\n",
        "# tune consensus params on dev (grid over small sets)\n",
        "best_params = base_params; bestF2 = -1.0\n",
        "for tau_base in TAU_BASE_SET:\n",
        "    for fb in FREQ_BONUS_SET:\n",
        "        for pb in PRIOR_BONUS_SET:\n",
        "            for fuzzv in DEDUP_FUZZ_SET:\n",
        "                for use_lem in KEY_BY_LEMMA_FLAGS:\n",
        "                    params=(tau_base, fb, pb, fuzzv, use_lem)\n",
        "                    pred_pairs = build_pred_pairs(best_w, params, dev_ids)\n",
        "                    _,_,F = score_micro(pred_pairs)\n",
        "                    if F>bestF2:\n",
        "                        bestF2 = F; best_params = params\n",
        "print(f\"[tuner] dev F1 (weights+consensus)={bestF2:.4f} with params={best_params}\")\n",
        "\n",
        "# -------------- Evaluate on ALL chunks --------------\n",
        "pred_all = build_pred_pairs(best_w, best_params, mapped_df[\"chunk_id\"].unique())\n",
        "P,R,F = score_micro(pred_all)\n",
        "with open(f\"{OUT_DIR}/Metrics_overall_micro.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Micro Precision: {P:.4f}\\nMicro Recall: {R:.4f}\\nMicro F1: {F:.4f}\\n\")\n",
        "print(f\"[overall] P={P:.4f} R={R:.4f} F1={F:.4f}\")\n",
        "\n",
        "# -------------- Save ensembled predictions ----------\n",
        "rows=[]\n",
        "for cid, pairs in pred_all.items():\n",
        "    for (tp_norm, tri) in pairs:\n",
        "        rows.append({\"chunk_id\": cid,\n",
        "                     \"schema_type\": canon_unders(norm2canon.get(tp_norm, tp_norm)),\n",
        "                     \"trigger\": tri})\n",
        "ens_df = pd.DataFrame(rows).sort_values([\"chunk_id\",\"schema_type\",\"trigger\"])\n",
        "ens_df.to_csv(f\"{OUT_DIR}/Ensembled_Predictions.csv\", index=False, encoding=\"utf-8\")\n",
        "print(f\"[ok] wrote {OUT_DIR}/Ensembled_Predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKJOF-LVCi65",
        "outputId": "0f5d25c7-2f6b-474d-f2a6-b439a37842d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tuner] dev F1 (weights only)=0.1677\n",
            "[tuner] dev F1 (weights+consensus)=0.1723 with params=(0.45, 0.05, 0.05, 85, False)\n",
            "[overall] P=0.6319 R=0.3614 F1=0.4598\n",
            "[ok] wrote /content/out_cmse_v2/Ensembled_Predictions.csv\n"
          ]
        }
      ]
    }
  ]
}